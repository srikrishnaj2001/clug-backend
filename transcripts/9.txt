
00:00:08
Hello everyone and welcome to our generative AI full course. You have seen it everywhere AI writing code, creating images and answering questions. But how does it actually work and more importantly how can you use it? Generative AI is a type of artificial intelligence that can create new content like text, images, music or even code based on the data it has learned. And it powers tools like chat GPD, DAL e and GitHub copilot which are now used in writing, design, coding and much more. As the technology advances, generative


00:00:44
AI is becoming more powerful, accessible and impactful, opening up new possibilities across creative and technical fields. So let's quickly have a look at what we will cover in this full course. First we will start with the basics understanding what is generative AI. You will learn how this powerful technology works and why it's different from traditional AI. Next we will look at the real world generative AI examples and then we will walk you through the most powerful generative AI tools including Chime GPD, DA, GitHub


00:01:16
copilot, Deep Seek and much more and how they use in everyday task. But what powers these tools? That's where LLM or large language models comes in. You will understand how they work and why they are the backbone of generative AI. We will also explore GANs, generative adversarial networks, and how they help machines generate realistic images, videos, and more. And of course, we can't ignore transformers, the deep learning models that made all of this possible. Now, how do we talk to these AI models effectively? That's


00:01:51
where prompt engineering comes in. You will learn how to craft prompts that get the results you want. And as we go deeper, we will explore the difference between agentic AI and generative AI. Two powerful concepts with very different goals. Then we will look at the future of generative AI and what it means for your career. Next, we will also dive into the latest in generative AI and AI models comparing innovations from major players in the field. One of the biggest questions right now is that deepseek versus open AI. Who's ahead and


00:02:25
what makes their models different? Then we will also look at the ethics of genetic AI. What are the risk and what are the responsibilities? And to guide your journey, we will provide a clear Gen AI road map showing what to learn step by step. Then we will cover the top five career opportunities in generative AI so you know where to aim and what to prepare for. Finally, we will wrap up with the frequently asked generative AI interview questions helping you get job ready. But before we get started, please


00:02:57
like, share, and subscribe to Edureka's YouTube channel and hit the bell icon to stay updated on the latest content from Edureka. Also, check out Edureka's generative AI course. It is designed to help you understand the core concepts of generative AI and apply them to real world projects. This course covers essential topics like Python programming, data science, artificial intelligence, natural language processing, generative AI, prompt engineering, CAD GPT, agentic AI and much more. It includes interactive


00:03:30
learning, hands-on projects, and assignments to give you practical experience. The curriculum is carefully crafted by industry experts based on insights from over 5,000 global job descriptions aiming to equip you with the skills needed to succeed in various AI related roles. All right, now let's start with our first topic that is what is generative AI. Simply put, it's a type of artificial intelligence that can produce new content across various formats. Here are a few areas where it's


00:04:01
making a huge impact. First content creation. It is generating text, articles and blog posts. Saving arts for writers and content creators. Next, image generation. Creating visuals or art from the text bronze as seen in models like DAL e now on its third version. Next, coding assistance providing code suggestions and completions with tools like GitHub copilot helping developers. Next, language translation, breaking down language barriers in real time through advanced translation models. Then personalized healthcare, enhancing


00:04:39
medical treatments by tailoring recommendations based on individuals patient data. And finally, marketing and optimization. It helps establish marketing strategies in businesses. And these capabilities make generative AI a versatile technology that is reshaping industries. So let's examine some real world application and see how they work. Generative AI is at the core of many revolutionary applications today. We have text generation. From producing entire articles to summarizing content, tools like GP4 are transforming content


00:05:12
creation across industries. Next, language translation. AI powered translation tools are improving cross language communications by understanding context. Then writing assistance. Grammarly are similar tools refine grammar, tone and clarity assisting professionals and student alike. Next we have business AI models are making business insights accessible, automating support and enhancing decision making. Next music generation AI is now venturing into creativity, composing unique tracks and musical elements.


00:05:49
Finally, machine learning models platforms like H2O.AI AI are giving access to machine learning models allowing users without deep expertise to create powerful models. Now that we understood the potential of generative AI and now let's go deeper into how it actually works. So to understand how generative AI operates, let's break down the process. First define objective. So start with a clear goal whether it's generating text, creating an image or assisting with code. Next gather and


00:06:20
preprocess data. So collect and prepare data ensuring it's clean and structured with the model. Then choose appropriate model. So select or design a model tailored to your needs. Sometimes building on pre-existing models. Next train the model. Feed data into the model so it can learn patterns and build into knowledge base. Next evaluate and refine. So fine-tune the model as needed to optimize its efficiency and accuracy. Then test and validate. run test to measure performance and accuracy ensuring it align with the objectives


00:06:56
and finally deploy and iterate. So once ready deploy the model and continue refining it with user feedback and data and this cycle ensures generative AI models stay relevant and improve over time. Now moving on to the examples of generative AI tools. So many generative AI tools are available today each with its own specialtity. Tools like GitHub copilot for coding, DALI 3 for image generation and advanced language models like GPT are among the top layers. And if you're curious to explore these tools


00:07:28
further, check out the video link in the description covering popular generative AI examples. Now let's look at the growing presence of generative AI across different sectors. First in healthcare, generative AI is projected to reach a $17.2 billion market by 2032, transforming clinical applications and healthcare systems. Next, education. The AIdriven education market is expanding, especially for students, teachers, and administrators, making personalized learning more accessible. and then workspace. Generative AI adaption is


00:08:05
rising with the highest impact in marketing and tech industries followed by sectors like consulting and healthcare. Each graph shows a significant trend as generative AI is becoming integral in diverse fields and reshaping workplace roles. As we look ahead, generative AI is set to transform even more areas. And here are some key impacts we can expect in AIdriven creativity. From art to music, AI will open new creative horizons. Next, AI personalizations. Tailored user experience will become the norm. Then


00:08:41
real time generation. Realtime content generation will improve virtual assistance and automated responses. Next, AI in architecture. AI will assist architects in design and material optimization. Then human AI collaboration. As AI matures, humans and AI will work in tandemss to maximize productivity and innovation. Then advanced AI models. We will see even more sophisticated models pushing the boundaries of what AI can achieve. Generative AI's future is promising. So with potential benefits that will


00:09:16
enhance lives and drive economic growth. Now, are you guys ready to get hands-on? Stick around for a mini LLM project where we will build a YouTube video summarizer. We will show you how to extract video transcript and then we will use an LLM to generate summaries and build a user-friendly interface using Streamlit. Are you excited? So, now let's jump in. So, first things first, we need a solid environment for our project. First, open your terminal and let's create a new cond environment


00:09:49
to keep everything organized. Let's run the code using VS Code. You can also use other code editors such as PyCharm, but let's use VS Code for now. Now, in the terminal, let's type the command for setting up the environment in your editor. For that, run this command. Just type create - p virtual environment which is v nv python and equal equal to we are using 3.10 which is the python version and give hyphen y. Here the hyphen p v and v specifies the path and the environment name while the hyphen y skips prompt for


00:10:30
a smoother install. So now while that's setting up let's create a few essential files. So first we will create a env file for our API keys and environment variables. Next we will create a requirements.txt file for the libraries we will need such as YouTube transcript API to extract transcript from YouTube and streamlit for our front end. Then Google generative for accessing the Google Gemini API. also the Python env for handling environment variables and part lip for the better part management. Now


00:11:08
with our files in place, let's set up the Google Gemini API access. Now head over to the makers.google.com and as you can see on the screen in the top left corner, you will get the API key interface and once you click that, it will redirect us to the API key interface. Here you can see a button which is create API key. So simply click on that and select your model and press create API key in the existing project and then your API key will be generated. Now copy your API key and once you have


00:11:44
got the API key open your environment file and add it there. So for that let's create a variable Google API key. Over here we will paste the API key. So once that is done back in the terminal activate your new enrollment with the command activate virtual environment back slash which is venv. Since I have already installed in my system it's not taking much time. So while you are installing it might take little time. Now let us install requirements.txt in our terminal. And for that the command is pip


00:12:26
install - r requirements txt. So once you enter the files present in the requirements txt will get installed. Awesome. So now let's move to the main code setup. So for that open app. py and start with imports. First let us import some important libraries. So for that import stream list as SD from env import load env. And next load env which will load environment variables and next let's import google.generative generative as gen AI and also import OS and from YouTube transcript API import YouTube transcript API.


00:13:22
So let's go to the YouTube transcript API and check if you're doing it correctly because in some system it doesn't work. So you can simply go here and copy this command and paste it into your terminal. Now let's move on to configure the API key. So for that just type genai dot configure and inside the bracket just type API key equal to OS dot get env and inside the bracket let's add the Google API key here. Now to prompt a model we will use a template like this. So simply type


00:14:03
please summarize this YouTube video transcript in 250 words or less highlighting key points. Now moving on to extracting YouTube transcript. So for that to fetch the transcript we will create a function. So here in this code as you can see on the screen the function extract transcript details takes the YouTube URL as an input and retrieves the transcript of the video if available. First, it extracts the video ID from the URL by splitting the URL at the equal sign and taking the second part and using YouTube transcript API


00:14:36
dot get transcript video ID, it fetches the transcript data for the video ID and it then combines the text from each part of the transcript into a single string and if there is an error like example no transcript available, it prints the error and returns none. So this function effectively converts a YouTube video URL into its textual transcript. Next, let's write a function to generate the summary. Here the generate Gemini content function generates the content based on the combination of transcript


00:15:06
text and a prompt. So it first initializes a generative AI model called Gemini and then the model creates a content processing the combined input of prompt and transcript text and finally it returns the generated text from the model's response. So this function effectively uses the Gemini model to produce AI generated content based on the specific text inputs. Now let's move on to building the front end. Now to connect it all with streaml first add a button label get detailed notes. So when


00:15:37
clicked it prompts the user to enter a YouTube link in a text input field and it uses the extract transcript details YouTube link to get video transcript. So if the transcript is successfully retrieved, it generates a detailed summary with generate Gemini content transcript text or prompt. So finally it displays the summary under the headings detailed summary. Now let's give a finishing touch to our app. So for that let's make it look great. You can add headers and also you can give a footer


00:16:09
and maybe you can also add a YouTube icon to it and even customize colors and incorporating thumbnails if the link is valid will create an even richer experience. Now let us test the app. So finally let's run a command streamlit run app. py. As you can see on the screen our app is successfully running. And now let's check if it's running fine. So for that let's open our YouTube and here select a video that you want a summary of. So copy the link paste a YouTube link into the app and hit generate.


00:16:50
As you can see on the screen it is running and in just a second you will get the summary of the video. It has given us the summary of the video. Also, there is no limit to the app and it will consider even the longer videos. And that's it. With just a few lines of code, we created a powerful YouTube summarizer using Streamlit and Google Gemini. If you enjoyed building this, stay tuned for more AI projects. [Music] In 1932, George Arstrroni invented a mission he reportedly called the mechanical brain to translate between


00:17:29
languages using a mechanical computer encoded onto punch cards. And then in 1966, MIT professor Joseph Vizenbomb created the first chatboard, Eliza, which simulates conversation with a psychotherapist. And in 2011, Apple released Siri, a voice powered personal assistant that generates responses and takes action in response to voice request. Also, by the end of 2022, Open AI developed Chat GPT, which was trained on 40 GB of data and consists of 117 million parameters. GPT paves the way for subsequent


00:18:07
language models in content generation, chatboards, and language translation. So the preparation of generative AI started before 1932 and is still developing at much faster rate. So let us now explore its real world impact. Well, generative AI has a wide range of application across various industries transforming the way we create and interact with content. So let's take a look at some of the most compelling use cases. The following image shows a flowchart of advancement in healthcare technology. So


00:18:39
it starts with general healthcare and then moves to medical image synthesis for better diagnostics and next is drug discovery and development followed by personalized medicine which tailor treatments to individual patients for better care and outcomes. And the next image displays a flowchart of developments in finance and trading. So it begins with general finance and trading followed by algorithmic trading which automates trades and then it shows predictive market analysis to forecast trends and finally the highfrequency


00:19:11
trading algorithms that quickly execute trades to take advantage of small market movements. And the next image shows the growth of AI powered content creation. It starts with a basic content creation and then moves to generating art and music followed by automating content for marketing and finally with the development of virtual assistants and chatboards highlighting how AI is improving content production and the last image illustrates the progression of AI in natural language processing. So it starts with basic NLP and then text


00:19:44
generation for content creation followed by sentiment analysis and finally the language translation that includes sentiment analysis showing how AI is becoming more advanced in understanding and interacting with human language. I hope you all got an idea now on how AI is shaping these industries and making task easier and more efficient. Now moving on to the impact of generative AI on industries. So the image provides a breakdown of the impact of generative AI across various industries categorizing


00:20:18
task into four key areas such as higher potential for automation, higher potential for augmentation, lower potential for augmentation and automation and non- language task. So the data shows that the industries like banking, insurance and software have a higher potential for automation while the sectors like natural resources the customer goods have a lower potential. So this analysis provides a valuable insights for businesses seeking to leverage AI for efficiency and innovation. Now let us explore the


00:20:51
forefront of generative AI with these powerful tools. From coding assistants like GitHub copilot to text generators like right Sony and image creators like Midjourney, these innovative platforms are transforming industries and unlocking new creative possibilities. So join us as we explore the diverse landscape of generative AI and its potential to transform how we work and create. First on the list we have GPT4. GPT4 is a state-of-the-art language model capable of generating human quality text, translating languages,


00:21:25
writing different kinds of creative content, and answering your questions in an informative way. Its advanced features include improved factual accuracy, reduced bias, and the ability to handle more complex problems. So, GP4 can be used for various applications such as content creation, customer service, language translation, and education. So as you can see the screen we are on the charg site and on the left corner you can see three icons. So the first icon shows the history. So suppose you have deleted any prompt you can go to


00:21:58
this and you can check and the second icon shows a new chat section. So here you can create multiple charts and the third icon shows the chart GP updates. So you can get different updates related to the model here. Now the text bar in the middle shows the area where you can write different prompts and press enter to get the answer or the solution you want. So I have a prompt for you guys. So let's test it on TPT and evaluate its performance. So let me just enter the prompt. And it starts generating.


00:22:32
So it says that to create a chatbot using machine learning algorithms. Here's a step-by-step guide to get you started. So as you can see on the screen, Charge has created a complete response based on the prompt we gave it. Next on our list, we have GitHub Copilot. GitHub Copilot is an AI powered code completion tool that suggests code snippet as you type. It leverages the massive data set of public code repositories to provide contextually relevant suggestion. So this can significantly boost productivity by


00:23:04
reducing the time spent on repetitive task. Copilot is particularly useful for writing code, generating unit test and exploring different coding approaches. So as you can see the screen, we are on the GitHub copilot site. GitHub Copilot generates unit test for a Python function that passes expenses. It includes test cases for valid input and empty input and ensures proper date formatting using the unit test framework. Copilot automatically assumes that the necessary imports like date time and unit test are in place.


00:23:39
Moving on to the next generative AI example that is DALI. DALI is a text to image AI model capable of generating highly realistic and creative images based on your textual description. With its ability to produce images in various styles and resolution, Deli is a versatile tool for artists, designers, and content creators. It can be used to generate concept art, product designs, illustrations and even entire visual stories. So as you can see the screen we are on the Dali site. Deli provides different functions related to


00:24:13
generation. So the first is from text and next is from images. And in the prompt section you write your prompt. Then select the number of images you want. And maybe you can also select the image style and the orientation you wish to which can be square, horizontal, vertical or panoromic. So you can simply click on the generate and image will be generated. So now let us try by giving prompt and then click on generate. So, Dali uses a powerful deep learning model to process your text prompt and generate an image that matches your


00:24:48
description. So, it can create images in various styles from realistic to abstract and can even generate images based on specific artistic movements or artists. The model is constantly being trained on a new data so it can generate increasingly accurate and creative photos over time. And next is Midjourney. Midjourney is a cuttingedge AI platform that transforms your words into stunning visuals. So with simply typing a description you can watch as a midjourney generates unique and captivating images that brings your


00:25:22
ideas to life. So whether you are an artist, designer or storyteller, midjourney offers a powerful tool to spark creativity and explore new visual horizons. Now we are on the midjourney site and on the left side the icon shows the different features of midjourney AI. Imagine you have a vivid ideas in your mind but you are struggling to express it visually. So midjourney is like a skilled artist who can interrupt your thoughts and translate them into stunning images. By simply providing a textual description you're giving


00:25:54
midjourney a creative prompt. The AI then uses its vast knowledge of art and visual styles to generate images that align with your vision. It's like having a personal artist at your disposal, ready to bring your imaginations to life. The next example of generative AI on our list is Jasper.ai. Jasper.ai is an AI writing assistant designed to help users create highquality content efficiently. It offers a wide range of features including content generation, rewriting, summarization, and translation. So,


00:26:28
Jasper.AI can be used for various content creation tasks such as blog posts, social media content, marketing copy, and email campaigns. It's particularly useful for writers who needs to produce large volumes of content quickly and efficiently. Now, we are on the Jasper site. So as you see in the navbar we have different sections like features, resources, solutions, enterprises, pricing etc. And Jasper.ai uses a powerful language model to understand and process your prompts. So when you provide a topic or a


00:27:04
specific request, Jasper.AI generates a relevant content by drawing from its vast database of information and creative writing techniques. It can adapt its style and tone to match your desired output, ensuring that the generated content is engaging and effective. Our next example is stable diffusion. So, stable diffusion is a textto image AI model that generates highly detailed and artistic images based on your textual description. With its ability to create images in various styles and resolutions, stable diffusion is a


00:27:37
popular tool for artists, designers, and content creators. It can be used to generate concept art, product designs, illustrations and even entire visual stories. Now we are at the site of stable diffusion so that you can see the different features on the navigation bar such as prompt database, pricing details and also the AI tools it includes. So the stable diffusion uses a powerful deep learning module to process your text prompts and generate an image that matches your description. So it can create images in various styles from


00:28:12
realistic to abstract and can even generate images based on specific artistic movements or artists. So the model is constantly being trained on new data so it can generate increasingly accurate and creative photos over time. And the next example is Adobe Firefly. Adobe Firefly is a generative AI platform that empowers users to create stunning images, graphics, and text effects with ease. So it offers a wide range of features including textto image generation, image editing and style transfer. Firefly is designed to


00:28:46
streamline the creative process and inspire new ideas and is particularly useful for artists, designers and marketers who need to create highquality visuals quickly and efficiently. So now we are at the Adobe Firefly site and if you click on the create and design so you can find sections like quick actions, create new featured products and artificial intelligence etc. So if you click on the products you will learn about the different products provided by Adobe. Adobe Fireflies uses a powerful deep learning model to


00:29:19
process your prompts and generate images, graphics or text effects. It can understand and interpret your textual description, allowing you to create custom visuals that matches your specific needs. Firefly also offers advanced features for editing and manipulating images such as removing backgrounds, changing colors, and applying artistic styles. By combining these capabilities, Firefly empowers users to create highquality visuals with minimal efforts. Next on our list, we have Gemini. Gemini is a powerful AI


00:29:53
model developed by Google that excels in a wide range of task including natural language understanding, text generation and language translation. It can process and generate text, code, images, and audio making it a versatile tool for various applications. Gemini is particularly adapt at complex reasoning, problem solving, and creative content generation. It can be used for tasks such as writing essays, translating languages, generating code and even creating art. Now we are at the Giny site and you can see different section


00:30:26
here on the left side and in the middle we have a text bar saying enter a prompt here where you're going to write your question or the prompt you want and press enter to get your answer. You can also see different icons in the text bar here. So the image icon is for uploading the image and the voice icon is for speaking. So if the prompt is bigger you can speak and it will convert your speech to text. So now let us try to upload an image and ask it to explain us. So let us take this and let us just write please explain


00:31:01
me about the image. So the Gemini functions by processing and analyzing large amount of data to understand and respond to prompts and question. It uses a complex neural network architecture that allows it to learn and adapt to new information. So when you provide a prompt, Jiminy process it and generates a response based on its understanding of the language, context, and available information. So it can also perform tasks such as translation, summarization, and creative writing. Moving on to our next example that is


00:31:38
Runway. Runway AI is a powerful platform that provides a suit of AI powered tools for creatives. It offers features such as image generation, video editing, and style transfer. Runway is designed to streamline the creative process and inspire new ideas. And it's particularly useful for artists, designers, and frame makers who need to create highquality visuals quickly and efficiently. Now we are on the site of runway AI and as you can see on the left side the runway provides many option and in the


00:32:10
library you can see here it provides workspace assets, favorite assets, custom elements and many more and in the tools section we have generative videos, generative audio, text to image and etc. Runway AI functions by leveraging advanced machine learning algorithms to process and manipulate visual data. Its tools can generate images from text description, edit and enhance existing videos and applying various artistic styles to images and videos and the platform offers a user-friendly interface that allows creatives to


00:32:45
experiment with different techniques and create unique visual content with ease. Now let us get started and enter some prompt. As you can see my screen over here, you can simply drop an image or video. So let us try to drop any video. So over here it gives an option to edit and also you have two options. Maybe you can keep it in the landscape or maybe the portrait. Over here you can simply crop your video. So it's processing. Now let's try to give an image. So here you can also crop the image if


00:33:33
you want. So if you simply click on portrait, it will change to portrait form and you can adjust it wherever you want, however you want. And then click on crop image. So there we go. Moving on to our last example of generative AI that is right sonic. Right AI is a powerful AI writing assistant designed to help users create highquality content efficiently and it offers a wide range of features including content generation, rewriting, summarization and language translation. Right can be used for various content


00:34:12
creation tasks such as blog post, social media content, marketing copy and email campaigns. It's particularly useful for writers who need to produce large volume of content quickly and efficiently. Now we are at the site of Right Sonic and you can see we have different features provided by Right Sonic such as creating content for advertising, general writing, video, e-commerce, social media, blog, emails and many more. And in the text box you write your prompt and just press create. Then it will redirect you to the next


00:34:46
page. Then select different parameters on which you want to create and press generate. So we will select the options as a tech professionals. Click next and it's focusing on education. So click next. And here we can go with anything. And then click on generate. Write sonic functions by using a powerful language model to understand and process your prompts. So when you provide a topic or a specific request, right sonic generates relevant content by drawing from its vast database of information and creative writing


00:35:28
techniques. It can adapt its style and tone to match your desired output, ensuring that the generated content is engaging and effective. Whether you are an artist, a writer, a developer, or just curious about the future of AI, the tools we have discussed offer endless opportunities to innovate and create. So whether you're looking to bring your ideas to life or just explore the cutting edge of AI technology, these generative AI tools are a great place to start. [Music] These generative AI tools can help you


00:36:04
save time, boost your creativity, and get more work done in less time. So whether you're a student, a professional, or just someone who wants to stay ahead of the curve, this video is for you. So let's dive in to learn more about the top 15 Gen AI tools together. And by the end of this video, you will know exactly which tools to use for your task. So we will explore tools that can help you generate ideas, write content, and even create art. You will learn how to automate repetitive task,


00:36:33
analyze data like a pro, and make production with ease. So we will also cover some fun tools that will make you wonder how you ever lived without them. So let's get started. First on our list, we have Chad GPD. Chat GPD developed by Open AI is a generative pre-trained transformer that is capable of understanding and generating humanlike text. It is one of the best tools for various applications in natural language processing. Chantity is everywhere these days, right? People from different industries and backgrounds use it for


00:37:04
all sort of things. Like for example, businesses use it to help out with customer support and to find new leads and to do market research. Also, the writers, bloggers, and content creators use it to come up with interesting ideas and to make their content super engaging. And developers and technologists even use chatb to build AI powered applications, chat boards and tools. So basically, Chad Gypty is used across many more domains for different purposes. It is one of the most commonly used tools. One of the best thing about


00:37:36
Chajipy is that it gives free access to AI content development. So to use Sajipity, simply open the interface and input a prompt or question. It can be about anything. Like for example, if you are a programmer and you want to understand how a program works, you can paste your code into chipd and ask it to explain. Let me take an example to find a factorial of a number. So once you copy paste your program just type in explain the program and here we go. It has explained the program step by step. So just like that


00:38:15
you can get assistance with writing content on any topic and if you're not satisfied with its response you can request another one and it will provide you with as many responses as you need. Customer support representatives, content creators, developers, educators and researchers use tagged to generate text, answer questions or engage in conversation in various applications and platforms. Let's move on to the next tool on our list and that is BR. Bard is a generative AI chatboard developed by


00:38:46
Google which is now called Gemini. It is designed for creative writing task and is built on Lambda, a transformerbased model. Bart can help you do tasks such as coding and solving math problems and as well as write, plan and learn more. You can also generate images through the Gemini. It recognizes speech in over 100 languages and help with audio translation as well. So without the need for any extra tools, Gemini can understand tricky visuals like charts and diagrams all by itself. And it is also the best tool for describing images


00:39:20
and answering questions about them. So to get access to Gemini, you need to sign into your Google account and start chatting with Gemini. You can provide prompts with various inputs such as text, images, videos, or even code. You will also have the options to integrate with Google apps such as Gmail, Docs, Sheets, and more. So let's try it out. So imagine you have an image or a diagram that needs explaining. So all you need to do is click upload image. and select the image from your device and type explain the diagram.


00:40:08
And there you have it. It provides explanation for all the six cases and it's really easy to use, right? And as I mentioned earlier, it can also generate an image. So let's try it out. Create a picture of a playful puppy. So as you can see the screen Gemini has generated images and if you're not happy with these images you can ask to generate more images. And a bar is used for content creation and writing. So it often used by writers, bloggers, marketers, educators and anyone who needs assistance with generating


00:40:52
content, brainstorming ideas or even improving their writing skills. The next tool that we have on our list is Burden. So Burden is a generative AI tool developed by Open AI and it's an AI assistance that simplifies repetitive task and effortlessly automate task within work app. There's no need for complicated builders or scripts. You just tell Burden what you need and it will do the task in no time. Burden learns from you to create personalized automation and it works with many apps and websites making your task easier.


00:41:25
Burden is a great tool for making work easier. So it is used by managers, sales team, recruiters and others who want to automate repetitive task and get more work done efficiently. And through burden you can connect data and actions from one app to automate task in another. Also, Burden works with many popular apps to make your work easier. Burden can also handle task in your apps and websites like filling out forms, sending messages or creating reports. It makes the task easy for you. And workflows are like chains of actions


00:41:57
triggered by you or changes in your app. So, they handle repetitive task for you, freeing up your time for more important things. The next generative AI tool on our list is Rephrase AI. Using this tool, you can effortlessly turn your text into videos. You can create professionallook videos with a digital avatar in minutes and no complex production is needed. It will convert text to video in just three easy steps. So, first pick your favorite digital avatar. Then add your message and finally render your video with Rephra


00:42:30
Studio. Simple, right? Refresh AI can handle over 40 languages, allowing you to create highquality videos and animations that a person can speak using only text as input. Refresh AI is used in various productions and content creations. Marketers and content creators often use it for social media platforms like YouTube and even business owner use it to generate videos content quickly and efficiently. Next, moving on to Cynthia. The next tool on our list. Synthesia is a number one AI video generation platform that effortlessly


00:43:04
transform text into videos. So you can produce studio quality videos with AI avatars and voiceovers in over 130 languages. And with over 160 readytouse options, you can create engaging video instantly. Choose from 160 characters to make your videos more interesting. Then customize avatar outfits and logos to represent your company and even make realistic conversations in the video to look more real. This tool will also create subtitles for your videos without doing it manually and allows you to


00:43:37
replicate your voice. It's just fun and professional to use. Synthesia is easy to use. Simply add your script, select the language, and click generate video. You can also customize the AI avatar, colors, fonts, and layouts to personalize the video. Finally, download the video in MP4 format or get a sharable link. Synthesia is used widely across industries to create engaging video content. So, it's used in learning, IT, sales, customer service, marketing and enterprise to streamline various task from training to


00:44:13
communication and advertising. Now moving on to Dali 2. It is an AI system from OpenAI that uses deep learning methodologies to produce realistic images and art just by describing them in natural language. Darling 2 learns from examples. So you can describe what you want in natural language and get different images and art. You can also make changes to existing pictures using the inpainting tool and replace a part of an image with AI generated imagery. Dali 2 is used in creating content generation and it's often used by


00:44:49
artists, designers, marketers, educators and developers who need assistance with generating visual content, creating art, designing graphics and exploring creative possibility using AI technology. Next, type studio. Type studio is a flexible editing tool ideal for podcast streams, interviews, and other type of content. It's a video editor that allows you to edit your video by simply editing the text transcript from the video. With its advanced AIdriven features, Type Studio streamlines the editing process,


00:45:24
enabling content creators, podcasters, and businesses to produce highquality videos and podcast with minimal efforts and time. Type Studio provides a range of features including autosubtitening, transcribing videos, converting video to articles, converting text to articles, translating subtitles, adding images to video podcast, voiceovers, and editing podcast. And next on our list, we have Descript. Descript is your all-in-one solution for writing, recording, transcribing, editing, collaborating,


00:45:59
and sharing your videos and podcast. In Descript, editing video feel as simple as working with documents and slides. Podcasters can effortlessly edit multitrack audio similar to editing a document. And screen recording are instantly captured, edited, and shared. Transcription is speedy and accurate with powerful correction tools. And finally, content can be repurposed into clips using templates, subtitles, and more. Descript offers a wide range of powerful AI features to elevate your video and podcast. Firstly, you can


00:46:33
seamlessly edit videos by editing text. Descript transcribes your video content, allowing you to make edits just as effortlessly as you would in a document. Secondly, with AI voice cloning, you can create ultra realistic AI voice clones and generate text to speech in a matter of seconds. Additionally, Transcript provides studio quality sounds with just one click through its studio sound features, which removes background noise and polishes up audio instantly. Lastly, the green screen effects allows you to


00:47:05
remove your video background and place your subject anywhere with a simple click. These features are helpful for a wide range of users, including content creators, podcasters, filmmakers, educators, and businesses. Next, moving on to Compose AI. Compose AI is an AI powered writing tool and a free Chrome plug-in that generates text using artificial intelligence. It helps you complete sentence as you type and speeds up the process of writing emails, creating documents and chatting. Also, it will help in rephrasing the


00:47:39
sentences. You can integrate this tool in your existing platforms and tools to speed up task and to save time. This tool is used to generate content, streamline writing processes to enhance productivity and to create engaging material for their audience. Marketers, content creators, writers and businesses utilizes this tool. The top 10 tool on our list is Chatson Sonic. Chatsonic is one of the leading alternatives of GPT on Google. So if you want insights on any of your PDFs, documents or links,


00:48:13
simply drag and drop the file into Chatson Sonic and ask for a quick summary. You can even chat with your images. Chatsonic accepts images as inputs and responds to any queries you have about them. But that's not all. It also creates AI images and delivers highquality output quickly. Sadsonic makes it easy to stay organized with advanced note takingaking. Keeps conversation flowing with smart flow of questions. Provide quick access with a Chrome extensions and lets you customize its tone and style. Simplifies team work


00:48:47
and fine-tune your prompt for better results. The top reasons chatonic excel include it allows real time Google search integration. Chat with your PDF and docs. Chat with your images. Summarize web pages. Generate humanlike voiceovers. Write in your brand voice. Also, it allows AI image generation and inbuilt fact checks and more. Chadsonic is used by professionals, businesses, educators, researchers and content creators to streamline task, enhance productivity and to leverage AI technology for various needs. Next, Tom.


00:49:24
With Tom, you can craft impactful presentation in less time. Its AIdriven features and personalized tools simplify the creation of sales and marketing materials that leave a lasting impression. It effortlessly generates presentations, pages, outlines, images, and even text with Tom's advanced AI capabilities. Tom is useful in sales, marketing, startups, product teams, and customer success. And it's a great starting point for any project. So, explore various projects or express your ideas promptly with Tom's versatile


00:49:59
templates. You can choose from founder, creative, design, product, sales, personal, marketing, education, and career. And next on our list, we have design.ai. With design.AI, you can produce logos, videos, banners, and mockups using AI in just 2 minutes. It just simplifies your workflows, save times, and also reduces cost with this platform. Design.ai AI is a platform that uses AI to help you create, edit, and scale content effortlessly. You will get all your content tools in just one place and


00:50:38
tools such as face swapper that the face swapper smoothly integrates your face into any picture or video. Logo maker that will help you launch your brand with a unique logo and full brand identity kit using AI generator. Next, image maker creates custom image effortlessly from your ideas with text to image generator and more tools such as video maker, speech maker, design maker, AI writer and more. And these creative tools are suitable for all content creators including influencer, small businesses, startups, educators,


00:51:14
enterprises, agencies and more. Now moving on to midjourney AI. So, MidJourney is a cool AI tool that blends art and tech. You can describe what you want in words and it turns those descriptions into sterning pictures. So, by using advanced algorithms, it turns words into unique art words. Midjourney uses machine learning algorithms to interpret and analyze the data. So, this data can be in the form of text, audio or image. And after getting this data, the algorithm will use it to generate new images, sounds or other media type


00:51:50
based on the patterns it finds in the data. Midjourney is used by many businesses, startups and organizations seeking assistance with website development, app creation, branding, user experience design and related services. And next we have GitHub Copilot. GitHub copilot is a code completion tool developed by GitHub which is owned by Microsoft and Open AI. So when provided with a programming problem in natural language, Copilot is capable of generating solution code and it is also able to describe input code


00:52:25
in English and translate the code between programming languages. It assists the users of Visual Studio Code, Visual Studio and Jet Brains integrated development environments by autocompleting code. It works best for users coding in Python, JavaScript, TypeScript, Ruby, and Go. And in March 2023, GitHub announced plans for Copilot X, which will incorporate a chatbot based on GPT4 as well as support for voice commands into C-Ilot. The last tool on our list is Alpha Code. By combining AI and computative


00:53:00
programming, it serves as a virtual assistant, guiding programmers through coding and providing smart suggestions. The features of alpha code is to leverage machine learning algorithms to analyze vast amount of code and learn from patterns. This enables it to generate optimized code solution and alpha codes machine learning models are trained on extensive code repositories enabling it to comprehend programming concepts and patterns. So this helps in analyze problems and generate optimized code snippets saving programmers time


00:53:31
and effort. With Alpha Code, programming learners and competitive programmers gain an access to a unique tool that enables them to explore problems solving strategies in a groundbreaking way. [Music] Now let us learn what is artificial intelligence. AI or artificial intelligence is the branch of computer science where focused on creating systems that were performed task and that would be normally required by human intelligence. These tasks can range understanding of natural language. Secondly, recognizing patterns then


00:54:10
making decisions and lastly learning from the experiences. AI is a collaboration of ideas, methods and knowledge where from the multiple academic disciplines work on a different problem solving and share their knowledge to a better understanding and come up with a good solution. But it can also be rule-based and operate under a set of rules and conditions only. So I would like to tell you all guys a small realtime experience or an experiment done by Alan Turing to propagate and to establish the artificial intelligence.


00:54:46
Alan Turing was the first person to conduct the sustainal research in the field that he called machine intelligence. The Turing test was conducted to explore whether machines could exhibit humanlike intelligence proposed by Alan Turing in 1950. It involves a human evaluator communicating with both a human and a machine through a text interface. If a evaluator cannot distinguish between the two based on their responses, the machine is set to have passed the test. It serves as a benchmark of the assessing the progress


00:55:20
of AI and discussions of the nature and intelligence and consciousnesses. So this is to evolve and to establish that even machines can work as humans and that is how it is made to bring up the machine's knowledge and the humans knowledge into machines. There are two types of artificial intelligence. First let's talk about weak AI. Before getting into what is weak AI, I would like to tell you with an example that is performed in a real world. I know you'll all guys will be knowing about Alexa,


00:55:54
Apple Siri and also self-driving vehicles. So all of these considered as in weak AI. It is also known as a narrow AI or an AI narrow intelligent that is trained by the AI and focused to perform a specific task. Weak AI drives most of the AI that surrounds by us today. Narrow might be a more apt descriptor for this type of AI as it is anything but weak. Next, let us learn about strong AI. Strong AI is made up of artificial general intelligence or artificial super intelligence. Simply it could be told as where a machine would


00:56:33
have an intelligence equal to humans where humans will track what AI needs to be done. It would be able to self-aware with the consciousness that would be this ability to solve problems, learn and also plan for the future. Have you ever thought what does AI do at its core? I'm here to tell you why AI is essential. It works by analyzing a lot of data to find patterns and a useful information. It even learns from this data to get better at a task over time. With this learning, AI can make decisions, predict future events and do


00:57:08
task automatically that would normally need human intelligence. This help business and other organizations work more efficiently. So AI is about making computers smarter and more helpful in everyday life. So let me tell you some use cases that is happening in daily users or daily real life based. Firstly I have taken is about cyber security as it is a very important and a vital role for many platforms hereafter. Cyber security is a critical concern for individual businesses and also governments. As cyber threats continue


00:57:43
to evolve in a complexity and sophistication. It would play a very vital role for argumenting cyber security defenses. It is all based on the false positive effects that is made by the false information given by any criteria leading the alert of fatigue and reduced operational efficiencies. Cyber attacks can exploit vulnerabilities in AI models by invading detection and compromising security defenses. they have some private data where it is trained on the basis of incomplete data sets may produce the


00:58:20
outcomes of private data. This will raise an ethical concern and also regulatory compliances issues having a very complicated problems too. So the next one will be your entertainment. As people know entertainment is taking a very huge part in everyone's life. For example, it would be a social media too. So AI is transforming the entertainment industry by revolutionizing content creation, personalization and audience engagement. So this can be such as television, gaming, music and also digital media platforms. One significant


00:58:55
use of AI in entertainment is personalized content recommendation. So personalized entertainment experiences are enhanced through AI driving. It is recommended through systems. They are even having some platforms like Netflix, Prime, Amazon and also Spotify that is making people engaged in a very hype as of now. These recommendations improve over time. So I would like to conclude by telling artificial intelligence can do amazing things like analyzing data and making task easier but it also brings up huge question about fairness,


00:59:32
jobs and who controls it. We need to be careful in how we develop and use the AI. Making sure it helps everyone and doesn't cause harm at all. So this makes easier for people to get into creativity and make rules and guidelines. [Music] History of artificial intelligence. The concept of AI goes back to the classical ages. Under Greek mythology, the concept of machines and mechanical men were well thought of. An example is Talos. Talos was supposedly a giant animated bronze warrior who was programmed to guard the


01:00:12
island of Cree. Now, let's get back to the 19th century. In 1950, Alan Turing proposed the Turing test. The Turing test basically determines whether or not a computer can intelligently think like a human being. The touring test was the first serious proposal in the philosophy of artificial intelligence. 1951 marked the era for game artificial intelligence. This period was called game AI because here a lot of computer scientists developed programs for checkers and for chess. However, these programs were later rewritten and redone


01:00:46
in a better way. 1956 marked the most important year for artificial intelligence. During this year, John McCarthy first coined the term artificial intelligence. This was followed by the first AI laboratory which was set up in 1959. MIT AI lab was the first setup which was basically dedicated to the research of AI. In 1960, the first robot was introduced to the General Motors assembly line. In 1961, the first AI chatbot called Eliza was introduced. In 1997, IBM's Deep Blue beats the world champion Gary Casprow in


01:01:24
the game of chess. 2005 marks for the year when an autonomous robotic car called Stanley won the DARPA Grand Challenge. In 2011, IBM's question answering machine Watson defeated the two greatest Jeopardy champions Brad Rutter and Ken Jennings. So, that was a brief history of AI. Now guys, since the emergence of artificial intelligence in 1950s, we have seen an exponential growth in its potential. AI covers domains such as machine learning, deep learning, neural networks, natural language processing, knowledge base,


01:01:59
expert systems, and so on. So now let's understand the different stages of artificial intelligence. So basically when I was doing my research I found a lot of videos and a lot of articles that stated that artificial general intelligence, artificial narrow intelligence and artificial super intelligence are the different types of AI. If I have to be more precise with you then artificial intelligence has three different stages, right? The types of AI are completely different from the stages of AI. So under the stages of


01:02:29
artificial intelligence we have artificial narrow intelligence, artificial general intelligence and artificial super intelligence. So what is artificial narrow intelligence? Artificial narrow intelligence also known as weak AI is a stage of artificial intelligence that involves machines that can perform only a narrowly defined set of specific tasks. Right? At this stage the machines don't possess any thinking ability. They just perform a set of predefined functions. Examples of weak AI include Siri, Alexa,


01:03:01
Alph Go, Sophia, the self-driving cars and so on. Almost all the AI based systems that are built till this date fall under the category of weak AI or artificial narrow intelligence. Next, we have something known as artificial general intelligence. Artificial general intelligence is also known as strong AI. This stage is the evolution of artificial intelligence wherein machines will possess the ability to think and make decisions just like human beings. There are currently no existing examples of strong AI, but it's believed that we


01:03:34
will soon be able to create machines that are as smart as human beings. Strong AI is actually considered a threat to human existence by many scientists. This includes Stephen Hawkings. Stephen Hawings quoted that the development of full artificial intelligence could spell the end of human race. Moving on to our last stage which is artificial super intelligence. Artificial super intelligence is that stage of AI when the capability of computers will surpass human beings. Artificial super intelligence is


01:04:05
currently seen as a hypothetical situation as depicted in movies and science fiction books. You see a lot of movies which show that machines are taking over the world. All of that is artificial super intelligence. Now I believe that machines are not very far from reaching the stage taking into consideration our current pace. However, such systems don't currently exist. Right? We don't have any machine that is capable of thinking better than a human being or reasoning in a better way than


01:04:32
a human. Artificial super intelligence basically any robot that is much smarter than humans. Now moving on to the different types of artificial intelligence. Based on the functionality of AI based systems, artificial intelligence can be categorized into four types. The first type is reactive machines AI. This type of AI includes machines that operate solely based on the present data and take into consideration only the current situation. Reactive AI machines cannot form inferences from the data to


01:05:04
evaluate any future actions. They can perform a narrowed range of predefined tasks. An example of reactive AI is the famous IBM chess program that beat the world champion Gary Caspro. This is one of the most impressive AI machines built so far. Next, we have limited memory AI. Now, like the name suggests, limited memory AI can make informed and improved decisions by studying the past data from its memory. So such an AI has a short-lived or you can say a temporary memory that can be used to store past


01:05:38
experiences and hence evaluate your future actions. Self-driving cars are limited memory AI that use the data collected in the recent past to make immediate decisions. For example, self-driving cars use sensors to identify civilians that are crossing the road. They identify any steep roads or traffic signals and they use this to make better driving decisions. This also helps in preventing any future accidents. Next, we have something known as theory of mind artificial intelligence. The theory of mind AI is a


01:06:11
more advanced type of artificial intelligence. This category is speculated to play a very important role in psychology. This type of AI will mainly focus on emotional intelligence so that human beliefs and thoughts can be better comprehended. The theory of mind AI has not been fully developed yet, but rigorous research is happening in this area. Moving on to our last type of artificial intelligence is the self-aware artificial intelligence. So guys, let's just fold hands and pray that we don't reach the state of AI


01:06:43
where machines have their own consciousness and become self-aware. This type of AI is a little far-fetched, but in the future, achieving a stage of super intelligence might be possible. Geniuses like Elon Musk and Stephen Hawkins have constantly warned us about evolution of AI. So guys, let me know your thoughts in the comment section. Do you ever think we'll reach the stage of artificial super intelligence? Moving on to the last topic of today's session is the different domains or the different


01:07:12
branches of artificial intelligence. So artificial intelligence can be used to solve real world problems by implementing machine learning, deep learning, natural language processing, robotics, expert systems and fuzzy logic. Now guys, these are the different domains or you can say the different branches that AI uses in order to solve any problem. Recently AI has also been used as an application in computer vision and image processing. Right? For now, let me tell you briefly about each of these domains. Machine learning is


01:07:42
basically the science of getting machines to interpret, process, and analyze data in order to solve real world problems. Right? Under machine learning, there's supervised, unsupervised, and reinforcement learning. If any of you are interested in learning about these technologies, I'll leave a link in the description box. You all can go through that content. Next, we have deep learning or neural networks. So, deep learning is a process of implementing neural networks on highdimensional data to gain insights


01:08:09
and form solutions. It is basically the logic behind the face verification algorithm on Facebook. It is the logic behind the self-driving cars, virtual assistants like Siri and Alexa. Then we have natural language processing. Natural language processing refers to the science of drawing insights from natural human language in order to communicate with machines and grow businesses. So an example of NLP is Twitter and Amazon. Twitter uses NLP to filter out terroristic language in their tweets. Amazon uses NLP to understand


01:08:42
customer reviews and improve user experience. Then we have robotics. Robotics is a branch of artificial intelligence which focuses on the different branches and applications of robots. AI robots are artificial agents which act in the real world environment to produce results by taking some accountable actions. So I'm sure all of you have heard of Sophia. Sophia the humanoid is a very good example of AI in robotics. Then we have fuzzy logic. So fuzzy logic is a computing approach that is based on the principle of degree of


01:09:15
truth instead of the usual modern logic that we use which is basically the boolean logic. Fuzzy logic is used in medical fields to solve complex problems which involve decision making. It is also used in automating gear systems in your cars and all of that. Then we have expert systems. An expert system is an AI based computer system that learns and reciprocates the decision-making ability of a human expert. Expert systems use if then logic notions in order to solve any complex problem. They do not rely on


01:09:48
conventional procedural programming. Expert systems are mainly used in information management. They are seen to be used in fraud detection, virus detection, also in managing medical and hospital records and so on. [Music] So let us understand when we have machine learning why do we need deep learning that is we'll look at various limitations of machine learning. Now the first limitation is high dimensionality of the data. Now the data that is now generated is huge in size. So we have a very large number of inputs and outputs.


01:10:25
So due to that machine learning algorithms fail. So they cannot deal with high dimensionality of data or you can say data with large number of inputs and outputs. Now there's another problem as well in which it is unable to solve the crucial AI problems which can be natural language processing, image recognition and things like that. Now one of the biggest challenges with machine learning models is feature extraction. Now let me tell you what are features. So in statistics we consider features as variables. But when we talk


01:10:50
about artificial intelligence these variables are nothing but the features. Now what happens because of that the complex problems such as object recognition or handwriting recognition becomes a huge challenge for machine learning algorithms to solve. Now let me give you an example of this feature extraction. Suppose if you want to predict that whether there will be a match today or not. So it depends on a various features. It depends on the whether the weather is sunny, whether it is windy all those things. So we have


01:11:14
provided all those features in our data set. But we have forgot one particular feature that is humidity. And now our machine learning models are not that efficient that they will automatically generate that particular feature. So this is one huge problem or you can say limitation with machine learning. Now obviously we have limitation and it won't be fair that if I don't give you the solution to this particular problem. So we'll move forward and understand how deep learning solves these kind of


01:11:37
problems. Now as you can see that the first line on your slide which says that deep learning models are capable to focus on right features by themselves requiring little guidance from the programmer. So with the help of little guidance what these deep learning models can do they can generate the features on which the outcome will depend on and at the same time it also solves the dimensionality problem as well. If you have very large number of inputs and outputs you can make use of a deep learning algorithm. Now what exactly is


01:12:03
deep learning? Again since we know that it has been evolved by machine learning and machine learning is nothing but a subset of artificial intelligence and the idea behind artificial intelligence is to imitate the human behavior. The same idea is for the deep learning as well is to build learning algorithms that can mimic brain. Now let us move forward and understand deep learning what exactly it is. Now the deep learning is implemented with the help of neural networks. And the idea or the motivation behind neural networks are


01:12:29
nothing but neurons. What are neurons? These are nothing but your brain cells. Now here is a diagram of neuron. So we have dendrites here which are used to provide input to our neuron. As you can see we have multiple dendrites here. So these many inputs will be provided to our neuron. Now this is called cell body and inside the cell body we have a nucleus which performs some function. After that that output will travel through exxon and it will go towards the exxon terminals and then this neuron


01:12:54
will fire this output towards the next neuron. Now the studies tell us that the next neuron or you can say the two neurons are never connected to each other. There's a gap between them. So that is called a synapse. So this is how basically a neuron works like and on the right hand side of your slide you can see an artificial neuron. Now let me explain you that. So over here similar to neurons we have multiple inputs. Now these inputs will be provided to a processing element like our cell body.


01:13:21
And over here in the processing element what will happen? Summation of your inputs and weights. Now when it moves on then what will happen? This input will be multiplied with our weights. So in the beginning what happens? These weights are randomly assigned. So what will happen if I take the example of X1? So X1 multiplied by W1 will go towards the processing element. Similarly x2 and w2 will go towards the processing element and similarly the other inputs as well and then summation will happen


01:13:48
which will generate a function of s that is f of s. After that comes the concept of activation function. Now what is activation function? It is nothing but in order to provide a threshold. So if your output is above the threshold then only this neuron will fire otherwise it won't fire. So you can use a step function as an activation function or you can even use a sigmoid function as your activation function. So this is how an artificial neuron looks like. So a network will be multiple neurons which


01:14:13
are connected to each other will form an artificial neural network and this activation function can be a sigmoid function or a step function that totally depends on your requirement. Now once it exceeds the threshold it will fire after that what will happen it will check the output. Now if this output is not equal to the desired output so these are the actual outputs and we know the real outputs. So we'll compare both of that and we'll find the difference between the actual output and the desired


01:14:38
output. On the basis of that difference, we are again going to update our weights and this process will keep on repeating until we get the desired output as our actual output. Now this process of updating weight is nothing but your back propagation method. So this is neural networks in a nutshell. So we'll move forward and understand what are deep networks. So basically deep learning is implemented by the help of deep networks and deep networks are nothing but neural networks with multiple hidden layers.


01:15:06
Now what are hidden layers? Let me explain you that. So you have inputs that comes here. So this will be your input layer. After that some process happens and it'll go to the next node or you can say to the hidden layer nodes. So this is nothing but your hidden layer one. So every node is interconnected if you can notice. After that you have one more hidden layer where some function will happen and as you can see that again these nodes are interconnected to each other. After this hidden layer two


01:15:34
comes the output layer and this output layer again we are going to check the output whether it is equal to the desired output or not. If it is not we are again going to update the weights. So this is how a deep network looks like. Now there can be multiple hidden layers. There can be hundreds of hidden layers as well. But when we talk about machine learning that was not the case. We were not able to process multiple hidden layers when we talk about machine learning. So because of deep learning we


01:15:59
have multiple hidden layers at once. Now let us understand this with an example. So we'll take an image which has four pixels. So if you can notice we have four pixels here among which the top two pixels are bright that is they're black in color whereas bottom two pixels are white. Now what happens we'll divide these pixels and we'll send these pixels to each and every node. So for that we need four nodes. So this particular pixel will go to this node. it will go to this node. This pixel will go to this


01:16:24
node. And finally, this pixel will go to this particular node that I'm highlighting with my cursor. Now what happens? We provide them random weights. So these white lines actually represent the positive weights and these black lines represents the negative weights. Now this particular brightness when we display high brightness, we'll consider it as negative. Now what happens when you see the next output or the next hidden layer, it'll be provided with the input with this particular layer. So


01:16:48
this will provide an input with positive weight to this particular node and the second input will come from this particular node. Since both of them are positive, so we'll get this kind of a node. Similarly, this node as well. Now when I talk about these two nodes, the first node over here, so this is getting input from this node as well as from this node. Now over here we have a negative weight. So because of that, the value will be negative and we have represented that with black color. Similarly over here as well we're


01:17:14
getting one input from here which has a negative weight and the another input from here which has again has a negative weight. So accordingly we get again a negative value here. So these two becomes black in color. Now if you notice what will happen next we'll provide one input here which will be negative and a positive weight which will be again negative and this will be also negative and a positive weight. So that will again come out to be negative. So that is why we have got this kind of


01:17:39
a structure. If you notice this this is nothing but the inverse of this particular image. When I talk about this node over here we are getting the negative value with a positive weight which is negative and a negative value with a negative weight which is positive. So we are getting something which is positive here. Now obviously I want this particular image to get inverse. I want these black strips to come up. So what I'll do I'll actually calculate the inverse by providing a negative weight like this. So over here


01:18:04
I've provided a negative weight it'll come up. So when I provide a positive weight so it'll stay wherever it is. After that it'll detect and the output you can see will be a horizontal image not a solid not a vertical not a diagonal but a horizontal and after that we are going to calculate the difference between the actual output and the desired output and we are going to update the weights accordingly. Now this is just an example guys. So guys this is one example of deep learning where what


01:18:27
happens we have images here we provide these raw data to the first layer to the input layer. Then what happens? These input layers will determine the patterns of local contrast or it'll fixate those patterns of local contrast which means that it will differentiate on the basis of colors and luminosity and all those things. So it'll differentiate those things and after that in the following layer what will happen? It'll determine the face features. It'll fixate those face features. So it'll form nose, eyes,


01:18:53
ears, all those things. Then what will happen? It'll accumulate those correct features for the correct face or you can say that it'll fixate those features on the correct face template. So it'll actually determine the faces here as you can see it over here and then it'll be sent to the output layer. Now basically you can add more hidden layers to solve more complex problem. For example, if I want to find out a particular kind of face for example a face which has large eyes or which has light complexion. So I


01:19:20
can do that by adding more hidden layers and I can increase the complexity also at the same time. If I want to find which image contains a dog. So for that also I can have one more hidden layer. So as and when hidden layer increases we are able to solve more and more complex problem. So this is just a general overview of how a deep network looks like. So we have first patterns of local contrast in the first layer. Then what happens? We fixate these patterns of local contrast in order to form the face


01:19:44
features such as eyes, nose, ears etc. And then we accumulate these features for the correct face and then we determine the image. So this is how uh deep learning network or you can say deep network looks like and I'll give you some applications of deep learning. So here are a few applications of deep learning. It can be used in self-driving cars. So you must have heard about self-driving cars. So what happens? It'll capture the images around it. It'll process that huge amount of data


01:20:09
and then it'll decide what action should it take. Should it take left, right? Should it stop? So accordingly it'll decide what action should it take and that will reduce the amount of accidents that happens every year. Then when we talk about voice control assistance, I'm pretty sure you must have heard about Siri. All the iPhone users know about Siri, right? So you can tell Siri whatever you want to do, it'll search it for you and display for you. Then when we talk about automatic image caption


01:20:31
generation, so what happens in this whatever image that you upload, the algorithm is in such a way that will generate the caption accordingly. So for example, if you have say blue colored eyes, so it'll display a blue colored eye caption at the bottom of the image. Now when I talk about automatic machine translation, so we can convert English language into Spanish, similarly Spanish to French. So basically automatic machine translation, you can convert one language to another language with the


01:20:56
help of deep learning. And these are just few examples guys. There are many many other examples of deep learning. It can be used in game playing. It can be used in many other things. And let me tell you one very fascinating thing that I've told you in the beginning as well. With the help of deep learning, MIT is trying to predict future. So yeah, I know it is growing exponentially right now guys. What is object detection? So object detection is a computer technology which is related to computer vision and image


01:21:27
processing. Here what we do in case of object detection is for a given image I'm going to check what are the objects that are present in the image and not only what I'm also going to say where exactly is that object is present. So that is what happens in place of object detection. An excellent example can be is Facebook photo tagging. So you might have observed the Facebook photo tagging feature. So when you upload a photo to your Facebook, so the Facebook will automatically recognize the people who are present in that


01:22:10
particular uploaded image and along with that it will also do one important thing. Along with that it will also show you as where exactly that person is present. So if they like it is going to draw a bounding box on top of that person's face to show it as hey this is your friend A and he's present over here do you want to tag him so that functionality that you would see in the Facebook that functionality that you see so that is a typical example of object detection now I'm not sure what is the latest


01:22:43
update because it's been more than 2 years me using social media but yeah so 2 years back that was there and I'm Sure today they it will be the same feature or it will be an upgraded feature but the main concept is this when you upload something for your Facebook or any social media I'm giving it Facebook as a generalized concept because that's where I've seen so it is going to tell us who are all the people that are present in the image and also it is going to tell us as where that person is present and


01:23:15
that is what an object detection algorithm is going to do. It's going to identify the objects in the pres objects in a given image and it also tells us as where exactly that object is present. So what and where both of them will be identified in case of object detection. So here you can see the GIF animation that you would see over here on your on your left hand of the screen. So here it says what are the objects that are present in the image. So the objects are cap, bowel, Pepsi can and so on. Now


01:23:48
along with saying what are the objects it is also drawing a bounding box. So you can see a green box that has been drawn on top of each and every object to tell us as where that particular object is present. And here on the another image that we have on the right hand of the slide. So here uh it also it's also doing a similar activity. It is saying there's a retriever dog, there is an American Stanford terrier dog and there is just a common dog. Okay. Okay. And there is a a built terrier dog. So there


01:24:20
are so it has identified the dogs that are present in the image. And along with identifying what is the breed of the dog, it is also drawing a box. See it says this is where my dog is. This is where my dog is. This is where my dog is. So this is how any typical object detection application would work. So it detects what and also it also tells us as where exactly that particular object is present. Now the next question is where do we see these applications? So one obvious face recognition. So this


01:24:58
we have been familiar with Facebook. So where we upload an image and we can identify who are all the people that are present in the image. And even Google photos has a good face recognition feature. I mean, so we've been understanding about this. And even the Google photos has a good uh face recognition or yeah, face recognition over there. So when we upload all the pictures, so it's going to group all the photos together and it will ask us to name that particular one person and it will automatically tag those names to


01:25:32
all the similar people wherever that person is present. So that's also an application of face recognition and we can also use this object detection concept for the people counting to understand as uh how many people are present over there in a given scenario or another example can be is uh whether that person is wearing a mask or not. So wearing a mask or not. So I can do it and I can also I can also use this special ma I can also use this mask detection object detection for checking whether people are following the


01:26:10
covid-19 protocols that is social distancing. So all these are the applications of using object detection and even we can use it for industrial purpose. It could be for industrial quality checking to check whether the objects that has been done is a object that has being manufactured is it as per the standards are being expected by the industry or self-driving cars. So in in case of self-driving cars so the cars will identify what are the objects that are present in a given image okay or a given


01:26:44
video feed and on the basis of the object it's going to drive. So that's another application of self driving car application of object detection and the another application that I could see is a security the face ID that we have in our Apple iPhones. So that uh uses the object detection to identify who is present in that image and another thing is like identifying the objects on the road. I think you might have seen uh uh you might have seen the latest Mahindra XUV700. So they they have an functionality to


01:27:20
detect the uh like to they have the functionality to detect the objects that are in front of uh in front of the car and if they if the object is very close to the car it is going to automatically apply the brake. Now here the object detection obviously uh like to detect the objects in front of uh the car. So they are making use of object detection over there. So that's another application. Okay. So these are some of the applications of using object detection in real life. Now coming to this object detection,


01:27:53
let's look at the typical workflow and how does it work when we want to build an object detection algorithm using tensorflow. When we are working with the applications like object detection, first we need to prepare the training data. I'll have to build a training data to such that to identify whether my given image has the required number of classes. So I'll have to build my classifier to identify as what is that individual object is present or what is my individual object contains. Now after


01:28:28
I have trained this so this is my training data. I have the images of cars and I have the images of bike and I have trained my machine learning model to identify whether my given image contains this car and bike. Now once it's trained I can obviously go ahead and send in my test data and with the test data I can detect whether my given input image contains bike or car. Okay. So this is the first step in preparing the object detection. Okay. First I'll build a classifier which will help me in


01:28:58
identifying what is present in the image. Now along with this what while training I'll also tell my machine learning or in this case deep learning system to tell us where exactly that object is present. So my machine learning model or this in this scenario it's a deep learning model. My deep learning model will do two things in parallel. one it will identify what and the another thing it will also do is where it will learn where exactly that object is present in that given image. So it does both the things in parallel.


01:29:34
So what happens is like I'll have some set of layers over here and from this layer another set of another another series of layers for detection to identify what is present over there and another set of uh layers for detect for detecting where exactly that object is present. Okay. So that's what it will be the workflow of object detection. Now once I have trained my object detection model I can send in a new image. Now when I send a new image my deep learning model will be able to identify what is present in the image


01:30:11
and also it also give tells me as where exactly that object is present in the given image and that's how an object detection machine learning algorithm would work. Now in order to build this deep learning models which is capable of performing object detection we have various frameworks. Now one of the common framework that we would use is the TensorFlow. So this is one of the very common uh framework that we are currently using which are in the latest scenario because of its uh ease of deployment using the TensorFlow extend.


01:30:45
So because of this functionality so now industries are preferring to use this TensorFlow framework. Now apart from this we have PyTorch, MXNet, Theo. So these are the various frameworks that are also available to us to build deep learning neural network. Now let's get an understanding about this TensorFlow and what does this TensorFlow is made of. Now as I mentioned already tensaflow is a deep learning framework which helps us in building the deep learning neural network. Now just like my numpy library


01:31:19
has the array object which is one of the basic uh basic object from this numpy library in case of tensorflow library or you can take any uh deep learning frameworks we have an object that's called as tensars. Now this tens are the standard way of representing the data in case of deep learning. The reason that we used to prefer representing the data with tensors because one it will help us to process the data with GPU. GPU means graphical processing unit which we use it for gaming purpose. Okay, you know already what is computer


01:32:01
gaming. So one like to in order to get the speed and in order to get the good graphics we know that we actually use the best best and the best uh graphics card so that we can play the game in a very high setting. Now using this deep learning frameworks and by creating the data inside that tensas I'll be having the ability I mean I'll be having the ability to process those data that I have using these GPU cores. So just like we have uh a course in CPU, we'll also have the processing course in GPU and


01:32:36
we'll be utilized those GPU cores and we have seen that we could get around 10 to 20x faster than we could than what we could achieve in our uh CPU. So that's the advantage of working with deep learning frameworks and creating the data as tensors. And if I have a CUDA supported GPU then I can also make use of this GPU cores to get 10 to 20 times of faster data processing compared to CPU. So that's the reason we we want to gen we want to represent the data in terms of tensas. Now what is this tensas? Tensas are just


01:33:12
a multi-dimensional array object. It's just an extension of two dimensional tables uh to data with higher dimension. So that's what these tensas are. Now in this tensaflow the computation is approached as a data flow diagram. So whenever I want to do anything over there in tensaflow so normally what we do if you remember in case of any machine learning model I'm going to represent how my yhat is computed and then from my yhat I'm going to calculate the cost. So I'll have to manually


01:33:44
specify how the how the cost should be done and I have to mention as how the gradient should be calculated with reference to every parameter that I am that I'm currently having in my model. But in case of TensorFlow I don't have to worry about it. I can let my TensorFlow to do the magic. I can let my TensorFlow to track what are the operations that I'm doing on the data. And once I've completed the operation, I can just say tell to my system as okay, I have finished doing the operation. Why


01:34:14
don't you update the parameters? So in order to do that, we actually make use of data flow graph or sometimes we also call as tensorflow graph. So it's going to track how the data is getting manipulated. Now once I have done this data manipulation, I can make use of this TensorFlow graph to update the parameters of my model during the operation. And that's the advantage of using deep learning framework. Now because of this feature, I don't have to worry about writing the complex


01:34:42
equation. I can just start building the models which are as as much as complex that I want and I can let my TensorFlow to do the magic and find the gradients and update the parameters with the with just a single command. So that's the advantage of working with this TensorFlow. And we have the similar functionality in PyTorch library. So it's not just advantage in tensaflow. I'm just giving as an example as tensaflow because we are discussing a tensaflow. The similar functionality is


01:35:11
also available in pytorch as well. Okay. And yes we represent the data in tensas and once we represent the data in tensas. So here it is just a flow over here. So here it it the flow says okay for the given data I'm going to add something and I'm going to perform matrix multiplication. So matimal is nothing but the matrix multiplication. So I'm doing some matrix multiplication and I'm getting the result and on the basis of result if I want to update something I can just go ahead and


01:35:41
perform some update using the uh TensorFlow data graph that I have and then find the parameters over there and I can do my required activity as per my preference. So what we do in case of building the model is we start with the input data. we are going to convert it into the tensars and once we convert the data into the tens using TensorFlow library we'll start training the model with Tensaflow model okay so here the TensorFlow model is nothing but the deep learning model which I've created with


01:36:14
Tensaflow library now once that is done I can go ahead and like start training the model in depth so here I'll create it as tensars and then I'm going to train my TensorFlow model which is a deep learning model and once the model has been trained I can send in a new data which is called as test data to test how my model is performing and once the testing is done once I'm satisfied with the output obviously I can use those test results to tell us as okay so this is an example of object detection so for a given image


01:36:52
this is where the objects are and this is these these are the objects So person, dog and horse. So there are three objects that are present over here and the bounding. This is a typical flow when it comes to performing object detection. Now here as a hands-on demo, we are going to look into a demo of YOLO model. Okay. So YOLO is one of the most commonly used uh object detection model when it comes to object performing the object detection using deep learning models. So it's an object detection


01:37:26
models. We'll use this already trained YOLO model and we'll send in our data and we'll see how the output would look like when I perform object object detection with this YOLO. Okay, so let's go back to our Google Collab. Okay, now let's have a quick look at the implementation of TensorFlow. So this is a notebook that I've just shared in the chat window. You can refer that notebook and this notebook has the implementation of object detection with YOLO. Now here to get started we are


01:38:01
going to get the repository from the GitHub. So this is going to clone my repository which is present over there that is the darknet repository and then I'm going to go inside the directory that is cd darknet and this is going to display the various files that we have in that directory. Now once that is done we are going to execute this command of make. So this will compile my library and this is going to make sure that it is ready for us to execute. So this would take a while depending on the


01:38:30
computer speed that we are running in. Now in this scenario I'm using the free system that is provided by Google collab. Hence this would take a while um for for it to become ready. And once that is done I'm going to get the trained weights or already trained weights from the GitHub repository. So this is actually available under this link. So I'm going to download those weights. So here the weights are nothing but the already trained data. So I'll use the already trained data and uh I would get


01:39:02
that data and once I get the data I can just go ahead and test my object detector. Now here for the testing purpose I'm going to make use of the uh image that is called as person's image which is present inside my data directory. Okay, I'll show you what is that image that I'm talking about. So data. So there are various images. Now this time I'm using this image of person. So this is the image of person. So on this image I'm going to identify what are the objects that are present in this


01:39:37
given image. So let's see what will be the output if I just execute this line. So this is going to tell me this would take a while to perform the prediction. Okay. And then it will give us the detection. Okay. So we have the output and if you observe over here like currently I'm running on CPU. So it actually took 21,915 milliseconds. So I'll just note this number in a separate notepad. So this is a mill this is the 21,50 milliseconds it took on my CPU. All right. And if I come down so this is the


01:40:30
uh predictions that has been given from my object detection model of YOLO. So it says where exactly that person is president. It also displays about the uh class name. Here the class name is says as person and along with that it also displays the confidence score. So how much confident it is u to how much confident it is to saying it as okay it belongs to the person. Okay. And it says 98% confidence that belongs to the horse. And this is 99% confident that it belongs to the dog classes. Now we are going to enable the GPU. And


01:41:10
once we enable the GPU, we'll have to execute some commands again and prepare our model to make sure that it runs on GPU. And once that is done, this time we are going to use a new image. The new image is called as giraffe. JPG. So this is the image that I'll be testing out. So this is the image of giraffe and I'll be using this image to detect the objects. So I'll just execute this to run the prediction and I'm going to show how the objects would look like in this image. Okay, it's still configuring my system


01:41:50
to to make sure that it runs on the GPU. Now the system setup is complete. Now let's look at the logs. Now if I come down and look at the logs, it just took less than a second to complete the prediction. So it just took around 165 milliseconds. See, you can look into the comparison. Now if I just compare it. Okay, 21 915 divided by 165. So it's like 132 times faster than what I have got uh from my CPU. So that's the computation speed that we would get when we work with uh GPUs. Okay. Now if I see over here, so


01:42:36
it says these are the object that has been identified from a given image. So there's a giraffe with 100% confidence and there is a zebra with 99% confidence. So clearly we would have we would get the high-speed combination when we are working with GPUs. Now below we have a code to test out about the various threshold values. So for that we are choosing an example image of hots. And on this example image of horse. So uh by default it has some threshold value. I think the threshold value is somewhere


01:43:08
around 0.8 or something. Now if I reduce the threshold value it is going to add some more images. And if I increase the threshold values, it is going to stop like displaying about the this in this example I have mentioned my threshold as 0.98. That means detect the objects for which you are confident with 98%. So here in the above image I had an I had an object which had 90% confidence and that has been omitted when I specify this threshold of 98%. And if I reduce the threshold value then in such scenarios it would randomly


01:43:48
assign the values like this because the threshold is very less. Okay. So that is how we can make use of tensorflow and uh start that is how we can make use of tensorflow and detect the objects in an image. And if I come down there is a code which talks about as we can perform the object detection in videos. Now instead of connecting to the videos we can also make use of openc library and detect the uh starter webcam session and then we can uh run this uh run this object detection on the video file as well or we can run that uh


01:44:27
object detection on the on the webcam stream as well to detect what are the objects that are present in the image. Now in order to do that obviously we need a system which has a very good memory and the GPU and you are free to check out at your end when you're exploring about videos. Okay. So the overall the identification or the overall understanding that you need to have is when I talk about object detection. So object detection means for a given input data I'll be doing two things. one I'll be uh detecting where


01:45:05
is my object is present. Okay, I'll detect where is my object and what is my object. I'll detect both the things when I'm performing object detection. So what and where which is which is like what we have seen in this video. So in our current scenario, so we have made use of YOLO object detector to detect the OB objects from our end and when you start your learning journey with TensorFlow obviously you'll learn how you can create your own object detection model to identify the objects


01:45:42
in a given image. So you learn that as well as you progress in your learning journey when you start exploring TensorFlow and when you try to do some task on your own. Okay. So that's how you can plan out in your learning journey. [Music] So let's understand how exactly a computer reads an image. So this is an image of New York skyline. I personally love this picture. So when a human will see this image, he'll first notice there are a lot of buildings and different colors and stuff like that. But how a


01:46:14
computer will read this image? So basically there will be three channels. One will be red, another will be green and finally we have blue channel which is popularly known as RGB. So each of these channels will they have their own respective pixel values as you can see it over here. So when I say that image size is B cross A + 3 it means that there are B rows A columns and three channels. All right. So so if somebody tells you that the size of an image is 28 + 28 + 3 pixels it means that it has 28 rows 28 columns and three channels.


01:46:45
So this is how a computer sees an image. And this is for colored images. For black and white images we have only two channels. So let's move forward and we'll see why can't we use fully connected networks for image classification. So consider an image which has 28 + 28 cross 3 pixels. So when I feed in this image to a fully connected network like this then the total number of weights required in the first hidden layer will be 2352. You can just go ahead and multiply it yourself. All right. But in real life the images


01:47:11
are not that small. All right. So whatever images that we have they are definitely above 200 + 200 + 3 pixels. So if I take an image which has 200 cross 200 cross 3 pixels and I feed it to a fully connected network. So at that time the number of weights required at the first hidden layer itself will be 120,000 guys. So we need to deal with such huge amount of parameters and obviously we require more number of neurons. So that can eventually lead to overfitting. So that's why we cannot use fully connected network for image


01:47:37
classification. Now let's see why we need convolutional neural networks. So basically in convolutional neural network a neuron in the layer will only be connected to a small region of the layer before it. So if you consider this particular neuron which I'm highlighting right now is only connected to three other neurons unlike the fully connected network where this particular neuron will be connected to all these five neurons. Because of this we need to handle less amount of weights and in


01:47:59
turn we need less number of neurons as well. So let us understand what exactly is convolutional neural network. So convolutional neural networks are special type of feed forward artificial neural networks which is inspired from visual cortex. So visual cortex is nothing but a small region in our brain brain which is present somewhere here where you can see the bulb and basically what happened there was an experiment conducted and people got to know that visual cortex is small regions of cells that are sensitive to specific regions


01:48:25
of visual field. So what I mean by that is for example some neurons in the visual cortex fires when exposed to vertical edges. Some will fire when exposed to horizontal edges. Some will fire when exposed to diagonal edges. And that is nothing but the motivation behind convolutional neural networks. So now let us understand how exactly a convolutional neural network works. So generally a convolutional neural network has three layers. Convolution layer, relu layer, pooling layer and fully connected layer. We'll understand each


01:48:49
of these layers one by one. We'll take an example of a classifier that can classify an image of an X as well as an O. So with this example we'll be understanding all these four layers. So let's begin guys. Now there are certain trickier cases. So what I mean by that is X can be represented in these four forms as well. Right? So these are nothing but the deformed images of X. Similarly for O as well. So these are deformed images. So even I want to classify these images either X or O. All


01:49:14
right? Because even this is X, this is X, this is X, this is X. But all these are deformed images but they are in turn X. Right? So I want my classifier to classify them as X. So basically that's what I want. So if you can notice here this is a proper image of an X and which is actually equal to this particular X which is a deformed image. Same goes for this O as well. So now what we are going to do is we know that a computer understands an image using numbers at each pixels. So what we'll do whatever


01:49:40
the white pixels that we have we are going to assign a value minus one to it and whatever black pixels we have we are going to assign a value one to it. When we use normal techniques to compare these two images one is a proper image of X and another is a deformed image of X. We got to know that a computer is not able to classify the deformed image of X correctly. Why? Because it is comparing it with the proper image of X. Right? So when you go ahead and add the pixel values of both of these images, you get


01:50:03
something like this. So basically our computer is not able to recognize whether it is an X or not. Now what we do with the help of CNN, we take small patches of our image. So these patches or these uh pieces are known as nothing but features or filters. So what we do by finding rough feature matches in roughly the same positions in two images, CNN gets a lot better at seeing the similarity between the whole image matching schemes. What I mean by that is we have these filters, right? We have these filters that you can see. So


01:50:30
consider this first filter. This is exactly equal to the feature or the part of the image in the deformed image as well. So this is our proper image and this is our deformed image. All right. So this particular filter or this particular part of the image is actually equal to this particular part of the image. Same goes for this particular feature of filter as well. And similarly, we have this filter as well which is actually equal to this particular part of the uh deformed image. All right. So let's move forward


01:50:53
and we'll see what all features that we'll be taking in our example. So we'll be considering these three features or filters. This is a diagonal filter. This is again a diagonal filter and this is nothing but a small. So we'll take these three filters and we'll move forward. So what we are going to do is we are going to compare these features the small pieces of the bigger image. We are going to put it on the input image and if it matches then the image will be classified correctly. Now we'll begin


01:51:16
guys. The first layer is convolution layer. So these are the beginning two steps of this particular layer. First we need to line up the feature in the image and then multiply image by the corresponding feature pixel. Now let me explain you with an example. So this is our first diagonal feature that we'll take. We are going to put this particular feature on our image of X. All right. And we are going to multiply the corresponding pixel value. So one will be multiplied with one. we'll get


01:51:38
one and we'll put it in another matrix. Similarly, we are going to move forward and we're going to multiply minus1 with minus1. We're going to multiply minus1 with minus1 as you can see. Similarly, we multiply this result -1 into -1. Then again -1 into -1. So, we are going to complete this whole process when we going to finish up this matrix. All right. And once we are done finishing up uh the multiplication of all the corresponding pixels in the feature as well as in the image, we need to follow


01:52:02
two more steps. We need to add them up and divide by the total number of the pixels in the feature. So what I mean by that is after the multiplication of the corresponding pixel values what we do we add all these values we divide it by the total number of pixels and we get some value right and then now our next step is to create a map and put the value of the filter at that particular place. We saw that after multiplying the pixel value of a feature with the corresponding pixel value of with that


01:52:27
of our image we get the output which is one. So we place one here. Similarly we are going to move this filter throughout the image. Next up, we are going to move this filter here. After that, we are going to move it here, here, here, everywhere on the image. We are going to move it and we going to follow the same process. All right. So, yeah, this is one more example where I've moved my filter in between. And after doing that, I've got the output something like this 1 one minus one and all. So, over here,


01:52:51
if you notice, I've got couple of times minus one as well. Due to which my output that comes is 0.55, right? So, I'm going to place 0.55 here. Similarly, after moving the pixel, after moving the filter throughout the image, I got this particular matrix. All right? And this is for one particular feature. After performing the same process for the other two filters as well, I've got these two values. So, we have these three values after passing through the convolution layer. Let me give you a


01:53:17
quick recap of what happens in convolution layer. So, basically we have taken three features. All right? And one by one we'll take one feature move it through the entire image and when we are moving it at that time we are multiplying the pixel value of the image with that of the corresponding pixel value of the filter adding them up dividing by the total number of pixels to get the output. So when we do that for all the filters we get we got these three outputs. All right. So let's move


01:53:39
forward and we'll see what happens in rel layer. So this is relu layer guys and people who have gone through the previous tutorial actually know what it is. So let me just give you a quick introduction of relu layer. So relu is nothing but an activation function. All right. Right? So what I mean by that is it will only activate a node if the input is above a certain quantity. While the input is below zero, the output is also zero. All right? And when the input rises above the certain threshold, it


01:54:03
has a linear relationship with the dependent variable. Now I'll explain you with an example. We have a graph of Reo function here. So my function says that when f ofx is equal to 0, if x is less than zero and it is equal to x when x is greater than zero. All right. So whatever values that I have which are below zero will actually in turn become zero and whatever values that are above zero our function value will also be equal to that particular value. So f of x will be equal to x if it is greater


01:54:30
than or equal to 0 and it will be zero if it is less than 0. So if I have x value is minus 3 so definitely it is less than 0. So f of x becomes zero. Similarly if I have minus 5 x value then again it is less than 0. So my f of x value becomes zero. But when I consider three as my x value then my f of x becomes equal to x which is nothing but three. So over here I'll have three. Again if I take my x value as five then obviously it is greater than or equal to zero. Then my f of x becomes equal to x.


01:54:59
So my f of x value becomes five. So this is how a relu function works. So why are we using relu function here is we want to remove all the negative values from our output that we got through the convolution layer. So we'll only take the first output that we got by moving one feature throughout the image. So uh this is the output that we have got for only one filter. All right. So over here I'm going to remove all negative values. So over here you can see that it it was minus.11 before and I've converted that


01:55:24
to zero. Similarly I'm going to repeat the whole process for the entire matrix and once I'm done with that I get this particular value. Now remember this is only for the output that we got through one feature. All right. So when we were doing convolution at that time we were using three features right? So this is output only for one filter. After doing it for the output of the other two filters as well, we have got these two values more. So totally we have these three values after passing through RLU


01:55:47
activation function. Next up we'll see what exactly is pooling layer. So in pooling layer what we do? We take a window size of two and we move it across the entire matrix that we have got after passing through layer and we take only the maximum value from there so that we can shrink the image. So what we are actually doing is we are reducing the size of our image. Earlier we had 7 + 7 matrix and now we have reduced that to 4 + 4 matrix. So after doing that for the entire image we have got this as our


01:56:16
output. This output we have got after moving our window throughout the image that we have got after passing through relu layer. Right? And when we repeat this process for all the three outputs that we have got after the relu layer then we get this particular output after pooling layer. Right? So basically we have shr our image to a 4 + 4 matrix. Now comes the tricky part. So what we are going to do now is stack up all these layers. So we have discussed convolution layer, relu layer and pooling layer. So I'll just give you a


01:56:44
brief recap of what all things we have discussed in convolution layer. What we did we took three features and then after that one by one we moved each filter throughout the image and when we were moving it we continuously multiplying the image pixel value with that of the corresponding filter pixel value and then we were dividing it by the total number of pixels. All right, with that we got three output. After passing through the convolution layer, then those three output we pass through a ReLU layer where we have removed the


01:57:10
uh negative value. All right, and after removing the negative value again we have got the three outputs. Then those three outputs we pass through pooling layer. So basically we're trying to shrink our image and what we did we took a window size of 2 +2 moved it through all the three outputs that we have got through relu layer and after doing that we were only taking the maximum value pixel value in that particular window and then we were putting it in a different matrix so that we get a shrink


01:57:34
image and after passing it through pooling layer we have got a 4 + 4 matrix and uh since we took three features in the beginning so therefore we have got three outputs after passing through pooling layer. All right, next up we are going to stack up all the layers. All right, so let's do that. So after passing through convolution, RLU and pooling, we have got this 4 + 4 matrix. This was our input image. Now when we add one more layer of convolution, ReLU and pooling, we have shrked our image


01:57:58
from 4 + 4 to 2 +2 as you can notice here. Now we are going to use fully connected layer. Now what happens in fully connected layer? The actual classification happens here guys. Okay, so what we are doing here is we are going to take the shr images and put it into a single list. So basically this is what we have got after passing through two layers of convolution rail and pooling and this is what we have got. So basically we are converting into a single list or a vector. How we do that? We take the first value one then we take


01:58:24
0.55 then we take 0.55 then we take one again then we take one then we take 0.55 0.55 then we again take 0.55 1 and 0.55. So this is nothing but a vector or you can say a list. If you notice here that there are certain values in my list which is high for x. And similarly if I repeat the entire process that we have discussed for O there'll be certain different values which will be high. So for X we have first fourth fifth 10th and 11th element vector values are high. For O we have second, third, 9inth and


01:58:57
12th element vector which are high. So basically we know now if if we have an input image which has first, fourth, fifth, 10th and 11th element vector values high, we know that we can classify it as X. Similarly, if our input image has a list which has the second, third, 9inth and 12th element vector values high, then we can classify it as zero. Now, let me explain you with an example. So, after the training is done after the after doing the entire process for both X and O, you know that our model is trained now. Okay. So, we


01:59:28
have given one new input image and that input image passes through all the layers and once it has passed through all the layers, we have got this 12 element vector. Now, it has 0.9.65 all these values. Right? Now how do we classify it whether it is an X or O. So what we do? We'll compare this with a list of X and O. Right? So we have got the list in the previous slide. If you notice we have got two different lists for X and O. We are going to compare this new input image list that we have got with that of X and O. Right? So


01:59:56
first let us compare that with X. Now as I've told you earlier as well for X there are certain values which will be high which is nothing but first, fourth, fifth, 10th and 11th value. Right? So I'm going to sum first, fourth, fifth, 10th and 11th value and I've got five. 1 + 1 + 1 + 1 and + 1. So 5 * 1 I've got five. And now I'm going to sum the corresponding values of my input image vector as well. So the first value is 0.9. Then the fourth value is 87. Fifth value is 96. 10th value is 89. And the


02:00:25
11th value is.94. So after doing the sum of these values, I've got 4.56. When I divide this by five, I got 91. Right? Now this is for x. Now when I do the same process for O. So in O if you notice I have second, third, 9th and 12th element vector values as high. So when I sum these values I get four and when I do the sum of the corresponding values in my input image I've got 2.07. When I divide that by four I got 0 51. And so now we notice that 91 is a higher value compared to 0.51. So we have when


02:00:57
we have compared our input image with the values of X we got a higher value than the value that we have got after comparing the input image with the values of O. So the input image is classified as X. All right. So now let us move towards our use case. So this is our use case guys. So over here what we are going to do is we are going to train our model on different types of dogs and cat images. And once the training is done we are going to provide it an input and it'll classify whether the input is


02:01:24
of a dog or a cat. Now let me tell you the steps involved in it. So what we are going to do in the beginning is obviously first we need to download the data set. After that we are going to write a function to encode the labels. Labels are nothing but the dependent variable that we are trying to predict. So in our training data and testing data obviously we know the labels right. So on that basis only we can train our model. So we are going to encode those label. After that we'll resize the image


02:01:48
to 50 cross 50 pixel and we are going to read it as a grayscale image. Then we are going to split the data 24,000 images for training and 50 for testing. Once this is done, we are going to reshape the data appropriately for TensorFlow. Now TensorFlow, I think everyone knows about TensorFlow. TensorFlow is nothing but a Python library for implementing deep learning models. Then we are going to build the model, calculate the loss. It is nothing but categorical cross entropy. Then we are going to reduce the loss by using


02:02:16
Adam optimizer with a learning rate set to 01. Then we are going to train the train the deep neural network for 10 epox. And finally we are going to make predictions. All right. So I'll just quickly open my pycharm and I'll show you the code how it looks like. So this is the code that I've written in order to implement the use case. In the beginning I need to import the uh libraries that I require. And once it is done what I'm doing I'm defining my training data and the testing data. So


02:02:41
train and test one uh contains both my training data as well as testing data respectively. Then I've taken my image size as 50 and learning rate I've defined here and I've given a name to my model. You can give whatever name you want. All right. So first thing that we saw we need to encode the dependent variable. That's what we are doing here. We are encoding our dependent variable. So whenever the label is cat then it will be converted to an array of 1 comma 0 and when it is dog it will be


02:03:07
converted to an array of 0a 1. So why we actually encoding the label? Because our code cannot understand the categorical variable. So we need to encode it. Right. Next what I'm doing is I'm resizing my image to 50 cross 50 and I am converting it to a grayscale image. Right? And once this is done I'm going to split my data set into training and testing parts. So yeah we are basically splitting the data set into two parts for training and testing. And here we are defining a model. So you can just I


02:03:34
can just go ahead and throw in a comment here. Building the model. Yeah. So this is where we are building the model. So basically what we have done here is we have resized our image to 50 + 50 + 1 matrix and that is the size of the input that we are using right this input that I'm talking about. Then what we have done a convolution layer we have defined with 32 filters and a stride of five with an activation function a rail and after that we have added a pooling layer match pool layer. Okay, again what we


02:04:05
have done we have repeated the same process but over here we are taking 64 filters and five stride passing it through a rail activation function and after that we have a pooling layer match pool layer then we have repeated the same process for 128 filters after that we have repeated for 64 filters then for 32 filters and after that we using a fully connected layer with 1024 neurons and finally we are using the dropout layer with key probability of8 to finish our model this is where our model is


02:04:33
actually finished and then what we are doing is we are using the atom optimizer to uh optimize our model. So basically whatever the loss that we have we are trying to reduce it and this is basically for your tensorboard. So we are creating some log files and then with that log file tensorboard will create a pretty fancy graphs for us that helps us to visualize the entire model and then what we are doing is we are trying to fit the model and we have defined epoch as 10 that is the number of iterations that will happen will be


02:05:02
10 and yeah so this is pretty much it model name we have given then input is x test to uh check the accuracy similarly uh the target will be y test labels associated with that test data will be y score test and which we have encoded basically. So this is how we are going to actually calculate the accuracy and we'll try to reduce the loss as much as possible in tenno. So till now our model is complete. We are done with it. Next what I'm doing is I'm feeding in some random input from the test data and I'm


02:05:33
validating whether my model is predicting it correct or not. All right. So I've already trained the model because it takes a lot of time and yeah I cannot do it here. So I've already trained the model and you can see that the loss that came after the 10th epoch is 2973 and the accuracy is somewhere around 88% which is pretty good guys and yeah and I've done the prediction on the test data as well. So let me just show it to you that. So this is the prediction that it has done on few of


02:06:00
the images in the test data. So yeah it is a cat predicted as cat cat predicted at cat cat and dogs as well. There are certain dogs as well. [Music] So this is the problem statement guys. We need to figure out if the bank notes are real or fake. And for that we'll be using artificial neural networks and obviously we need some sort of data in order to train our network. So let us see how the data set looks like. So over here I've taken a screenshot of the data set with few of the rows. In it data


02:06:32
were extracted from images that were taken from genuine and forged banknotelike specimens. After that wavelet transform tools were used to extract features from those images. And these are few features that I'm highlighting with my cursor. And the final column or the last column actually represents the label. So basically label tells us to which class that pattern represents whether that pattern represents a fake note or it represents a real note. Let us discuss these features and labels one by one. So the


02:07:00
first feature or the first column is nothing but variance of wavelength transformed image. The second column is about skewess. The third is courtesis of wavelength transformed image. And finally fourth one is entropy of the image. After that when I talk about label which is nothing but my last column over here if the value is one that means the pattern represents a real node whereas when value is zero that means it represents a fake node. So guys let's move forward and we'll see what are the various steps involved in order


02:07:25
to implement this use case. So over here we'll first begin by reading the data set that we have. We'll define features and labels. After that we are going to encode the dependent variable. And what is a dependent variable? It is nothing but your label. Then we are going to divide the data set into two parts. One for training, another for testing. After that we'll use TensorFlow data structures for holding features, labels, etc. And TensorFlow is nothing but a Python library that is used in order to


02:07:51
implement deep learning models or you can say neural networks. Then we'll write the code in order to implement the model. And once this is done, we will train our model on the training data. We'll calculate the error. The error is nothing but your difference between the model output and the actual output. And we'll try to reduce this error. And once this error becomes minimum, we'll make prediction on the test data and we'll calculate the final accuracy. So guys, let me quickly open my PyCharm and I'll


02:08:16
show you how the output looks like. So this is my PyCharm guys. Over here, I've already written the code in order to execute the use case. I'll go ahead and run this and I'll show you the output. So over here as you can see with every iteration the accuracy is increasing. So let me just stop it right here. So we'll move forward and we'll understand why we need neural networks. So in order to understand why we need neural networks we are going to compare the approach before and after neural networks and


02:08:45
we'll see what were the various problems that were there before neural networks. So earlier conventional computers use an algorithmic approach that is the computer follows a set of instructions in order to solve a problem and unless the specific steps that the computer needs to follow are known the computer cannot solve the problem. So obviously we need a person who actually knows how to solve that problem and he or she can provide the instructions to the computer as to how to solve that particular


02:09:12
problem. Right? So we first should know the answer to that problem or we should know how to overcome that challenge or problem which is there in front of us. Then only we can provide instructions to the computer. So this restricts the problem solving capability of conventional computers to problems that we already understand and know how to solve. But what about those problems whose answer we have no clue of. So that's where our traditional approach was a failure. So that's why neural networks were introduced. Now let us see


02:09:39
what was the scenario after neural networks. So neural networks basically process information in a similar way the human brain does. And these networks they actually learn from examples. You cannot program them to perform a specific task. They will learn from their examples from their experience. So you don't need to provide all the instructions to perform a specific task and your network will learn on its own with its own experience. All right. So this is what basically neural network does. So even if you don't know how to


02:10:06
solve a problem, you can train your network in such a way that with experience it can actually learn how to solve the problem. So that was a major reason why neural networks came into existence. So we'll move forward and we'll understand what is the motivation behind neural networks. So these neural networks are basically inspired by neurons which are nothing but your brain cells. And the exact working of the human brain is still a mystery though. So as I've told you earlier as well that


02:10:32
neural networks work like human brain and so the name and similar to a newborn human baby as he or she learns from his or her experience we want a network to do that as well but we wanted to do very quickly. So here's a diagram of a neuron. Basically a biological neuron receives input from other sources combines them in some way perform a generally nonlinear operation on the result and then outputs the final result. So here if you notice these dendrites these dendrites will receive signals from the other neurons. Then


02:11:03
what will happen? It will transfer it to the cell body. The cell body will perform some function. It can be summation. It can be multiplication. So after performing that summation on the set of inputs via exxon it is transferred to the next neuron. Now let's understand what exactly are artificial neural networks. It is basically a computing system that is designed to simulate the way the human brain analyzes and process the information. Artificial neural networks has self-arning capabilities that enable


02:11:32
it to produce better results as more data becomes available. So if you train your network on more data, it'll be more accurate. So these neural networks, they actually learn by example. And you can configure your neural network for specific applications. It can be pattern recognition or it can be data classification, anything like that. All right. So because of neural networks, we see a lot of new technology has evolved from translating web pages to other languages to having a virtual assistant


02:11:58
to order groceries online to conversing with chat bots. All of these things are possible because of neural networks. So in a nutshell, if I need to tell you artificial neural network is nothing but a network of various artificial neurons. All right. So let me show you the importance of neural network with two scenarios before and after neural network. So over here we have a machine and we have trained this machine on four types of dogs as you can see where I'm highlighting with my cursor and once the


02:12:27
training is done we provide a random image to this particular machine which has a dog but this dog is not like the other dogs on which we have trained our system on. So without neural networks our machine cannot identify that dog in the picture as you can see it over here. Basically our machine will be confused. It cannot figure out where the dog is. Now when I talk about neural networks, even if we have not trained our machine on this specific dog, but still it can identify certain features of the dogs


02:12:53
that we have trained on and it can match those features with the dog that is there in this particular image and it can identify that dog. So this happens all because of neural networks. So this is just an example to show you how important are neural networks. Now I know you all must be thinking how neural networks work. So for that we'll move forward and understand how it actually works. So over here I'll begin by first explaining a single artificial neuron that is called as perceptron. So this is


02:13:22
an example of a perceptron. Over here we have multiple inputs x1 x2 dash till xn and we have corresponding weights as well. W1 for x1 w2 for x2 similarly wn for xn. Then what happens? We calculated the weighted sum of these inputs. And after doing that we pass it through an activation function. This activation function is nothing but it provides a threshold value. So above that value my neuron will fire else it won't fire. So this is basically an artificial neuron. So when I talk about a neural network it


02:13:53
involves a lot of these artificial neurons with their own activation function and their processing element. Now we'll move forward and we'll actually understand various modes of this perceptron or single artificial neuron. So there are two modes in a perceptron. One is training, another is using mode. In training mode, the neuron can be trained to fire for particular input patterns. Which means that we'll actually train our neuron to fire on certain set of inputs and to not fire on the other set of inputs. That's what


02:14:21
basically training mode is. When I talk about using mode, it means that when a tot input pattern is detected at the input, its associated output becomes the current output. Which means that once the training is done and we provide an input on which the neuron has been trained on so it'll detect the input and we'll provide the associated output. So that's what basically using mode is. So first you need to train it then only you can use your perceptron or your uh network. So these were the two modes


02:14:46
guys. Next up we'll understand what are the various activation functions available. So these are the three activation functions although there are many more but I've listed down three step function. So over here the moment your input is greater than this particular value your neuron will fire else it won't. Similarly for sigmoid and sine function as well. So these are three activation functions. There are many more that I've told you earlier as well. So these are the three majorly


02:15:08
used activation functions. Next up what we are going to do we are going to understand how a neuron learns from its experience. So I'll give you a very good analogy in order to understand that and later on when we talk about neural networks or you can say multiple neurons in a network I'll explain you the math behind it. I'll explain you the math behind learning how it actually happens. So right now I'll explain you with an analogy and guys trust me that analogy is pretty interesting. So I know all of


02:15:34
you must have guessed it. So these are two beer mugs and all of you who love beer can actually relate to this analogy a lot and I know most of you actually love beer. So that's why I've chosen this particular analogy so that all of you can relate to it. All right, jokes apart. So fine guys, so there's a beer festival happening near your house and you want to badly go there. But your decision actually depends on three factors. First is how is the weather, whether it is good or bad. Second is


02:15:59
your wife or husband is going with you or not. And the third one is any public transport is available. So on these three factors, your decision will depend whether you'll go or not. So we'll consider these three factors as inputs to our perceptron and we'll consider our decision of going or not going to the beer festival as our output. So let us move forward with that. So the first input is how is the weather we'll consider it as x1. So when weather is good it'll be one and when it is bad


02:16:26
it'll be zero. Similarly your wife is going with you or not. So that be your x2. If she is going then it's one. If she's not going then it's zero. Similarly for public transport if it is available then it is one else it is zero. So these are the three inputs that I'm talking about. Let's see the output. So output will be one when you're going to the beer festival and output will be zero when you want to relax at home. You want to have beer at home only. You don't want to go outside. So these are


02:16:50
the two outputs whether you are going or you're not going. Now what a human brain does over here. Okay fine. I need to go to the beer festival. But there are three things that I need to consider. But will I give importance to all these factors equally? Definitely not. there'll be certain factors which will be of higher priority for me. I'll focus on those factors more whereas few factors won't affect that much to me. All right. So let's prioritize our inputs or factors. So here our most


02:17:19
important factor is weather. So if weather is good, I love beer so much that I don't care even if my wife is going with me or not or if there is a public transport available. So I love beer that much that if weather is good that definitely I'm going there. That means when x1 is high output will be definitely high. So how we do that? How we actually prioritize our factors or how we actually give importance more to a particular input and less to another input in a perceptron or in a neuron. So


02:17:44
we do that by using weights. So we assign high weights to the more important factors or more important inputs and we assign low weights to those particular inputs which are not that important for us. So let's assign weights guys. So weight w is associated with input x1, w2 with x2 and similarly w3 with x3. Now as I've told you earlier as well that weather is a very important factor. So I'll assign a pretty high weight to weather and I'll keep it as six. Similarly W2 and W3 are not that


02:18:11
important. So I'll keep it as 22. After that I've defined a threshold value as five which means that when the weighted sum of my input is greater than five then only my neuron will fire or you can say then only I'll be going to the BF festival. All right. So I'll use my pen and we'll see what happens when weather is good. So when weather is good, our x1 is 1. Our weight is six. We'll multiply it with six. Then if my wife decides that she is going to stay at home and she will probably be


02:18:42
busy with cooking and she doesn't want to drink beer with me, so she's not coming. So that input becomes zero. 0 into two will actually make no difference because it will be zero only. Then again there is no public transport available also. Then also this will be 0 into two. So what output I get here? I get here as six. And notice the threshold value it is five. So definitely six is greater than five. That means my output will be one or you can say my neuron will fire or I'll actually go to the


02:19:17
beer festival. So even if these two inputs are zero for me that means my wife is not willing to go with me and there is no public transport available but weather is good which has very high weight value and it actually matters a lot to me. So if that is high it doesn't really matter whether the two inputs are high or not I will definitely go to the BF festival. All right now I'll explain you a different scenario. So over here our threshold was five but what if I change this threshold to three. So in


02:19:44
that scenario even if my weather is not good uh I'll give it the zero. So 0 into 6 but my wife and public transport both are available. All right. So 1 into 2 + 1 into 2 which is equal to 4 and it is definitely greater than three. Then also my output will be one. that means I will definitely go to the beer festival even if weather is bad and my neuron will fire. So these are the two scenarios that I have discussed with you. All right. So there can be many other ways in which you can actually


02:20:24
assign weight to your uh problem or to your learning algorithm. So these are the two ways in which you can assign weights and prioritize your inputs or factors on which your output will depend. So obviously in real life all the inputs or all the factors are not as important for you. So you actually prioritize them and how you do that in perceptron you provide high weight to it. This is just an analogy so that you can relate to a perceptron to a real life. We'll actually discuss the math behind it later in the session as to how


02:20:52
a network or a neuron learns. All right. So how the weights are actually updated and how the output is changing that all those things we'll be discussing later in the session. But my aim is to make you understand that you can actually relate to a real life problem with that of a perceptron. All right? And in real life problems are not that easy. They are very very complex problems that we actually face. So in order to solve those problems a single neuron is definitely not enough. So we need


02:21:21
networks of neuron and that's where artificial neural network or you can say multi-layer perceptron comes into the picture. Now let us discuss that multi-layer perceptron or artificial neural network. So this is how an artificial neural network actually looks like. So over here we have multiple neurons in present in different layers. The first layer is always your input layer. This is where you actually feed in all of your inputs. Then we have the first hidden layer. Then we have second hidden layer and then we have the output


02:21:49
layer. Although the number of hidden layers depend on your application on what are you working what is your problem. So that actually determines how many hidden layers you'll have. So let me explain you what is actually happening here. So you provide in some input to the first layer which is nothing but your input layer. You provide inputs to these neurons. All right? And after some function the output of these neurons will become the input to the next layer which is nothing but your hidden layer one. Then these


02:22:13
hidden layers also have various neurons. These neurons will have different activation functions. So they'll perform their own function on the inputs that it receives from the previous layer. And then the output of this layer will be the input to the next hidden layer which is hidden layer 2. Similarly, the output of this hidden layer will be the input to the output layer and finally we get the output. So this is how basically an artificial neural network looks like. Now let me explain you this with an


02:22:38
example. So over here I'll take an example of image recognition using neural networks. So over here what happens? We feed in a lot of images to our input layer. Now this input layer will actually detect the patterns of local contrast and then we'll feed that to the next layer which is hidden layer 1. So in this hidden layer one, the face features will be recognized. We'll recognize eyes, nose, ears, things like that. And then that will be again fed as input to the next hidden layer. And in


02:23:06
this hidden layer, we'll assemble those features and we'll try to make a face. And then we'll get the output that is the face will be recognized properly. So if you notice here with every layer, we are trying to get a more abstract version or the generalized version of the input. So this is now basically an artificial neural network how it works. All right. And there's a lot of training and learning which is involved that I'll show you now training a neural network. So how we actually train a neural


02:23:32
network. So basically the most common algorithm for training a network is called back propagation. So what happens in back propagation after the weighted sum of inputs and passing through an activation function and getting the output we compare that output to the actual output that we already know. We figure out how much is the difference. We calculate the error and based on that error what we do we propagate backwards and we'll see what happens when we change the weight will the error decrease or will it increase and if it


02:24:00
increases when it increases by increasing the value of the variables or by decreasing the value of variables. So we kind of calculate all those things and we update our variables in such a way that our error becomes minimum and it takes a lot of iterations. Trust me guys it takes a lot of iterations. We get output a lot of times and then we compare it with the model with the actual output. Then again we propagate backwards. We change the variables. Then again we calculate the output. We compare it again with the desired output


02:24:26
of the actual output. Then again we propagate backwards. So this process keeps on repeating until we get the minimum value. All right. So there's an example that is there in front of your screen. Don't be scared of the terms that I used. I'll actually explain you with an example. So this is the example over here. We have 0 1 and two as inputs. And our desired output or the output that we already know is 0 1 and four. All right. So over here we can actually figure out that desired output


02:24:49
is nothing but twice of your input. But I'm training a computer to do that. Right? The computer is not a human. So what happens? I actually initialize my weight. I keep the value as three. So the model output will be 3 into 0 is 0. 3 into 1 is 3. 3 into 2 is 6. Now obviously it is not equal to your desired output. So we check the error. Now the error that we have got here is 0 1 and 2 which is nothing but your difference. So 0 - 0 is 0 3 - 2 is 1. 6 - 4 is 2. Now this is called an absolute error. After squaring this error we get


02:25:21
square error which is nothing but 0 1 and 4. All right. So now what we need to do we need to update the variables. We have seen that the output that we got is actually different from the desired output. So we need to update the value of the weight. So instead of three our computer makes it as four. After making the value as four, we get the model output as 0 4 and 8. And then we saw that the error has actually increased. Instead of decreasing, the error has increased. So after updating the variable, the error has increased. So


02:25:49
you can see that square error is now 0 4 and 16. And earlier it was 0 1 and 4. That means we cannot increase the weight value right now. But if we decrease that make it as two, we get the output which is actually equal to desired out. But is it always the case that we need to only decrease the weight? Definitely not. So in this particular scenario, whenever I'm increasing the weight, error is increasing and when I'm decreasing the weight, error is decreasing. But as I've told you earlier as well, this is not


02:26:16
the case every time. Sometimes you need to increase the weight as well. So how we determine that? All right, fine guys. This is how basically a computer decide whether it has to increase the weight or decrease the weight. So what happens here? This is a graph of square error versus weight. So over here what happens? Suppose your square error is somewhere here and your computer it starts increasing the weight in order to reduce the square error and it notices that whenever it increases the weight


02:26:39
square error is actually decreasing. So it'll keep on so the square error is increasing. So at that time we cannot increase the weight at that time computer will realize okay fine whenever I'm increasing the weight the square error is increasing. So it'll go in the opposite direction. So it'll start decreasing the weight and it'll keep on doing that until the square error becomes minimum and the moment it decreases more the square error again increases. So our network will load that


02:27:03
whenever it decreases the weight value the square error is increasing. So that point will be our final weight value. So guys this is what basically back propagation in a nutshell is fine. So we'll move forward and now is the correct time to understand how to implement the use case that I was talking about at the beginning that is how to determine whether a node is fake or real. So for that I'll open my PyCharm. This is my PyCharm again guys. Uh let me just close this. All right. So this is the code that I've written in


02:27:32
order to implement the use case. So over here what we do we import the first important libraries which are required. Matt plot lab is used for visualization. TensorFlow we know in order to implement the neural networks. Numpy for arrays. pandas for reading the data set. Similarly, sklearn for label encoding as well as for shuffing and also to split the data set into training and testing task. All right. Fine guys. So we'll begin by first reading the data set as I've told you earlier as well when I was


02:27:56
explaining the steps. So what I'll do, I'll use pandas in order to read the CSV file which has the data set. After that I'll define features and labels. So x will be my feature and y will contain my label. So basically x includes all the columns apart from the last column which is the fifth one. And because the indexing starts from zero, that's why we have written 0 till fourth. So it won't include the fourth column. All right? And so our last column will actually be our label. Then what we need to do, we


02:28:23
need to encode the dependent variable. So the dependent variable as I've told you earlier as well is nothing but your label. So I've discussed encoding in TensorFlow tutorial. You can go through it and you can actually get to know why and how we do that. Then what we have done, we have read the data set. Then what we need to do is to split our data set into training and testing. And these are all optional steps. You can print the shape of your training and test data. If you don't want to do it, you're


02:28:46
still fine. Then we have defined learning rate. So learning rate is actually the steps in which the weights will be updated. All right. So that is what basically learning rate is. Then when we talk about epoch means iterations. Then we have defined cause history that will be an empty numpy array and it shape will be one and it'll include the flow type objects. Then we have defined end which is nothing but your x shape of axis one which means your column. Then we'll print that. After that we have defined the number of


02:29:15
classes. So there can be only two class whether the node can be fake or it can be real. And this model path I have given in order to save my model. So I've just given a path where I need to save it. So I'll just save it here only in the current working directory. Now is a time to actually define our neural network. So we'll first make sure that we have defined the important parameters like hidden layers, number of neurons in hidden layers. So I'll take 10 neurons in every hidden layer and I'm taking


02:29:40
four layers like that. Then x will be my placeholder and the shape of this particular placeholder is none, n dim. N dim value. I'll get it from here and none can be any value. I'll define one variable W and I'll initialize it with zeros and this will be the shape of my weight. Similarly for bias as well, this will be the particular shape. And there will be one more placeholder ydash which will actually be used in order to provide us with the actual output of the model. There will be one model output


02:30:08
and there'll be one actual output which we use in order to calculate the difference. Right? So we'll feed in the actual values of the labels in this particular placeholder ydash. And now we'll define the model. So over here we have named the function as multi-layer perceptron. And in it we'll first define the first layer. So the first hidden layer and we are going to name it as layer_1 which will be nothing but the matrix multiplication of x and weights of h1 that is the hidden layer 1 and


02:30:37
that'll be added to your biases b1. After that we'll pass it through a sigmoid activation function. Similarly in layer 2 as well matrix multiplication of layer 1 and weights of h2. So if you can notice layer 1 was the network layer just before the layer two right. So the output of this layer 1 will become input to the layer 2 and that's why we have written layer_1 it'll be multiplied by weight h2 and then we'll add it with the bias. Similarly for this particular hidden layer as well and this particular


02:31:05
layer as well but over here we are going to use rail activation function instead of sigmoid. Then we are going to define the weights and biases. So this is how we basically define weights. This is how we basically define weights. So weights h1 will be a variable which will be a truncated normal with the shape of n dim and n hidden_1. So these are nothing but your shapes. All right. And after that what we have done we have defined biases as well. Then we need to initialize all the variables. So all these things


02:31:33
actually I've discussed in brief when I was talking about tensorflow. So you can go through tensorflow tutorial at any point of time if you have any question. We have discussed everything there. Since in tensorflow we need to initialize the variables before we use it. So that's how we do it. We first initialize it and then we need to run it. That's when your variables will be initialized. After that we are going to create a saver object and then finally I'm going to call my model. And then


02:31:57
comes the part where the training happens. Cost function. Cost function is nothing but you can say an error that will be calculated between the actual output and the model output. All right. So y is nothing but our model output and ydash is nothing but actual output or the output that we already know. All right. And then we are going to use a gradient descent optimizer to reduce error. Then we are going to create a session object as well. And finally what we are going to do we are going to run the session. So this is how we basically


02:32:26
do that. For every epoch we will be calculating the change in the error as well as the accuracy that comes after every epoch on the training data. After we have calculated the accuracy on the training data, we're going to plot it for every epoch how the accuracy is and after plotting that we're going to print the final accuracy which will be on our test data. So using the same model we'll make prediction on the test data and after that we going to print the final accuracy and the mean squared error. So


02:32:54
let's go ahead and execute this guys. All right. So training is done and this is the graph we have got for accuracy versus epoch. This is accuracy. Y-axis represents accuracy whereas this is epochs we have taken 100 epochs and our accuracy has reached somewhere around 99%. So with every epoch it is actually increasing apart from a couple of instances it is actually keep on increasing. So the more data you train your model on it'll be more accurate. Let me just close it. So now the model


02:33:23
has also been saved where I wanted it to be. This is my final test accuracy and this is the mean squared error. All right. So these are the files that will appear once you save your model. These are the four files that I've highlighted. Now what we need to do is restore this particular model. And I've explained this in detail how to restore a model that you have already saved. So over here what I'll do I'll take some random range. I've taken it actually from 754 to 768. So all the values in


02:33:50
the row of 754 and 768 will be fed to our model and our model will make prediction on that. So let us go ahead and run this. So when I'm restoring my model, it seems that my model is 100% accurate for the values that I have fed in. So whatever values that I have actually given as input to my model, it has correctly identified its class whether it's a fake node or a real node because zero stands for fake node and one stands for real node. Okay. So original class is nothing but which is there in my data set. So it


02:34:20
is zero already and what prediction my model has made is zero. That means it is fake. So accuracy becomes 100%. Similarly for other values as well. Fine guys. So this is how we basically implement the use case that we saw in the beginning. So in the slide you can notice that I've listed out only two applications although there are many more. So neural networks in medicine. Artificial neural networks are currently a very hot research area in medicine and it is believed that they will receive


02:34:47
extensive application to biomedical systems in the next few years and currently the research is mostly on modeling parts of human body and recognizing diseases from various scans. For example, it can be cardiograms, CAT scans, ultrasonic scans etc. And currently the research is going mostly on two major areas. First is modeling and diagnosing the cardiovascular system. So neural networks are used experimentally to model the human cardiovascular system. Diagnosis can be achieved by building a model of the


02:35:16
cardiovascular system of an individual and comparing it with the real-time physiological measurements taken from the patient. And trust me guys, if this routine is carried out regularly, potential harmful medical conditions can be detected at an early stage and thus make the process of combating disease much easier. Apart from that it is currently being used in electronic noses as well. Electronic noses have several potential applications in tele medicine. Now let me just give you an introduction


02:35:43
to tele medicine. Tele medicine is a practice of medicine over long distance via a communication link. So what the electronic noses will do? They would identify odors in the remote surgical environment. These identified odors would then be electronically transmitted to another site where an door generation system would recreate them. Because the sense of the smell can be an important sense to the surgeon. Teley smell would enhance teleresent surgery. So these are the two ways in which you can use it in


02:36:13
medicine. You can use it in business as well guys. So business is basically a diverted field with several general areas of specialization such as accounting or financial analysis. Almost any neural network application would fit into one business area or financial analysis. Now there is some potential for using neural networks for business purposes including resource allocation and scheduling. I've listed down two major areas where it can be used. One is marketing. So there is a marketing application which has been integrated


02:36:41
with a neural network system. The airline marketing tactician is a computer system made of various intelligent technologies including expert systems. A feed forward neural network is integrated with the AMT which is nothing but airline marketing tactician and was trained using back propagation to assist the marketing control of airline seat or locationation. So it has wide applications in marketing as well. Now the second area is credit evaluation. Now I'll give you an example here. The HNC company has developed several neural


02:37:12
network applications and one of them is a credit scoring system which increases the profitability of existing model up to 27%. So these are few applications that I'm telling you guys neural network is actually the future. People are talking about neural networks everywhere and especially after the introduction of GPUs and the amount of data that we have now neural network is actually spreading like plague right now. [Music] Why can't we use feed forward networks? Now let us take an example of a feed


02:37:47
forward network that is used for image classification. So we have trained this particular network for classifying various images of animals. Now if you feed in an image of a dog, it will identify that image and will provide a relevant label to that particular image. Similarly, if you feed in an image of an elephant, it will provide a relevant label to that particular image as well. Now if you notice the new output that we have got that is classifying an elephant has no relation with the previous output


02:38:13
that is of a dog or you can say that the output at time t is independent of output at time t minus one as we can see that there is no relation between the new output and the previous output. So we can say that in feed forward networks outputs are independent to each other. Now there are few scenarios where we actually need the previous output to get the new output. Let us discuss one such scenario. Now what happens when you read a book? You'll understand that book only on the understanding of your previous


02:38:40
words. All right? So if I use a feed forward network and try to predict the next word in a sentence, I can't do that. Why can't I do that? Because my output will actually depend on the previous outputs. But in the feed forward network, my new output is independent of the previous outputs. That is output at t + 1 has no relation with output at t minus 2, t minus 1 and at t. So basically we cannot use feed forward networks for predicting the next word in a sentence. Similarly you can think of many other examples where we


02:39:09
need the previous output some information from the previous output so as to infer the new output. This is just one small example. There are many other examples that you can think of. So we'll move forward and understand how we can solve this particular problem. So over here what we have done we have input at t minus one. We'll feed it to our network. Then we'll get the output at t minus one. Then at the next time stamp that is at time t we have input at time t that will be given to our network


02:39:35
along with the information from the previous time stamp that is t minus one and that will help us to get the output at t. Similarly at output for t + one we have two inputs one is a new input that we give another is the information coming from the previous time stamp that is t in order to get the output at time t + 1. Similarly, it can go on. So over here, I have just written a generalized way to represent it. There's a loop where the information from the previous time stamp is flowing. And this is how


02:40:04
we can solve this particular challenge. Now let us understand what exactly are recurrent neural networks. So for understanding recurrent neural network, I'll take an analogy. Suppose your gym trainer has made a schedule for you. The exercises are repeated after every third day. Now this is the order of your exercises. First day you'll be doing shoulders. Second day you'll be doing biceps. Third day you'll be doing cardio and all these exercises are repeated in a proper order. Now what happens when we


02:40:28
use a feed forward network for predicting the exercise today. So we'll provide in the inputs such as day of the week, month of the year and health status. All right. And we need to train our model or our network on the exercises that we have done in the past. After that there'll be a complex voting procedure involved that will predict the exercise for us and that procedure won't be that accurate. So whatever output we'll get won't be as accurate as we want it to be. Now what if I change my


02:40:53
inputs and I make my inputs as what exercise I've done yesterday. So if I've done shoulder then definitely today I'll be doing biceps. Similarly if I've done biceps yesterday, today I'll be doing cardio. Similarly if I've done cardio yesterday, today I'll be doing shoulder. Now there can be one scenario where you are unable to go to gym for one day due to some personal reasons. You could not go to the gym. Now what will happen at that time? We'll go one time stamp back


02:41:17
and we'll feed in what exercise that happened day before yesterday. So if the exercise that happened day before yesterday was shoulder then yesterday there were biceps exercises. All right. Similarly biceps happened day before yesterday then yesterday would have been cardio exercises. Similarly if cardio would have happened day before yesterday would have been shoulder exercises. All right. And this prediction the prediction for the exercise that happened yesterday will be fed back to our network and these predictions will


02:41:43
be used as inputs in order to predict what exercise will happen today. Similarly, if you have missed your gym, say for 2 days, 3 days or one week. So, you need to roll back. You need to go to the last day when you went to the gym, you need to figure out what exercise you did on that day. Feed that as an input and then only you'll be getting the relevant output as to what exercise will happen today. Now, what I'll do, I'll convert these things into a vector. Now, what is a vector? Vector is nothing but


02:42:09
a list of numbers. All right? So, this is the new information guys along with the information from the prediction at the previous time step. So we need both of these in order to get the prediction at time t. Imagine if I have done shoulder exercises yesterday. So this will be one, this will be zero and this will be zero. Now the prediction that will happen will be biceps exercise because if I've done shoulder yesterday, today it will be biceps. So my output will be 0 1 and zero. And this is how


02:42:33
vectors work guys. So I hope you have understood this guys. Now this is how a neural network looks like guys. We have new information along with the information from the previous time stamp. The output that we have got in the previous time stamp will uh use certain information from that will feed into our network as inputs and then that will help us to get the new output. Similarly, this new output that we have got will take some information from that feed in as an input to our network along with the new information to get the new


02:43:01
prediction and this process keeps on repeating. Now let me show you the math behind the recurrent neural networks. So this is the structure of a recurrent neural network guys. Let me explain you what happens here. Now consider at time t equals to0 we have input x note and we need to figure out what is hnot. So according to this equation h of0 is equal to w i weight matrix multiplied by input x of 0 plus w into h of 0 - 1 which is h of minus1 and time can never be negative. So we this particular


02:43:32
equation cannot be applied here plus a bias. So wi into x of0 plus bh pass passes through a function g of h to get h of 0 over here. After that I want to calculate y. So for y not I'll multiply h of 0 with the weight matrix wi and I'll add a bias to it and pass it through a function g of i to get y not. Now in the next time stamp that is at time t= to 1 things become a bit tricky. Now let me explain you what happens here. So at time t= to 1 I have input x1. I need to figure out what is h1. So


02:44:03
for that I'll use this equation. So I'll multiply wi that is a weight matrix by the input x1 plus w into h of 1 - 1 which is 0. h of 0 we know what we got from here. So w into h of 0 plus the bias pass it through a function g of h to get the output as h1. Now this h1 we'll use to get y1. We'll multiply h1 with wy plus a bias and we'll pass it through a function g of y to get y1. Similarly, the next time stamp that is at time t= to 2. We have input x2. We need to figure out what will be h2. So,


02:44:38
we'll multiply the weight matrix wi with x of2 plus w into h of 1 that we have got here plus b of h and pass it through a function g of h to get h of 2. From h of2, we'll calculate y of 2. Wy into h of 2 plus y that is the bias. Pass it through a function g of y to get y. And this is how recurrent neural network works guys. Now you must be thinking how to train a recurrent neural network. So a recurrent neural network uses back propagation algorithm for training. But back propagation happens for every time


02:45:08
stamp. That is why it is commonly called as back propagation through type. And I've discussed back propagation in detail in artificial neural network tutorials. So you can go through that over here. I won't be discussing back propagation uh in detail. I'll just give you a brief introduction of what it is. Now with back propagation there are certain issues namely vanishing and exploding gradients. Let us see those one by one. So in vanishing gradient what happens when you use back propagation you tend to calculate the


02:45:34
error which is nothing but the actual output that you already know minus the model output output that you got through your model and the square of that. So you figure out the error with that error. What do you do? You tend to find out the change in error with respect to change in weight or any variable. So we'll call it weight here. So change of error with respect to weight multiplied by learning rate will give you the change in weight. Then you need to add that change in weight to the old weight


02:46:01
to get the new weight. All right. So obviously what we trying to do we trying to reduce the error. So for that we need to figure out what will be the change in error if my variables are changed. Right? So that way we can get the change in the variable and add it to our old variable to get the new variable. Now over here what can happen if the value de by dw that is a gradient or you can say the rate of change of error with respect to our variable weight becomes very small than one like it is 0 something. So if you multiply that with


02:46:30
the learning rate which is definitely smaller than one then you get the change of weight which is negligible. All right so there might be certain examples where you know you are trying to predict say a next word in a sentence and the sentence is pretty long. For example, if I say I went to France dash dash I went to France then there are certain words then I say few of them speak dash now I need to predict speak what will come after speak so for that I need to go back in time and check what was the context


02:47:01
which will be very complex and due to that there will be a lot of iterations and because of that this error this change in weight will become very small very small so the new weight that we'll get will be actually almost equal to your old weight. So there won't be any updation of weight that will be happening and that is nothing but your vanishing gradient. All right, I'll repeat it once more. So what happens in back propagation? You first calculate the error. This error is nothing but the


02:47:27
difference between the actual output and the model output and the square of that. With that error, we figure out what will be the change in error when we change a particular variable say weight. So DE by DW multiply it with learning rate to get the change in the variable or change in the weight. Now we'll add that change in the weight to our old weight to get the new weight. This is what back propagation is guys. All right, I've just given you a small introduction to back propagation. Now consider a


02:47:51
scenario where you need to predict the next word in the sentence and your sentence is something like this. I have been to France then there are a lot of words after that few people speak and then you need to predict what comes after speak. Now if I need to do that I need to go back and understand the context. What is it talking about? And that is nothing but your long-term dependencies. So what happens during long-term dependencies? If this DE by DW becomes very small, then when you multiply it with N which is again


02:48:20
smaller than one, you get deltaW which will be very very small. That will be negligible. So the new weight that you'll get here will be almost equal to your old weight. So I hope you're getting my point. So this new weight so there will be no updation of weights guys. This new weight will definitely be will always be almost equal to our old weight. So there won't be any learning here. So that is nothing but your vanishing gradient problem. Similarly, when I talk about exploding gradient, it


02:48:47
is just the opposite of vanishing gradient. So what happens when your gradient or de by dw becomes very large becomes greater than greater than one. All right? And you have some long-term dependencies. So at that time your d by dw will keep on increasing. Delta w will become large and because of that your weights the new weight that will come will be very different from your old weight. So these two are the problems with back propagation. Now let us see how to solve these problems. Now exploding gradients can be solved with


02:49:15
the help of truncated BTD back propagation through time. So instead of starting back propagation at the last time stamp, we can choose a smaller time stamp like 10 or we can clip the gradients at a threshold. So there can be a threshold value where we can you know clip the gradients and we can adjust the learning rate as well. Now for vanishing gradient we can use a relu activation function. Similarly, we can also use LSTM and GRUs. Now, let us understand what exactly are LSTMs. So guys, we saw what are the two


02:49:43
limitations with the recurrent neural networks. Now, we'll understand how we can solve that with the help of LSTMs. Now, what are LSTMs? Long short-term memory networks, usually called as LSTMs, are nothing but a special kind of recurrent neural network. And these recurrent neural networks are capable of learning long-term dependencies. Now, what are long-term dependencies? I've discussed that in the previous slide but I'll just explain it to you here as well. Now what happens sometimes you


02:50:08
only need to look at the recent information to perform the present task. Now let me give you an example. Consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in the sentence say the clouds are in the sky. So we don't need any further context. It's pretty obvious that the next word is going to be sky. Now in such cases where the gap between the relevant information and the place that it's needed is small RNN can learn to


02:50:36
use the past information and at that time there would be such problems like vanishing and exploring gradient. But there are few cases where we need more context. Consider trying to predict the last word in the text I grew up in France. Then there are some words after that comes I speak fluent French. Now recent information suggests that the next word is probably the name of a language. But if you want to narrow down which language we need the context of France from further back and it's entirely possible for the gap between


02:51:07
the relevant information and the point where it is needed to become very large and this is nothing but long-term dependencies and the LSTMs are capable of handling such long-term dependencies. Now LSTMs also have a chain-like structure like recurrent neural networks. Now all the recurrent neural networks have the form of a chain of repeating modules of neural networks. Now in standard RNN the repeating module will have a very simple structure such as a single tan layer that you can see. Now this tan layer is nothing but a


02:51:34
squashing function. Now what I mean by squashing function is to convert my values between minus1 and one. All right that's why we use tanage and this is an example of an RNN. Now we'll understand what exactly are LSTMs. Now this is a structure of an LSTM or if you notice LSTM also have a chain-like structure but the repeating module has different structures instead of having a single neural network here there are four interacting in a very special way now the key to LSTM is the cell state now


02:52:03
this particular line that I'm highlighting this is what what is called the cell state the horizontal line running through the top of the diagram so this is nothing but your cell state now you can consider the cell state as a kind of a conveyor belt it runs straight down the entire chain with only some minor linear interactions. Now what I'll do, I'll give you a walk through of LSTM step by step. All right. So we'll start with the first step. All right guys, so the first step in our LSTM is to decide


02:52:29
what information we are going to throw away from the cell state. And you know what is a cell state, right? I've discussed in the previous slide. Now this decision is made by the sigmoid layer. So the layer that I'm highlighting with my cursor, it is a sigmoid layer called the forget gate layer. It looks at HT minus one. that is the information from the previous time stamp and XT which is a new input and outputs a number between zeros and one for each number in the cell state ct minus one which is coming from the


02:52:56
previous time stamp one represents completely keep this while a zero represents completely get rid of this. Now if we go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem the cell state might include the gender of the present subject so that the correct pronouns can be used. When we see a new subject we want to forget the gender of the old subject. Right? We want to use the gender of the new subject. So we'll forget the gender of the previous


02:53:24
subject here. This is just an example to explain you what is happening here. Uh now let me explain you the equations which I have written here. So f_t will be combining with the sales date later on that I'll tell you. So currently f_t will be nothing but the weight matrix multiplied by ht minus one and xt and plus a bias and this equation is passed through a sigmoid layer. All right and we get an output that is 0 and one. 0 means completely get rid of this and one means completely keep this. All right.


02:53:54
So this is what basically is happening in the first step. Now let us see what happens in the next step. So the next step is to decide what information we are going to store. In the previous step we decided what information we are going to keep. But here we are going to decide what information we are going to store here. All right. What new information we going to store in the cell state. Now this has two parts. First a sigmoid layer. This is called a sigmoid layer and which is also known as an input gate


02:54:19
layer. Decide which values we'll update. All right. So what values we need to update. And there's also a tan ash layer that creates a vector of the candidate values c bar of t minus one that will be added to the state later on. All right. So let me explain it to you in a simpler terms. So whatever input that we are getting from the previous time stamp and the new input it will be passed through an sigmoid function which will give us i of t. All right. And this I oft will be multiplied by CT which is nothing but


02:54:47
the input coming from the previous time stamp and the new input with that is passed through a tan hatch. That will result in CT and this will be later added onto our sales state. And then next step we'll combine these two to update the states. Now let me explain the equations. So I oft will be what? Weight matrix. And then we have HD - 1 comma XT multiplied by the weight matrix plus the bias pass it through a sigmoid function we get I oft C bar of T we'll get by passing a weight matrix HD minus1


02:55:13
XT plus bias through a tan squashing function and we'll get C bar of T. All right. So as I've told you earlier is where the next step we'll combine these two to update the state. Let us see how we do that. So now is the time to update the old cell state CT minus one with the new cell state CT. All right. And the previous steps we have already decided what to do. we just need to actually do it. So what we'll do, we'll multiply the old cell state CT minus one with FT that we got in the first step. Forgetting the


02:55:40
things that we decided to forget earlier in the first step if you can recall. Then what we do? We add it to it and CT. Then we add it by the term that will come after multiplication of it and C bar and this new candidate value scaled by how much we decided to update each state value. All right. So in the case of the language model that we were discussing, this is where we would actually drop the information about the old subject gender and add the new information as we decided in the previous steps. So I hope you are able


02:56:09
to follow me guys. All right. So let us move forward and we'll see what is the next step. Now our last step is to decide what we are going to output. And this output will depend on a cell state but it'll be a filtered version. Now finally what we need to do is we need to decide what we are going to output. And this output will be based on our cell state. First we need to pass HD minus1 and X3 through a sigmoid activation function so that we get a output that is OT. All right. And this OT will be in


02:56:36
turn multiplied by the cell state after passing it through a tan squashing function or an activation function. And why we do that just to push the values between minus1 and one. So after multiplying OT that is this value and a tan c we'll get the output h2 which will be our new output and that will only output the part that we decided to whatever we have decided in the previous steps it will only output that value. All right. Now, I'll take the example of the language model again. Since it just


02:57:05
saw a subject, it might want to output information relevant to a verb. And in case that's what is coming next, for example, it might output whether the subject is singular or plural so that we know what form of a verb should be conjugated into. All right? And you can see from the you can see the equations as well. Again, we have a sigmoid function. then that uh whatever output we get from there we multiply it with tanatchect CT to get the new output. All right guys so this is basically LSTMs in


02:57:35
a nutshell. So in the first step we decided what we need to forget. In the next step we decided what are we going to add to our cell state? What new information we are going to add to our cell state and we were taking an example of the gender throughout this whole process. All right and in the third step what we do we actually combined it to get the new cell state. Now in the fourth step what we did we finally got the output that we want and how we did that just by passing HD minus one and XT through a sigmoid function multiplying


02:58:02
it with the tan at CT the tan new cell state and we get the new output fine guys so this is what basically LSTM is guys now we'll look at a use case where we'll be using LSTM to predict the next word in a sentence all right let me show you how we are going to do that so this is what we are trying to do in our use case guys we'll feed LSTM with correct sequences from the text of three symbols. For example, had a general and a label that is council in this particular example. Eventually, our


02:58:31
network will learn to predict the next symbol correctly. So, obviously, we need to train it on something. Let us see what we are going to train it on. So, we'll be training our LSTM to predict the next word using a sample short story that you can see over here. All right. So, it has basically 112 unique symbols. So, even comma and full stop are considered as symbols. All right. So this is what we are going to train it on. So technically we know that LSTMs can only understand real numbers. All


02:58:57
right. So what we need to do is we need to convert these unique symbols into a unique integer value based on the frequency of occurrence. And like that we'll create a dictionary. For example, we have had here that will have value 20. A will have value six. General will have value 33. All right. And then what happens? Our LSTM will create a 112 element vector that will contain the probability of each of these words or each of these unique integer values. All right. So since 6 has the highest


02:59:27
probability in this particular vector, it'll pick the index value of 6 then it will see what symbol is attached to that particular integer value. So 37 is attached to council. So this will be our prediction which is absolutely correct as a label is also council according to our training data. All right. So this is what we are going to do in our use case. So guys, this is what we'll be doing in our today's use case. Now I'll quickly open my PyCharm and I'll show you how you can implement it using Python. We'll


02:59:54
be using TensorFlow which is a popular Python library for implementing deep neural networks or neural networks in general. All right. So I'll quickly open my PyCharm now. So guys, this is my PyCharm and I over here I've already written the code in order to execute the use case that we have. So first we need to do is import the libraries. uh numpy for arrays tensorflow we know tensorflow contract from that we need to import rnn and random collections in time all right so this particular block of code is used


03:00:20
to evaluate the time taken for the training after that we have log path and this log path is basically telling us a path where the graph will be stored all right so there will be a graph that will be created and then that graph will be launched then only our ron model will be executed then that's how tensorflow work guys so that graph will be created in this particular path All right. And we are using summary writer. So that will actually create the log file that will be used in order to display the graph using tens support.


03:00:48
All right. So then we have defined a training file which will have our story on which we'll train our model on. Then what we need to do is read this file. So how are we going to do that? First is uh read line by line whatever content that we have in our file. Then we are going to strip it. That means we are going to uh remove the first and the last white space. Then again we are splitting it just to remove all the white spaces that are there. After that we're creating an array and then we are reshaping it. Now


03:01:16
during the reshape if you notice this minus one value tells us the compatibility. All right. So when you're reshaping it you need to make sure that uh you know we are providing it the correct parameters to reshape it. So you can convert a 3 +2 matrix to a 2 + 3 matrix like right. So just to make sure that that it is compatible enough we add this minus one and it'll be done automatically. All right. Then return content. After that what we are doing we are feeding in the training data that we


03:01:42
have training file. We are feeding in our story and calling the function readers data. Then what we are doing? We are creating a dictionary. What is a dictionary? We all know key value pairs based on the frequency of occurrences of each symbol. All right. So from here collections counterwords do most common. So most common words with their frequency of occurrence there will be a dictionary created and after that we'll call this dict function. And this dict function will feed in word and which is


03:02:08
equal to length of dictionary. That means whatever the length of that particular dictionary is how many time it is repeated. So we'll have the frequency as well as a symbol that will be our key value pair and we reversing it as well. Then what we are doing we are calling it build data set and we're feeding in our training data there. This is our vocabulary size which is nothing but the length of your dictionary. Then we have defined various parameters such as learning rate, iterations or epochs.


03:02:34
Then we have display step n input. Now learning rate we all know what it is. Uh the steps in which our variables are updated. Training iterations is nothing but your epoch the total number of iterations. So we have given 50,000 iterations here. Then we have display step that is th00and which is basically your batch size. So batch size is what after every thousand epox you'll see the output. All right. So it'll be processing it in batches of thousand iterations. Then we have n input as three. Now the number of units in the


03:03:02
RNN cell will keep it as 512. Then we need to define X and Y. So X will be our placeholder that will have the input values and Y will have all the labels. All right, vocab size. So X is a placeholder where we'll be feeding in our input dictionary. Similarly, Y is also one more placeholder and it'll have a shape of none, vocab size. Vocab size we have defined earlier as you can see which is nothing but the length of your dictionary. Then we are defining weights as well as biases. After that we have


03:03:31
defined our model. All right. So this is how we are going to define it. We'll create a function RNN when we'll have X weights and biases. And after that we are calling in RNN. RNN cell function. And this is basically to create a two-layer LSTM. And each layer has N hidden units. After that what we are doing we are generating the predictions but once we have generated the prediction there are n_input outputs but we only want the last output. All right so for that we have written this particular line and then finally we are


03:04:01
making a prediction. We are calling this rnn function feeding in x weights and biases. After that we are calculating the loss as and then we are optimizing it. For calculating the ROSS we are using uh reduce mean softmax cross entropy and this will give us basically the probability of each symbol and then we are optimizing it using RMS prop optimizer. All right and this gives actually a better accuracy than atom optimizer and that's the reason why we are using it. Then we are going to calculate the accuracy and after that we


03:04:30
are going to initialize the variables that we have used as we have seen in tensorflow that we need to initialize all the variables unlike constants and placeholders in tensorflow. All right. And once we are done with that, we are feeding in our values, then calculating the accuracy, how accurate it is. And then when optimization is done, we are calculating the elapse time as well. So that will give us how much time it took in order to train our model. Then this is just to run the tensorboard on a


03:04:54
local host 6006. And uh yeah, and this particular block of code is is used in order to handle the exceptions. So exceptions can be like whatever word that we are putting in might not be there in our dictionary or might not be there in our training data. So those exceptions will be handled here and if it is not there in our dictionary then it'll print word not in a dictionary. All right. So fine guys let's uh input some values and we'll have some fun with this model. All right. So the first uh


03:05:22
thing that I'm going to feed in is had a general. So whenever I feed in these three values had a general there'll be a story that will be generated by feeding back the predicted output as the next symbol in the inputs. All right. So when I feed in had a general so it'll predict the correct output as council and this council will be fed back as a part of the new input and our new input will be a general council. So it'll be a general council. All right. So these three words will become our new input to predict the


03:05:49
new output which is two. All right and so on. So surprisingly LSTM actually creates a story that you know somehow makes sense. So let's just read it. Had a general counselor to consider what measures they could take to outwit their common enemy the cat. By this means we should always know when she was about and could easily All right. So somehow it actually makes sense when you feed in that. So what will happen when you feed in these three inputs it'll predict the next word that is council. After that


03:06:16
it'll take council and it'll feed back as an input along with earth general. So earth general council will be your next input to predict two. Similarly in the next iteration it will take general council 2 and predict consider for us and this will keep on repeating. [Music] Kas is a python based deep learning framework which is actually the highle API of tensorflow. And now I have four major highlights for you guys. So let's check it out from the start. Well, KAS basically runs on top of Theo,


03:06:48
TensorFlow or CNTK. Since it runs on top of any of these frameworks, KAS is amazingly simple to work with. You might be wondering why. Well, building models are as simple as stacking layers and later connecting these graphs. Guys, Kiras attracts a lot of attention, but why? Since it is open source, it is actively developed by all the contributors across the world. And the documentation is nearly endless. Well, we're good with the documentation, but how does KASS perform, guys? Since it is an API which is actually used to specify


03:07:18
and train differentiable programs, high performance naturally follows through here. So, now that we know what KASS actually is, who makes KAS what it is? Well, we need to check out some of the contributors and the backers to the deep learning framework. Well, guys, Kiras had over 4,800 contributors during its launch and the initial stages. And now that number has gone up to 250,000 active developers. Well, what amuses me is that there is a 2x growth ever since every year since its launch. Also, it


03:07:49
holds a really good amount of traction among multiple startups. Big players like Microsoft, Google, Nvidia, and Amazon actively contribute to the development of KAS. Good enough. So, at this point, I'm sure all of you are curious about who uses KAS. Well, Kiras has an amazing industry traction and it is used in the development of popular firms like Netflix, Uber, Expedia, Yelp and more. Well, now that you know the next time you watch a movie on Netflix or book an Uber, you know KAS is being


03:08:17
used. So guys, if KAS is getting all this attention, what makes it so special? What makes KAS stand out among all the top framework? Here I have for you top 10 snippets that makes KAS so special. Well guys, the focus on user experience has always been the major part of KAS. And next, large adoption in the industry definitely. We just checked out all of the industry traction it gets and this holds well. And next, it is multibackend and supports multiplatform as well. This helps all the coders come


03:08:45
together and code easily. Next up, the research community present for Kiras is amazing along with the production community. So, this is a win-win for me guys. So, what do you think? And moving on, all the concepts are really easy to grasp with Kiras guys. And next up, it supports fast processing which is really good. And guys, it runs seamlessly on both the CPU and the GPU. And it has support for both Nvidia and AMD as well. And the best part for me is the freedom to design on any architecture and then


03:09:14
later implement it as an API for your projects. Guys, this definitely is a major advantage for me. So, next up for all the beginners, KAS is really simple to get started with and I'm here to help you guys with this tutorial for that. So, stay tuned. And lastly, the easy production of models makes KAS that much special. Guys, now that we know KAS is special, guys, let's dig in a little bit about one of the major concepts which make KASS what it is, the user experience. Well, in my opinion, this is very


03:09:42
important for anyone who wants to know more about KAS or better they want to start creating their own neural nets using KAS. So clearly, KAS is an API designed for humans. Well, why so? because it follows the best practices for reducing cognitive load which ensures that the models are consistent and the corresponding APIs are simple. And moving on, Kiras provides clear feedback upon occurrence of any error and this minimizes the number of user actions required for the majority of the common use cases guys. And lastly, KAS


03:10:13
provides high flexibility to all of its developers. Well, we all love high flexibility, right? So, how is Keras doing this guys? It's very simple. It integrates with lower level deep learning framework languages like TensorFlow or Theo. So guys, this ensures that you can implement anything in KAS which you actually built in your base language which is amazing. So next up we need to talk about how KAS supports the claim of being able to support multiplatform and lets us work with multiple backends. You can develop


03:10:41
KAS in Python as well as R. The code can be run with TensorFlow, CNTK, Theo or MXNet totally based on your requirement. This almost feels like a tailor-made API for the framework guys. The code can be run on the CPU or the GPU as well. Support for both the big players being Nvidia and AMD here. So this ensures that producing models with Keras is really simple. Total support to run with TensorFlow serving GPU acceleration such as CUDA when using modules such as webkas and KASJS. Native support to


03:11:11
develop Android and iOS apps using TensorFlow and Core ML. And yes, full-blown support to work with Raspberry Pi as well. So guys moving on we need to check one quick concept which forms the backbone as a walking principle of keras. So let's check out a computation graph. Here is an example for you guys. Just before decoding and working our way through the graph let's look at the features. So guys do note that computational graphs are used for expressing complex expression as a combination of simple operations for


03:11:39
crash to work with. It is mainly useful for calculating the derivatives during the phase of back propagation and hence it makes it easier to implement distributed computing on the whole. So all it takes is to specify the inputs, outputs and to make sure that the graph is connected throughout. I hope you guys know what a connected graph is. So let's check out our graph. We'll be working away from the leaf nodes to the top. So guys here as you can see E equal to C multiplied with D where C is equal to A


03:12:05
+ B and D is equal to B + 1. So all we're doing is we're making sure we land at E equal to C cross T which is ahead of our tree and we get to this by performing two operations on the leaf nodes. So as you can see working down further E equal to A + B into B + 1 actually makes sense now and in our case A and B are inputs. So guys it is as simple as that. So next let's dive a little bit deeper and check out the two major models guys. So the first model is a sequential model. The working is


03:12:35
basically like a linear stack of layers. So the first thing that comes into my mind when I think about the layered approach is the sequential model. It is majorly useful for building simple classification network and encoder decoder model guys. And definitely yes this is the model which we all know and love. So here we basically treat every layer as an object that feeds into the next layer and so on. And now in the simple code we'll import keras into python. We define the model as sequential and with the hidden layers we


03:13:03
have 20 neurons and we'll be using ReLU here. ReLU is rectifier linear unit guys. It is one of the activation functions we'll be using. Well, model.fit is used to train the network here by epoch. I'm sure all of you guys are familiar with it already. So, it is basically the forward and the backward pass of all of our training examples. And bat size is really straightforward as well. It is the number of training examples in one forward and backward pass guys. So higher the batch size the


03:13:28
more the memory you'll need. So next we need to check out the functional model. It is widely used and it holds good for about 95% of the use cases. Well imagine the concept of playing with Legos guys. I'm pretty sure most of us have played with Legos in our childhood. It is pretty much the same here as well. Well the highlights of the functional model is that it supports multi-input multi output and an arbitrary static graph topology. We have branches. So whenever we have a complex model, the model is


03:13:55
folded into two or more branches based on the requirement. Guys, the code which we have here is pretty much similar to the previous one but with subtle changes. We first import the models, we work on its architecture and lastly we train the network. Well, with functional models, we have this concept called as domain adaption. So guys, what we did until this stage is that we train a model on one domain but test it on the other. This definitely results in poor performance on the overall test data set


03:14:20
because the data is different for each of the domains. Right? So what's the solution for this? Well, we adapt the model to work on both the domains at the same time. And guys, we'll be looking at a very interesting use case using the functional models in the upcoming slides. So stay tuned for that. So moving on, we need to understand about the two basic types of execution in KAS. Deferred and eager execution. It is also called as symbolic and imperative execution as well. Well, with deferred


03:14:46
we use Python to build a computation graph first like we previously discussed and then this compile graph gets executed. Well, with eager execution there is a slight change guys. It is here that the Python runtime itself becomes the execution runtime for all of the models. It is very similar to execution with numpy. So if you're familiar with numpy then it's a cakewalk guys. So on the whole here is a quick note. Symbolic tensors don't have a value in the python code as of yet. But eager tensors will have a value in the


03:15:14
Python code. And with eager execution, we make use of a type of recurring neural networks called as trees. So guys, it is basically a value dependent dynamic topology structure. So what are you guys thinking about KAS at this point? Well, it is actually really easy to grasp guys. Well, next, let's look at the steps needed to implement our own neural network using Keras. There are five major steps here guys. So starting out, we need to prepare the inputs for the model. We do this by analyzing our


03:15:40
requirements and specifying the input dimensions. Well, as you know it, the common inputs are images, videos, text or audio based on your model requirement. The next step is to actually define the artificial neural network model. Here we do everything from defining the model architecture to building the computation graph and also defining the style we'll be using for the model. It is as straightforward as that. Well, step three is to specify the optimizer. Think of it this way. A neural network is just a complex


03:16:08
function. We need to simplify the process of making the machine learn. Well, the optimizer is just for that. There are many types of optimizers such as SGD which is stoastic gradient descent. We have RMS prop which is based on root mean square and atom and so on. And the next step is to define the loss function. So for every step in our training, we'll be checking the accuracy of prediction by comparing the obtained value with the actual one. We check for the difference between them and we print


03:16:34
out the loss guys. Well, the goal is to actually define a function which we will use to reduce the losses in each pass of the training phase. There are many types of loss function such as MSE which is mean squared error and we have cross entropy loss which is also called as a log loss in most cases and so on. And the last step is to actually train the network based on the input data which is also called as a training data. And after training we will need to test the model based on the trained data to check


03:16:59
if the model actually learned anything. So it is as simple as this guys. What do you think? I would love to know your views on this. So head to the comment section. Let's have an interaction there. And now guys, let's spice things up a bit. I'm sure you guys were curious about the use case. So let me walk you through the entire thing. Well, we'll be checking out a wine classifier in this use case. So let's begin by checking out our problem statement. So we're trying to predict the price of a bottle of wine


03:17:24
just by knowing the description and the variety of wine. Well, we can work this out with the KAS functional API and TensorFlow. We'll be building a wide and a deep network using KAS to make predictions for us. Well, can we achieve this goal? Yes, we can. This is a problem statement suited for wide and deep learning networks as I mentioned. Well, it involves textual input and there isn't any correlation between the wine's description and its price. Well, this is what makes it fun in my opinion


03:17:51
guys. So, next we need to check out the model. A lot of kas models are built using the sequential model API as I told you earlier. But let's make use of the functional API for our use case. Well, true, the sequential API is still the best way to get started with Kiras. Why? Because it lets you define models easily as a stack of layers like I explained earlier. However, the functional model allows for more flexibility and is best suited for models with multiple inputs. So, we need to know a little bit about


03:18:18
wide and deep guys. Well, wide models are models with sparse feature vectors. Well, what I mean by sparse feature vectors is that it consists of mostly zeros and a little bit of ones. And deep networks are networks which do really well on tasks like speech and image recognition. So now that that's sorted, we need to take a look at the data set. Well, for this case, we'll be using a wine data set from Kaggle. So what's the data? Well, it's basically 12 columns of data and it's as follows. Here we'll be


03:18:45
talking about the country that the wine is from. Next up is description. A few sentences from the somier descripting the wine's taste, smell, look, and feel. A sumeier is a person who is a professional wine taster. Guys, next up is designation. The wineard within the winery where the grapes that the wine has been made from. Next up is points. The number of points that the wine enthusiasts rated the wine on a scale of 1 to 10. However, they only say that they posted reviews for the wines which


03:19:11
scores greater than equal to 80. Next up is price, the cost of the bottle of the wine, obviously, followed by province. the province or the state where the wine is from. Next up, I have something called as region one. Well, with region one, it's the wine growing area in a province or a state. Let's say for example, India. Next, I have region two. Well, sometimes there are more specific regions within a specified wine growing area. For example, we can say Bangalore, India. But this value can sometimes be


03:19:38
blank as well. Next up is taster's name. Well, as it suggests, it's the name of the person who tasted and reviewed the wine. Taster Twitter handle. Twitter handle for the person who tasted and reviewed the wine. Well, the title of the wine review, which often contains the vintage if you're interested in extracting that feature, variety, the type of the grapes that is used to make the wine. Let's say for example, Pinoyer, that's a type of a grape. And finally, the winery that made the wine.


03:20:03
Guys, the overall goal here is to actually create a model that can identify the variety, winery, and the location of a wine based on the description alone. And this data set offers some really great opportunities for sentiment analysis and other text related predictive models as well. So now that that's clear, we need to take a look at the sample data case. So here we have a description for the wine such as scent, if it's tart, firm or needs more decanting etc. So this forms our input for the model guys and the output our


03:20:31
model provides just from all of this textual information is the pricing that it predicts. How cool is that guys? So basically we need to check out some of the prerequisites before jumping into the code. Since you're working with Python, we'll require pandas, we'll require numpy, skyitlearn and jupyter notebook. So yes, kas works on top of tensorflow. So we will require both kas and tensorflow to be installed on the machine. So now that that's done, moving on, let's look at a small piece of code.


03:20:57
Here are all the inputs that we'll require to build the model. And lastly, we test the presence of TensorFlow by printing the installed version. Well, without TensorFlow, it wouldn't make any sense. So we go to Kaggle and download the data and end up converting the data to a pandas data frame. Guys, well that's good enough to start. Let's look at the code for this model. I'll quickly open up Google Collaboratory which is basically a Jupyter notebook hosted on their Google cloud. You can actually do


03:21:22
this on your local machine as well by installing all of the frameworks that I've previously mentioned. So let me go ahead and open Collaboratory and let's begin guys. So guys, we'll be executing each of these blocks and we'll be going on from there. So let us check out the first block. So here we import all of the modules that we require. So guys, let me run it and that's done. So next we'll need to install the latest version of TensorFlow. Well, with Google Collab, it doesn't require any extra setup. So


03:21:47
it's pretty much straightforward. So guys, that took about 2 minutes and TensorFlow is installed. And as I explained before, we need to import the models that we'll use to build the model. And after that, we'll actually run the code to check the version output of the TensorFlow that we just installed. And the output we're supposed to be expecting is version 1.7 because 1.7 is the TensorFlow version that we installed. As you can check out the output, you have TensorFlow version 1.7.


03:22:11
So beautiful. So moving on, we need to download the data which is from a CSV file hosted on the cloud. So let's go ahead and do that. Now that it is downloaded and ready, let us convert all the data from the CSC file into a pandas data frame as I mentioned. So now that that is done, let's go ahead and mix up the data by shuffling it. And yes, so let's start printing samples from our data set. So we'll be printing the first five rows. And as you can see all the columns that we discussed earlier from country,


03:22:39
price, province all the way till a detailed description of the wine is present here. So now that that's done, next we need to do some pre-processing to limit the number of varieties of wine in the data set. So let us go ahead and set a threshold of 500 in our case. So anything less than 500 will be removed from analysis in our model. we'll be replacing it with nan which is not a number instead of letting it to be blank. So let me go ahead and run this. So now that that's done the next step we


03:23:07
need to do is actually split the data into a training data set and the testing data set. So let's go ahead and do that and print the size of both the training data set and the testing data set. So let me go ahead and run this code for you guys. And there it is. We have the training data set size and the testing data set size. Now that we have the size, we need to actually extract the testing and the training features and all of the labels. So let me go ahead and run the score and we can actually


03:23:30
get the labels. And so that does it. The training and the testing features and the labels are known to our machine by now. So now it's very obvious that we're using a test description. Well, instead of looking at every word that we have found in our description in our data set, let us limit our bag of words to let's say top 12,000 words in our data set. So guys, don't worry. There is actually a built-in kerass utility for creating just this vocabulary. This is considered as wide because the input to


03:23:56
our model for each description will be a 12k element wide vector with zeros and ones indicating the presence of word from our vocabulary in a particular description. Well, kas has some handy utilities for text prep-processing that we'll use to convert the text descriptions into a bag of words. With the bag of words model, we'll typically want to use only a subset of all the total words that we found in our data set. And in this example, I used 12,000 words, but this is a hyperparameter that


03:24:23
you can tune. Well, you can try out a few values to see what works best on your data set. Well, we can use the kas tokenizer class to create our bag of words vocabulary for us. So, let me go ahead and run this to create the tokenizer, guys. So now that that's done, we'll be actually using text to matrix function to convert each description to a bag of words vector. So let me go ahead and run it. Well, now that that's done guys, in the original Kaggle data set, there are about 632 total varieties of wine. To


03:24:52
make it easier for our model to extract the patterns, we did a bit of pre-processing to keep only the top 40 varieties. Well, around 65% of the original data set or 96k total examples. Well, we'll use a kas utility to convert each of these varieties to integer representation and then we'll create 40 element wide one hot vectors for each input to indicate the variety. So let me go ahead and run it guys. So now that that's run guys at this stage we are ready to build our wide model. Well


03:25:21
guys, KAS has two APIs for building the models. The sequential API and the functional API. The functional API gives us a bit more flexibility in how we define our layers and lets us combine multiple feature inputs into one layer. It also makes it easy for our wide and deep models into one when we're ready. Guys, with the functional API, we can define our wide model in just a few lines of code. As you see, well, first we need to define our input layer as a 12k element vector. Well, for each word


03:25:48
in our vocabulary and then we'll connect this to our dense output layer to generate the price prediction. So, let me go ahead and run this. Well, now that that's done, we'll compile the model so that it is ready to use. If we were using the wide model all on its own, this is when we'd actually start training it with the fit function and evaluate later with the evaluate function. Since we're going to combine it with our deep model later on, we can hold off on training until the two


03:26:12
models are combined, which is done later, guys. So, let me go ahead and execute this. So, we define our wide model. And yep, our wide model is done. Let's go ahead and print out a summary from the wide model. Well, now that we have a summary, we can realize the total number of trainable parameters and non-trainable parameters. Well, in our case, the non-trainable parameters are zero, guys. So, guys, that's the end to the construction of the wide model, and it's time to build our deep model. So, let's go ahead and


03:26:38
check that. Well, to create a deep representation of the wine's description, we'll represent it as an embedding. Well, there are a lot of resources on word embeddings, but the short version is that they can provide a map word to vector so that the similar words are closer together in the vector space. Well, to convert our text descriptions to an embedding layer, we'll need to first convert each description to a vector of integers corresponding to each word in our vocabulary. We can do that with the


03:27:03
handy kas text to sequence method. And now we've got the integerized description vectors. We need to make sure that they're all the same to feed into our models. Well, Kasa is fancy and it has a handy method for that, too. We'll use pattern sequences to add zeros to each description vector so that they're all the same length. Well, in this case, I used 170 as the max length, so no descriptions were cut short. So, let me go ahead and run this. Well, with our descriptions converted to


03:27:31
vectors that are all the same length, we're ready to create our embedding layer and then feed it into our deep model, guys. So, let's start building our deep model. Well, there are two ways to create an embedding layer. We can use weights from the pre-tained embeddings as I previously explained or we can actually learn the embeddings from our vocabulary. Well, it's best to experiment with both and to see which performs better on your particular data set. Here we'll consider using learned


03:27:55
embeddings. Well, firstly, we'll define the shape of our input to the deep model. Then we'll feed it into the embedding layer. And here I'm using an embedding layer with eight dimensions. Well, you can experiment this with tweaking the dimensionality of your embedding layer as per your choice. And the output of the embedding layer will be a three-dimensional vectors with the following shape. Well, it'll have a bath size, a sequence length. Well, in our case, the sequence length is 170. It'll


03:28:21
have an embedding dimension. It is 8 in our example. And in order to connect our embedding layer to the dense fully connected output layer, we actually need to flatten it first. So, let's go ahead and define our model. And then we can flatten our layer. Well, once the embedding layer is flattened, it's ready to be fed into the model and to compile it. So, let's go ahead and compile it. And now that the compilation is done, as you can see, the loss function we've used in this case is MSC, which is mean


03:28:46
square error. The optimizer we'll be using is Adams and the metrics is accuracy. And at this point of time, we have established the wide model and the deep model. So once we have defined both our models, combining them is really, really easy guys. We simply need to create a layer that concatenates the output from each of the model and then merge them into a fully connected dense layer and finally define a combined model that combines the input and the output from each one. Well, obviously since each model is predicting the same


03:29:14
thing which is the price, the outputs or the labels from each one will be the same. Also guys, do note that since the output of our model is a numeric value, we will not need to do any pre-processing and it's already in the right format as well. How cool is that? Well, that that's done guys, it's time for the training and the evaluation. Well, you can experiment with the number of training epochs and the bat size that works best for your data set. Well, this is going to take some time, so I'll save


03:29:41
you the pain by fast forwarding this a bit. So guys, as you can see, the training is actually done. So each of the epoch took about 100 seconds and we had 10 epochs for the same. So guys, here's the important thing that you have to notice. With every epoch, we were actually reducing the loss all the way from 1,100 to 130, guys. And the accuracy of prediction went from 0.02 all the way till 0.0994, which is almost 0.1. Well, wow. That's definitely a breakthrough for just 10 passes, guys. And now that the training


03:30:20
is done, it's time to evaluate it. So, let me go ahead and run this piece of code for you guys. So that was quick. That took only about 5 seconds and we have evaluated the model. And now it's time for the most important part guys. Seeing how our model actually performs on the data that it has never seen before. To do this, we'll actually call the predict function on our trained model and we'll be passing it our data set. So let's go ahead and do just that. Well, now that that's done, we'll have


03:30:47
to compare the predictions to the actual values for the first 15 wines from our test data set. So guys, as you can see, we have a set of predictions from the description and the predicted value is about $24 while the actual value is $22. Next up, we have $34 as a predicted one, while the average is 70. Well, this is not a really good case, but okay, that's tolerable. And next up, we have predicted 11.9 when the actual value still. Wow, that is actually really close. So, next up, we have 15.7 versus


03:31:13
9. Well, this goes on and on for the first 15 and it's actually really, really good. Well guys, pretty well it turns out that there is some relationship between a wine's description and its price. Well, we might not be able to see it instinctively, but our machine learning model certainly can. So, lastly, let's compare the average difference between the actual price and the mold's predicted price. Well, the average prediction difference is about $10 for every wine bottle. Wow, that is really,


03:31:39
really nice, guys. [Music] Generative AI is transforming multiple creative fields. Image generation tools power visual design. Music compositions algorithm create original scores and AI assist video editors in automatic task. LLM help generate and translate text while code generation tools like GitHub copilot boost developer efficiency. AI generated voices are even being used in audio books and voice assistants. So now let's take some of these tools and check. So this time we will use Ptory AI


03:32:18
and Flicky AI. First let's explore Ptory AI. So for that let's go to its site and check its functions. So we are at the Ptory AI site and on the left side we have the home project and brand kits and on the main screen we have different features Ptory AI provides. So let's choose text to video. Here, let's write some names and description and press generate. Pictory AI is a tool designed for video creators that helps transform long- form content such as articles or blog post into short engaging videos. It uses AI


03:32:56
to automatically extract key highlights and create professionallook videos with minimal efforts. Due to its simplicity and time-saving capabilities, Ptory AI is a popular for social media content creation and marketing. Now let's see our next tool which is flicky. So now we are at the flicky.ai site. Here we have different features like videos where you can create videos from all of these blogs, prompts etc. You can also create audios from these features and then we also have a design feature


03:33:29
and on the left hand side you can see options like files, templates, brand kits, voice clones etc. So now let's take an idea and convert it into a video. Now let's write our topic and generate. Flicky AI is a content creation tool that turns text into videos using AI generated voices and visuals. It helps users create professional videos quickly by pairing written content with a stock images, animations, and voiceovers. Flicky is ideal for marketers, content creators, and educators looking to create engaging


03:34:04
video content efficiently. Now that we have seen the applications, so let's step back and look at the journey that brought us here. So basically our journey starts in 1947 with Alan Turing's concept of intelligent machines. By 1961, Joseph Venbomb introduced Ela, the first chatbot. The 1980s saw the birth of recurrent neural networks while 1997 brought long short-term memory networks to tackle sequential data and then GANs emerged in 2014 transforming creative task. Fast forward to 2017 when


03:34:42
transformers like GPT entered the scene. By 2023, GPT3.5 and Google's farm marked significant milestones and by 2025 we are on the brink of AI breakthroughs in chemistry and genome editing. So what exactly are these LLMs and why are they so powerful? An LLM or large language models analyzes and understands natural language using machine learning. Examples include OpenAI's GPT, Google Spal, and Metas Lama. These models drive applications such as chat bots, language translation, and more by learning from


03:35:21
extensive data to predict and generate text sequences. But before this, there was a very famous term called language model. A language model is a machine learning model that uses probability statistics and mathematics to predict the next sequence of words. Suppose you have a sentence like I have a boy who is my dash. Here if we ask a language model to predict the next word, it considers the context provided by the words before the blank. Based on common usage patterns from its training data, it may


03:35:52
predict words like boyfriend, brother or friend which fit naturally. However, it's less likely to predict colleague or sibling as those words may not commonly follow these type of phrases. So this process shows how language models predict text by calculating probabilities for each possible word based on their likelihood in context. So when a language model is trained on massive amounts of diverse text, it gains a wider vocabulary and more understanding of language enabling it to make more accurate predictions. For


03:36:24
example, if we give it a phrase like you are a dash to me, a model trained on extensive data might suggest various fitting words. for example, friend, inspiration or anything else. So based on the sentiment or context, it has learned from the data. Now here reinforcement learning is used to improve the model's responses over time. By giving feedbacks, be it positive or negative on the responses, we help the model learn which type of responses are preferred in specific context. For example, if the model frequently


03:36:57
misinterprets the tone or intent, the reinforcement learning helps adjust its productions to be more contextually appropriate and aligned with the intended meaning. But what do these models look like under the hood? Well, LLMs are built on neural networks composed of input, hidden, and output layers. The hidden layer process information to learn complex patterns and more layers means the model can capture deeper insights. This structure allows LLM to perform task from generating text to complex code


03:37:30
completions. Now how do these layers interact and function in real time? Now LLM is based on the transformer and a transformer uses deep learning to process any information coming to it. Now let me tell you a story of three friends. Imagine we have three characters. First is our friend. The next character is minion Bob and the third character is Gru. So our friend asks Bob, "What's the price of the jet? It must be $50,000." Minion Bob isn't sure. So he goes to Gru and asks, "Is


03:38:03
the jet $50,000?" Gru replies, "No, it's $70,000." In this back and forth, Minion Bob is like the neural network layer trying to make an accurate guess. So each time he goes back to group like receiving more data or feedback he gets corrected if his guess is wrong leading him to refine his response. Now after the first check minion Bob returns to our friend saying I guess it's more than $60,000. Our friend assumes it might be around $65,000 and sends Bob back to Gru to verify. Again group corrects him no it's


03:38:40
actually $70,000. So this process repeats with Bob adjusting his guess each time. Eventually he learns that the correct answer is $70,000 and updates his knowledge. So just like minion Bob, neural networks make initial guesses based on available information with each feedback loop like Bob going back to group the model's hidden layers adjust the parameters to refine its guesses ultimately arriving at the most accurate prediction possible. So after getting corrected multiple times, Minion Bob's


03:39:13
guesses improve until he knows the price is $70,000. Similarly, in a neural network, gradually learning the correct answer through training. So once the network learns, it can give accurate answers in future cases without checking every time. Now let us move on to understand how LLMs work. LLMs begin the collection of data sets, then tokenize text and break it into a manageable pieces. Using a transformer architecture, they process the data sequence all at once, leveraging vast training data. Llms


03:39:47
contain millions of learn parameters that predict the text tokens and generate coherent outputs. Models often undergo pre-training for general knowledge and fine-tuning for specific task. So now let's see some practical uses of LLMs. LLMs power content generation, creating anything from articles to code. They excel in language translation, enhanced search engines, personalized recommendation, code development assistance, and sentiment analysis, which also owe much to LLM's predictive capabilities. So guys, are you ready to


03:40:22
use all that knowledge in coding and witness how these LLMs come together to drive innovation? Whether through developing applications, analyzing data or building smart assistance, the gear of technology keep turning to unlock AI's full potential. So now let us look at our problem statement. So one of the difficulties in the healthcare industry is effectively evaluating medical pictures such as MRIs, CT scans and X-rays in order to identify anomalies and illnesses. This procedure takes a lot of time and calls from specialized


03:40:56
understanding. Automated methods must be developed to help medical personnel recognize possible health problems in medical imaging. In order to provide better patient care, a system that integrates cuttingedge machine learning models with image analysis can greatly help in the early detection of diseases including cancer, infections, and other illnesses. So the method uses generative AI to evaluate medical photos and generate a thorough diagnosis report based on the findings. This technology allows users to upload medical images


03:41:28
which the AI model then processes. Now let us build our project on a medical image analysis application using Streamlit Python and an LLM of Google Gemini AI. So this app helps health care professional analyze medical images such as X-rays, MRIs and CT scans to detect anomalies and diseases. First let's import the necessary libraries. So first import streamlit as st. So if this is not working or showing an error then open the terminal and write pip install streamllet and from path lib import path.


03:42:14
Next import google dot generative AI as gen AI. So we are importing streaml for the app interface and path from path lip for handling file paths and Google generative AI which allows us to interact with the Gemini AI model. Next we will configure Google's Gemini API by setting up our API key. So this will allow us to connect to the AI model and generate insights from medical images. So before proceeding let's get our API key and we will go to the Google to generate an API key. So on your left there is an API key


03:43:00
option and after clicking you will get the create API option. So just select your model and create your API key. So as you can see the screen just copy this API key and go back to terminal. So now let's configure our model. So just type gel AI dot configure and inside the bracket give API key equal to and over here paste the key. Now we set up the system prompt which defines the role of the AI model. So the prompt specifies that our AI is a medical image analysis system capable of detecting diseases like cancer,


03:43:47
cardiovascular issues, neurological conditions and more. So guys I have already researched the prompts and written here. So basically the system prompt should be inside the triple quotes. So this prompt guides the model to analyze medical images for conditions such as cancer, fractures, infections and more making it a valuable tool for healthcare professionals. Now let's configure the model settings for generating responses. We define parameters like temperature and top K to control the creativity of the model's


03:44:21
output. First type generation_config equal to and inside the double quote we will give temperature which is 1 then top_p which is 0.95 next top_k 40 then max output tokens which is 8192 two next response_m type which is of text or plane. So over here the temperature one that controls randomness a value of one given balanced output diversity. Next top P 0.95 uses nucleus sampling selecting tokens from the top 25% cumulative probability distribution for diverse responses. Next, the top K 40 which


03:45:22
limits token selection to the top 40 tokens based on the probability. Narrowing possible outputs to high probability tokens. Next, max output token. This setting allows for longer responses by limiting the maximum length of the generated text to 8,192 tokens. And then we have response m type which specifies the format of the output as plain text. So for more information read the Google Gemini documentation. Next we will also configure safety settings to ensure that the model doesn't generate harmful content. So for


03:45:56
example we block categories like harassment, hate speech and sexual explicit content. Here we are using two things. First categories and then the threshold. Then copy this four times like harassment, hate speech and sexual explicit content. Now let's set up the layout for our streamlit application. So for that we will configure the title and the layout of the page and even add a logo to make the interface more user friendly. So first type st dot set page_config and inside the bracket let's give page


03:46:40
title equal to and inside the double quotes we will give diagnostic analytics comma page icon equal to robot. Now let us type column 1 comma column 2 comma column 3 is equal to st dot columns and inside the bracket give 1a 2a 1 1 comma 2a 1 next with column 2 so I'll be using edureka and medical images so this will show you how to set up images using streaml now type st dot image and give a bracket and inside the double quotes let's type edurea dotpng and give a comma and give width is equal


03:47:36
to 200. Now let us copy and paste it for medical. So let's type medical.png. Here we are using streamllets column to center the logo and title and this makes the app look professional and visually appealing. Next, let's allow the user to upload medical images for analysis. So, we use Streamlits file uploader widget to accept image file in PNG, JPG or JPEG formats. For that, let's type upload file equal to ST dot file uploader and inside the bracket inside the double quotes, let's type please


03:48:19
upload the medical images for analysis. Comma type is equal to so basically the image type is equal to and inside the bracket inside the double quotes let's give PNG comma jpg and jpeg next let us type submit button equal to st dot button and inside the bracket let's give generate image analysis is so here when the user uploads a file and click the generate image analysis button the model process the image and prepare it for analysis. So once the user submit the image we send it to the AI model for


03:49:07
analysis and then the model generates a response based on the prompt and image which we then display in the app. So here as you can see the screen we have another function. So the if submit button which runs the code when the submit button is pressed. Next the image data is equal to upload file.get value. This actually gets the raw image data from the uploaded file. And next we have the image parts where it creates a list with the image data in a structured format. Then we have the prompt parts.


03:49:41
So this combines the image data and a text prompt for the model. So this part of the code actually sends the image and text prompt to the model to generate a response. And then we have the st.right which displays the model's responses in the app. So here we use the image data and system prompts to generate content with the Gemini AI model. The result is displayed as a detailed report with insights about the medical image. Now it's time to test the code. So open the terminal and type streamllet run main.


03:50:15
py. So once you enter it will redirect you to our model interface. And there you go. So the model is ready. So here's a live demo of the app. We will upload a sample image and the app will analyze it and provide a detailed diagnosis based on the AI models inside. So this is how we use streamlit and Google's Gemini AI model to create a medical image analysis app. So this app can help medical practitioners by offering precise and thorough analysis of medical photos. Now it is the time for testing. So let's


03:50:53
take one image of any disease and test it. So upload the image from your computer. Then we will select an image and press the generate button. So as you can see it's running. So it generates fabulous response and can help doctors in assisting their patients saving time and money. So this is how we built a realtime medical diagnostic helper using Streamlit Python and Google Gemini AI. [Music] Let us understand what are GANs. So we going to start with generative models. So generative models are nothing but


03:51:38
those models that use an unsupervised learning approach. In a generative model, there are samples in the data that is input variables X, but it lacks the output variable Y. And we use the only input variables to train the generative model. and it recognizes patterns from the input variables to generate an output that is unknown and based on the training data only. In supervised learning, we are more aligned towards creating predictive models from the input variables. And this type of modeling is also known as discriminative


03:52:13
modeling. And in a classification problem, the model has to discriminate as to which class the example belongs to. And on the other hand, unsupervised models are used to create or generate new examples in the input distribution. To define a generative model in layman terms, we can say generative models are able to generate new examples from the sample that are not only similar to the examples but are indistinguishable as well. And the most common example of a generative model is a knive bias classifier which is more often used as a


03:52:49
discriminative model. Other examples of generative models include gshian mixture model and a rather modern example that is generative adversial networks. So let us try to understand what exactly are GANs or generative adversarial networks. Generative adversial networks or GANs are a deep learning based generative model that is used for unsupervised learning. It is basically a system where two competing neural networks compete with each other to create or generate variations in the data. It was first


03:53:23
described in a paper in 2014 by Ian Goodfellow and a standardized and much stable model theory was proposed by Alec Ratford in 2016 which is also known as DGC also known as DC GAN or we can call it as deep convolational general adversial networks and most of the GANs today use deep convolational general adversial networks. The GANs architecture consists of two submodels known as the generator model and the discriminator model. So a generator network takes a sample and generates a sample of data. A


03:54:00
discriminator network decides whether the data is generated or taken from the real sample using a binary classification problem with the help of a sigmoid function that gives the output in the form or the range 0 and one. So let us go ahead and take a look at how GANs actually work. To understand how GANs work, let's break it down. So generative means that the model follows the unsupervised learning approach and is a generative model. When we talk about adversarial, the model is trained in an adversarial setting. And


03:54:34
network simply means for the training of the model, we use the neural networks as artificial intelligence algorithms. In GANs, there is a generator network that takes a sample and generates a sample of data. And after this the discriminator network decides whether the data is generated or taken from the real sample using a binary classification problem with the help of a sigmoid function that gives the output in the range 0 to one. The generative model analyzes the distribution of the data in such a way


03:55:04
that after the training phase, the probability of the discriminator making a mistake maximizes. And the discriminator on the other hand is based on a model that will estimate the probability that the sample is coming from the real data or not the generator. The whole process can be formalized in a mathematical formula. So G over here is generator. D is equal to discriminator. P data X is the distribution of real data. P data Z is the distributor of generator. X is the sample from the real data and Z is the sample from generator


03:55:39
where DX is the discriminator network and GZ is a generator network. So let's take a look at the flowchart once again guys. So we have the training data which is going to give the real sample and the generator network is going to generate the sample from the random noise or the examples and then it will go to the discriminator network where it's going to check if the sample that is coming is real or fake. So that is how a GAN actually work. Now let's take a look at the training phase like how a generative


03:56:10
adversal network is actually trained. So it happens in two phases guys. So the first phase is where we train the discriminator and we actually freeze the generator which means that the training set for the generator is turns false and the network will only do the forward pass and there will not be any back propagation. Basically the discriminator is trained with real data and checks if it can predict them correctly and the same with the fake data to identify them as fake. After this there's the second


03:56:41
part where we train the generator and freeze the discriminator. So we get the result from the first phase and we use them to make better from the previous state to try and fool the discriminator better. So to understand this in the layman's term I'm going to tell you a few steps for training like how you should start. So the first step is you have to define the problem. You have got to define the problem and collect the data. After this the second step is you have to choose the architecture of GAN.


03:57:10
So in this step depending on your problem you have to choose how your GAN should look like. The third step is training the discriminator on real data. So we train the discriminator with real data to predict them as real for any number of times or we call it epochs as well and then we generate the fake inputs from the generator. So in this step we are going to generate the fake samples from the generator. And the next step is we train the discriminator on fake data. So whatever samples are generated from the generator network,


03:57:41
we're going to train the discriminator to predict the generator data as fake. So that's how we know that discriminator is actually predicting the values as correctly. And the last step is we train the generator with the output of discriminator. So after getting the discriminator predictions, we train the generator to fool the discriminator. So that's how we train the GAN to actually get our solution from the problem which is like defining the problem. So you'll understand this when I'm talking about


03:58:09
the applications guys. So no worry. Now let's go ahead and take a look at a few challenges of generative adversial networks. So the concept of GANs is rather fascinating but there are a lot of setbacks that can cause a lot of hindrance in its path. Some of the major challenges faced by GANs are the first one is the stability. So there has to be a stability that is required between discriminator and the generator network. Otherwise the whole network would just fall. For example, in case let's say if


03:58:39
the discriminator is too powerful, the generator will fail to train altogether. Won't be able to push fake samples to the discriminator and it will always identify them as fake. And let's say if the network is too lenient, the discriminator network is too lenient. So any image that would be generated by the generator network would make the network useless. The next challenge that is faced by GANs is GANs fail miserably in determining the positioning of the objects in terms of how many times the


03:59:09
object should occur at that location. Suppose we have a image in which we have let's say three dogs with two eyes and sometimes a GAN will fail to you know determine the positioning of the objects in terms of it will generate an image with like one dog and six eyes. So that's kind of a problem that we face while working on GANs and the next challenge is 3D perspective troubles GANs as it is not able to understand the perspective also. So it will often give a flat image for a 3D object. So that's


03:59:40
one challenge that we face with GANs as well and GANs have a problem of understanding the global objects and it cannot differentiate or understand a holistic structure like if you're talking about trees or if you're talking about flowers that's a problem the GANs will follow and last but not the least newer types of GANs are more advanced that I've talked about that is deep convolutional generative adversial networks and are expected to overcome these shortcomings altogether. So we


04:00:09
don't have to worry about these. These are the shortcomings that we face with normal GANs. The initial generative adversal networks. Now that they have become more advanced, they actually overcome these shortcomings. So you don't have to worry guys. So last but not the least, I want to talk about a few applications of generative adversal networks. So the first one is prediction of next frame in a video. So let's say the prediction of future events in a video frame is made possible with the


04:00:35
help of GANs and DVD GAN or we can call it as dual video discriminator GAN can generate a 256x 256 videos of notable fidelity up to 48 frames in length and this can be used for various purposes including surveillance in which we can determine the activities in a frame that gets distorted due to other factors like rain, dust, smoke etc. So the possibilities are immense with this. If you are able to predict the next frame in a video that actually helps in a lot of things like surveillance, security


04:01:08
and we can predict outcomes based on these frames that we generate from a video. After this comes the text to image generation. So basically object-driven attentive GAN which is also known as object GAN performs the text to image synthesis in two steps. So the first step is generating the semantic layout and then generating the image by synthesizing the image by using a deconvolational image generator is the final step. So this could be used intensively to generate images by understanding the captions, the layouts


04:01:39
and refine details by synthesizing the words. And there is another study about the story GANs that can synthesize the whole story board from mere paragraphs. So that's actually very good idea if you're talking about GANs. So we can just give a few layouts and captions. Based on that it will generate image for us. Talking about the next application we have image to image translation. So pixtopix is a model which is designed for general purpose image to image translation. So let's say we have three


04:02:10
images. We have a real image. Then we'll be having a generated image which is basically a fake and then it will be reconstructed to the previous image which was real. So this is how image to image translation work guys and after this we have enhancing the resolution of an image. So super resolution generative adversial network or also known as SR GAN is a GAN which can generate the super resolution images from low resolution images with finer details and better quality. So this is actually a


04:02:43
very good application of GANs guys. The applications can be immense. So you imagine a higher quality image with finer details generated from a low resolution image. The amount of help it would produce to identify details in lower resolution images can be used for wider purposes including surveillance. We can use it for documentation, security, we can use it for detecting patterns etc. And last but not the least we have interactive image generation. So GANs can be used to generate interactive images as well. And computer science and


04:03:17
artificial intelligence library also known as CS AIL has developed a GAN that can generate 3D models with realistic lighting and reflections enabled by the shape and texture editing. And more recently, researchers have come up with a model that can synthesize a reenacted face animated by a person's movement while preserving the appearance of the face at the same time. There are a lot more applications we can work on. [Music] So what is a transformer? Transformers operate on a concept called


04:03:56
sequencetosequence learning. Essentially they take a sequence of tokens as an input and predict the next token in the output. A great example of this is language translation. Imagine inputting good morning in English and the transformer process this and outputs the translation in languages like Japanese, Korean or German. The key is how it efficiently process the relationship between words. Since we know what a transformer is, let's dig a bit deep about them. A transformer has two primary components encoder and decoder.


04:04:31
Encoder identifies relationship between parts of the input sequence whereas the decoder uses these relationships to generate the output sequence. This division is what allows transformers to handle task like text translation or summarizations with remarkable accuracy. Now that we have the idea of transformers, let's discuss how they evolve. Before transformers, there were other neural networks like RNN, recurrent neural networks invented by David Rumlhur in 1986. However, RNN's face significant


04:05:03
challenges. They would forget early parts of the sequence as they processed longer ones and couldn't handle dependencies efficiently. Additionally, RNN relied on recurrence which made them inefficient and incapable of parallelization. Then came long short-term memory introduced by Howeter and Smith Huber in 1997. Long short-term memory improved by remembering sequences for a longer duration and addressing some of the memory issues in RNN. However, they were slow to train and difficult to manage at


04:05:38
scale. Finally, transformers transformed neural networks. First introduced in the landmark paper, attention is all you need. Transformers addressed all the problems faced by RNNs and LSTMs. They used a completely attention-based mechanism, eliminating reliance on recurrence. This made transformers capable of remembering context efficiently, training faster and being parallelized, enabling multitasking and significantly speeding up processes. Now, let's discuss on the attention mechanism. Think about the sentence.


04:06:13
This cat wants to jump on the box. The attention mechanism identifies the most relevant parts of this sentence like cat, jump and box and focuses on these elements while processing the data. Now that we know how transformers have evolved, now let's discuss their architecture. A transformer consists of two main components, an encoder and a decoder. Each typically consisting six layers. Inside the encoder, there is one attention layer and one feed forward layer. While the decoder contains two


04:06:47
attention layers and one feed forward layer. The magic of parallelism comes from how data is fed into the network. In the attention layer, all the words are processed simultaneously with each word forming combinations with other in the sentence. This allows the model to capture relationships and context efficiently. After processing in the attention layer, the data is sent to the feed forward layer where it is learned layer by layer. The input to the encoder and decoder are the raw input embeddings


04:07:18
which are numerical representation of words. On top of these embeddings, positional encodings are added to help the model understand the position and the order of each word in the sequence. If we simplify embeddings, there are essentially vector representation of words in an n dimensional space. At the top of the architecture, there are two layers of the output probabilities converting the final output into the form of human can understand. These inputs are represented as vectors with their length corresponding to the size


04:07:49
of vocabulary. Now what truly makes transformer unique is the inclusion of normalization layers which normalize the output from sub layers. Additionally, skip connections, the dark arrows in the architecture, forward critical information that bypasses self attention or fed forward layers directly to the normalization layers. This ensures the model does not forget important details and effectively passes vital information further into the network. Now moving forward, let's discuss why transformers are important.


04:08:23
Transformers are vital because they utilize semi-supervised learning. They are trained on massive unlabelled data sets enabling them to generalize across a wide range of task. Unlike older models, transformers don't need to process data sequentially. Their attention mechanism allow them to focus on the most relevant context which significantly speeds up training. Transformers revolutionize data processing by eliminating the need to handle data sequentially allowing for parallel processing and significantly


04:08:53
enhancing efficiency. The attention mechanism lies at the core of transformers, enabling the model to focus on the most relevant parts of the input sequence and improving accuracy and understanding of context. Furthermore, transformers excel at providing context, ensuring that the meaning of each word or token is accurately interpreted within its surroundings. Lastly, these models dramatically speeds up the training process, making them faster and more efficient compared to traditional neural networks, thus redefining AI's


04:09:26
capabilities across diverse applications. Now that we know why transformers are important, let's discuss some applications. We have OpenAI's GPT, a groundbreaking model that leverages the power of transformers for natural language processing task. Additionally, Google has developed several transformer based models including vision transformer for image recognition but birectional encoder representations from transformers for understanding the context of words in a sentence and T5 which stands for texttoext transfer


04:10:00
transformer for a wide range of text generation task. Microsoft has also contributed with debirth decoding enhanced bird with disentangled attention a model designed to improve contextually understanding and enhance NLP applications. These models demonstrate the versatility and impact of transformer architecture across various domains. Now that we know the application of transformers, how about checking their realtime products? Transformers have become an integral part of many real world products that we


04:10:33
use style. Examples include Grammarly which leverages transformers for advanced grammar and writing assistance. Google search and its translation tools powered by models like bird and t5 and chat GPT open AI's conversational AI that relays on the generative pre-train transformer architecture. Additionally, Meta's deep fake detector uses transformer-based models for facial recognition task. These applications highlight how transformers have revolutionarized technology, seamlessly integrating it into tools that enhance


04:11:07
our everyday lives. In conclusion, transformers are changing the tech world by enabling smarter, faster, and more efficient AI systems. Whether it's generating text, translating languages, or enhancing search engines, these models are the cornerstone of modern AI. [Music] Imagine you're developing a virtual assistant application using a large language model such as GP3. The goal is to provide users with an engaging and helpful experience by designing effective prompts that generate informative and relevant responses from


04:11:45
the model. So let's consider a scenario in which the virtual assistant assist users with the travel planning. So here's how prompt engineering plays a major part. So the scenario is you're planning a trip to Paris and want the virtual assistant to provide recommendations for activities, restaurants, and landmarks to visit during your stay. So let's say you're looking for a help with a traditional prompt and you ask for what should I do in Paris and virtual assistant will assist you here like here are some


04:12:17
recommendations for activities in Paris and here's how the enhanced prompt through prompt engineering respond to your queries. So if you input a query that goes like hey there I'm super excited about my upcoming trip to Paris. So could you please recommend some must visit places and activities for me then the virtual assistant will generate the response as something like this. So of course Paris is an amazing city with so much to offer. So here are some must visit places and activities and it


04:12:48
continues with the explanation about each place. I hope you got the idea of how enhanced prompt provides users with an engaging and helpful experience by designing effective prompts that generate informative and relevant responses from the model. So now let us understand what exactly is prompt engineering. Prompt engineering is a method used in natural language processing that is NLP and machine learning. It's all about crafting clear and precise instruction to interact with large language model like GPT3 or B. So


04:13:20
this models can generate humanlike responses based on the prompts they receive. Think of prompt engineering as giving direction to these models. By crafting specific and concise prompts, we guide them to produce the response we want. So to do this effectively, we need to understand the capabilities of the model and the problem we are trying to solve. Fine-tuning prompts allows researchers and developers to improve the performance and usability of LLMS for a variety of applications including text generation, question answering,


04:13:51
language translations and others. Effective prompts engineering necessitates a thorough understanding of the underlying models capabilities as well as the problem domain and desired result. Now let's find out why prompt engineering matters for AI. So prompt engineering is important in AI because it improves model performance, customization and reliability. By creating clear and tailored prompts, developers can help AI models produce more accurate and relevant result, reduce biases, improve user experience


04:14:24
and address ethical concerns. In simple terms, prompt engineering ensures that AI system produce useful and reliable result that meets the needs of users while adhering to ethical principles. So now let's consider an example in the context of text generation for generating product description. Assume you're using an AI model to create product description for an online store. So without prompt engineering, you may issue a generic prompt such as generate a product description for a smartphone.


04:14:54
So without prompt engineering you would get something like this. This smartphone has a high resolution display, powerful processor and a longlasting battery life. The given prompt is less effective because it lacks specificity. So it simply says generate a product description for a smartphone. So this may make it difficult to come up with an idea and write something engaging and informative. So having a good prompt can make a significant difference in your writing. They give you a clear idea of


04:15:23
what you need to write about and keep you focused and organized making it easier to generate ideas and express yourself. On the other hand, by using prompt engineering techniques, you can provide more specific instructions or constraints that will tailor the generated descriptions to the target audience or brand style. So with prompt engineering if you input a query such as create a product descriptions for a budget friendly smartphone perfect for the young professionals highlights it's affordable sleek and packed with a


04:15:53
top-notch camera features and the generated response would be something like this. Introducing our sleek and affordable smartphone designed for young professionals with its stylish design and advanced camera features capturing life's moments and have never been easier and it goes on giving its key features along with it. So through this example we understood that prompt engineering enables the creation of a product description that is useful to the target audience and highlight specific features based on the


04:16:23
instructions provided. So this shows how prompt engineering can improve the importance and effectiveness of AI generated content for specific applications. To help AI models give accurate answers, it's important to create clear prompts. So here are some simple rules for generating effective prompts. First, make it clear. So clearly explain what you want the AI to do. Unclear prompts might confuse the AI and lead to wrong answers. So make sure that the prompts is clear. For example, the unclear prompts is something like


04:16:55
write about cars. So where we have not mentioned which type of car or anything much in details whereas the clear prompt is write a description of a red convertible sports car. Next give context. So provide enough information so that AI understands the task. So this helps it give accurate response that makes sense in the given situation. So for example, prompt without context is write a story. Prompt with context is write a story about a girl who discovers a magic book in her ethic. Next, show examples. Use examples to show the AI


04:17:32
what you are looking for. So this helps it understand the type of response you want. So for example, the prompt without example is describe a beat scene. So prompts with examples is describe a beach scene with the palm trees, crashing waves, and people playing volleyball. Next is keep it short. So don't overload the AI with too much information. Short prompts help the AI focus and give quicker, more accurate responses. For example, long prompts are like this. Write a detailed essay discussing the impact of climate change


04:18:08
on biodiversity and ecosystems in tropical rainforest. And short prompts look something like this. Write about climate change effects on rainforest. Next, avoid biases. So, make sure your prompts are fair and don't include any unfair assumptions. So, biased prompts can lead to biased answers which isn't helpful. So, for example, write about a woman who struggles with her weight. So unbiased prompts are right about a person overcoming challenges. Next, set limits. So tell the AI any rules or


04:18:42
restrictions it needs to follow. This helps guide its response and ensures they meet your specific needs. For example, prompt without limits are write a story and prompt with limits are write a story set in a haunted house with a maximum word count of 500 words. And I hope it's very clear. Next moving on to some example of prompts for generating text using chat GPT. For text generation task, prompts usually consists of a textual instructions or starting point that directs the model to produce


04:19:14
coherent and relevant text. Prompts can be story prompts, questions or incomplete sentences. Text generation prompts provide context and directions to the model allowing it to generate humanlike text responses. They influence the generated text tone, style, and context. So let's say the prompt is write a short story about a character who discovers a hidden treasure. So by providing a specific story line and theme in the prompt, the model is guided to generate a coherent and engaging narrative centered around the discovery


04:19:46
of a hidden treasure. So the picture illustrate how tragic crafts stories with the engaging touch making them more captivating and interesting for readers. Next question answering. So prompt is can you describe the common signs and symptoms of COVID 19 along with any precautions that can be taken to stay safe and just like that it can generate answers to all your questions in mere seconds. So by framing the prompt as a question the model is directed to provide a concise answer regarding the symptoms of COVID 19 ensuring relevant


04:20:21
and informative responses. Next, language translation. Translate the given English sentence. The quick grown fox jumps over the lazy dog into Spanish while maintaining its original meaning. So, by specifying the source and target language in the prompt along with the input sentence, the model is instructed to perform a precise translation task ensuring accurate language conversion. Next, code autocomp completion using OpenAI codeex or Chadi. you can perform code order completion task. So here we


04:20:56
go with TPD. Code generation prompts are usually partial code snippets or descriptions of programming task. They specify the desired functionality or behavior that the model should show. Code generation prompts allow the model to generate code that satisfied specific programming requirements such as implementing algorithms, defining functions, or solving coding problems. So the prompt is complete the following Python function to calculate the factorial of a number and here you have also added the function. So by


04:21:28
presenting an incomplete code snippet along with clear instructions the model is directed to suggest appropriate code completion helping developers write code more efficiently. Now moving on to the text to image generation. Image generation prompts specify the visual sense, objects or concept that the model should generate. They may include textual descriptions, keywords or images. So image generation prompts allow the model on what visual content to generate. They influence the generated images, composition, style and


04:22:02
detail. For example, the prompt is imagine a tree where the branches are made of stacks of books. So can you paint me a picture of that? And for the given prompt, we got the image generated as something like this. An imaginative portrayal of a tree with branches composed of stack books. Eight book representing a leaf and covers visible. And the next prompt is picture a cloud in the sky that looks like a huge heart. Can you draw that for me? And here we go. These AI tools leverage prompt engineering techniques to generate text,


04:22:39
perform language translation, code auto completion, and text to image generation, demonstrating the versatility and power of prompt based on interactions with AI models. Next, why is machine learning useful in prompt engineering? Machine learning is very helpful in prompt engineering especially in linguistic and language models because it helps create better prompts and interactions by analyzing lots of data and finding patterns. So first understanding language patterns. Machine learning algorithms can analyze large


04:23:12
amounts of text to understand linguistic patterns like grammar, syntax, semantics and context. So this understanding is critical for developing effective prompts that generate desired responses from language models. Next, generating relevant prompts. Machine learning models can suggest or generate prompts based on input data and user preferences. These prompts can be tailored to specific task, domains, or user requirements, making them more useful and efficient for guiding language models. Next, optimizing


04:23:47
prompts design. Machine learning techniques can be used to optimize prompt design by comparing the performance of various prompts and selecting the one that produce the best result. This iterative process improves prompt engineering practices and the overall performance of language models. And the next is personalizing interactions. Machine learning enables personalized interaction by creating prompts to individuals users preferences, history and context. This personalization increase user engagement


04:24:18
and satisfaction with the language model interaction. Next, improving model performance. Machine learning algorithms can be used to fine-tune language models based on prompt response pass increasing their performance and accuracy over time. Language model can be trained on a variety of data set and prompts to produce more relevant and contextually appropriate responses. And next, mitigating bias and misinformation. Machine learning techniques can help identify and mitigate preferences in prompt engineering by examining prompt


04:24:52
responses pairs for potential biases or inaccuracies. Language models can produce more fair, inclusive, and reliable results by detecting and correcting for preferences. And I hope it is clear why machine learning is useful in prompt engineering. [Music] What is prompt engineering for code generation? Prompt engineering is a process that creates a specific prompts or instructions for AI language models to generate a code snippets or scripts. It involves defining objects, using relevant keywords, providing examples


04:25:33
and being specific and concise. This process enhances the accuracy, efficiency, and relevance of code generation tasks performed by AI models. Understanding how prompts LLMS can lead to more powerful and efficient applications. Now, let's understand the principles of prompt engineering. These principles provide a basic guidelines that can create a consistently used to increase prompt engineering effectiveness when it comes to code generation tasks. First one is clarify objective and understand task or goal.


04:26:07
And second one is utilize keywords and specificity. And the third one is provide examples for context. Next one is conciseness and relevance. And the last one is encourage creativity and adaptability. Let's understand them in brief. First one is clarify objective and understand task or goal. Here understanding what you want from code output including inputs, outputs, evaluation criteria and any constraints or difficulties is essential before creating a prompt. It is easier to develop prompts that accurately guide


04:26:43
the model towards a desired outcome where there is a clarity. And the second one is utilize keywords and specificity. Including a appropriate keyword in a prompt helps to communicate the specificis of the task to the model. By avoiding inconsistency and using a clear language and instructions, you can make sure that the model produces a precise and focused code rather than requesting a function to process data. For example, be explicit about the kind of data and expected actions. And the next one is


04:27:17
provide examples for the context. The model can better understand the expected output format and functionality by referring to the examples. Prompts are made more understandable by providing a specific examples of the desired code which helps the models to understand the task. This will increase the probability of producing a code that is in align with the expectation. And next one is conciseness and relevance. The prompts needs to be brief concentrating on relevant information that is crucial to


04:27:48
the assignment and excluding unnecessary informations. Code generation is made more efficient by the model. Simplified decision making process and clear and concise prompts reduces the confusion. Removing irrelevant informations lowers noise and improves the timely efficiency. And the last one is encourage creativity and adaptability. Flexible prompt formulation allows for experimentation with the various strategies such as linguistic structures, constraints or templates, continuous improvements in prompt by


04:28:23
tracking a model outputs and iteratively improving prompts. This design creativity maximizes the code generation results by adapting to a various scenarios. Now how prompt engineering is employed in various tools for code generations. First one is GitHub copilot. Based on given prompts, GitHub copilot suggest completions, creates documentations and suggest new features to help developers write code. And second one is Google AI code decy. This helps developers to write code in a variety of programming languages by


04:29:00
using prompt engineering producing code snippets in line with a predefined prompts facilitating a variety of task from web development to a natural language processing and machine learning. And the third one is open AI codeex. Codex assist developers with a coding task in a variety of domains such as data science, web development, and game development by utilizing prompts engineering. It creates a code in a number of programming languages based on the precise instructions that users provide. Now, prompt engineering is


04:29:35
crucial for guiding AI models in generating code accurately. Let's explore practical examples across different complexity levels. First we will try for universal starter code that is hello world program in Java. So for that we have to give a prompt like generate a hello world program in Java. Hit the enter button. You will get a code of hello world program in Java. Next we will look into the basics one that is sum of two numbers. For this we have to give a prompt like generate a function in Python


04:30:25
that takes two numbers as input and returns their sum. So now you can see here the Python code is generated with a function name add numbers. So now we have seen how easy level task works. Let's move on to the medium level examples. We'll try for to turn the commands into code. First we'll create a list of countries and then generate a list of their respective capitals. After that merge the list to create a dictionary mapping each country to its capital. To get this we have to give a prompt as


04:31:12
generate Python code to create a list of countries and generate their corresponding capitals and combine them into a dictionary. Mapping countries to capitals. Hit the enter button. Here is the generated Python code for given prompt. Here you can see the country's name and also generated a corresponding capitals. So this function define generate capital which gave us dictionary mapping countries to capitals. Next one complete function or next line. If I want to complete a function to calculate the


04:32:05
area of rectangle, then we have to give a prompt as write a Python function named calculate rectangle area that takes the two parameter as length and width and returns the area of rectangle. And to include the comments, we just have to type it as include comments to explain each step of the function. So in this code you can see the function name which we have given in the prompt that is calculate rectangle area with the parameters length and width. You can also see the comments here. If you use


04:33:04
this code, you can get a area of rectangle value. Next, we will try for MySQL code generation. To get a names of employees, we have to give a prompt like generate a MySQL query to retract the names of all employees from the employee table. This also generated a MySQL query with a select statement. You can see here the last example is about how to get a explanation of generated code. To get explanation of a code, you have to give a prompt as provide a line by line explanation of the Python function


04:33:54
named calculate factorial which takes a parameter yen and returns the factorial of y. Here in this code you can see a function name so-called calculate factorial which we mentioned in our prompt. After that you can see a line by line explanation of a code. This is how you can get the explanation of a code. To become a prompt engineer, you have to follow certain steps. Before that, to accomplish this role, you need to be knowledgeable in a number of fields which includes artificial intelligence and then natural language processing.


04:34:49
After that, natural language understanding and then programming languages and finally UIUX design. And here are the steps to become a prompt engineer. First one is educational foundation and the next one is programming proficiency. After that AI and NLP knowledge and the fourth one is networking and continuous learning and last one is build a portfolio and specialize. Let's explore them in brief. First step is educational foundation. So here you have to obtain a degree in computer science, artificial


04:35:22
intelligence or NLP for a quick entry into engineering roles and gain a thorough understanding of basics of NLP and AI concepts and also you have to focus on the courses covering NLP, machine learning and also programming languages during your studies and then apply theoretical knowledges to a real world scenarios by engaging in projects and courses related to AI. AI applications and continuous learning to stay updated and handle prompt engineering challenges in the evolving of AI field. And the next step is


04:35:58
programming proficiency. For those who want to become a prompt engineers, knowing how to program is essential especially in languages like Python and also gain a hands-on experience by working on coding projects by implementing prompt engineering strategies and testing models. After that step, we have AI and NLP knowledge. Gain in-depth knowledge of AI, machine learning and NLP to become a proficient prompt engineering. And then acquire a practical experience by working on projects and apply fundamental concepts


04:36:33
and tools in a real world scenario and also utilize the acquired fundamental knowledge for efficient and effective prompt engineering practices. And the fourth step is networking and continuous learning. Connect with AI and NLP professionals to build a network and gain valuable insights. And also you should participate in conferences to stay updated on the latest developments in AI. And engage in online forums to discuss ideas, challenges and advancements with peers. And you have to stay informed about the latest AI


04:37:08
advancements to enhance prompt engineering skills continuously. And finally, you have to regularly develop your knowledge and skills to become a proficient prompt engineer. And the last step is build a portfolio and specialize. Here you have to create a projects that emphasizes your abilities forming a strong portfolio to enhance employability as a prompt engineer. And then showcase your prompt engineering skills through your portfolio by providing tangible evidence of your capabilities to potential employers. and


04:37:42
also strengthen your credentials by obtaining relevant certifications and validating your expertise in prompt engineering. Now let's understand demand of prompt engineering. The demands for prompt engineering is increasing due to the growing adoption of AI and NLP technologies in various industries. These professionals are crucial in optimizing languages models and ensuring accurate contextual relevant and ethical responses. As AI evolves and finds application in multiple sectors, the demand for skilled prompt engineers is


04:38:18
expected to grow as they contribute to the successful deployment of AI technologies. Now let's see the salary details for prompt engineering. The annual salary range for junior or entry-level prompt engineers with a 0.2 years of experience is between rupees 3 lakhs to 6 lakh. And then the annual salary range for mid-level prompt engineers with a two to five years of experience is around 6 lakh to 12 lakhs peranom. The annual salary for senior prompt engineer is 12 lakhs per anom. It completely depends upon the individual


04:38:54
skills and the salary policies of the organization. In USA, according to the ZIP recruiter report, the salary for entry-level prompt engineer is $31,000. And then the average annual salary for mid-level prompt engineer is around $59,500. And finally, for senior prompt engineer with a five or more than 5 years of experience, it can exceed up to $91,500. Let us know which companies are hiring for prompt engineers. Amazon, Facebook, Microsoft, OpenAI, and Google. [Music] Embarking on the creation of AI persona


04:39:41
chatbots opens up a thrilling and financially rewarding chapter in the realm of artificial intelligence chatbots. This innovative venture stand outs because it normalizes the field of AI, removing the barrier of technical expertise. In other words, you don't need to be a programmer or have any coding background to dive into this creative process. Platforms like Flow Wise are at the forefront of this revolution, offering intuitive tools that simplify the creation of this sophisticated chat bots. With such


04:40:10
platforms, the process becomes accessible to everyone, empowering users to bring to life digital avdars of well-known personalities, celebrities or influencers. These virtual entities can serve various roles acting as personal assistant ready to manage task or offering guidance and tailored advice to users needs. Hello everyone, Edureka welcomes you all once again to our YouTube channel and this time let's build a chatbot using prompt engineering. In this tutorial, we'll craft an AI personal chatbot inspired by


04:40:42
Steve Harvey offering a tailored advice on life motivation and personal growth. We'll start from the ground up with the flow wise guiding you through each step to build your own AI post in a chatbot. But before we begin, kindly consider subscribing to YouTube channel and hit the bell icon to stay updated on the latest tech content from EDA. Also visit the EDA website for training and certification courses. The link to which is in the description box below. So coming back to our video, firstly we


04:41:11
need to equip our systems with some tools. So let's proceed with installing flow wise. Now in order to install flow wise we need to visit the flow wise official website. So let's check that out. So I'm going to type flow wise. Okay. So here's the official website of the flow wise. I'm also going to add this link to the description. You guys can also access it from there. Then in this official website you can see the GitHub link over here. So we'll go on GitHub. Then we'll scroll down


04:41:42
and here we see the quick start option. And this quick start we have all the steps to start with the flow wise. So firstly it says to install NodeJS. So we'll do that. So over here based on your system you can just download the NodeJS. So I have already downloaded the NodeJS in my system. So I'm not going to repeat the step but you guys can follow on that. Then again over here these are the commands which we have to type on our command prompt. And over here the first command that you see on your screen is to install the


04:42:20
flow wise. Okay, I have already installed flow wise in my system but you guys have to follow the step if you are new into the flow wise. I have already installed the flow wise in my system but for the first timers you have to put this command in your command prompt. So I will go directly with the start flow wise. So this command I'm going to copy and I'm going to put it on my command prompt. You might have to wait for some time. Okay. So over here we can see the flow wise is listening at port 3000 and data


04:42:56
source is being initialized. So after getting these two messages you can directly go to port 3000. So we'll go into that and here is the flow wise user interface in front of us. Okay. So very firstly we have to start gathering the data to feed into our chatbot. Okay. So you can choose any of the mediums you can refer any blogs any of the YouTube videos or anything. For us we are going to take some Steve Harvey motivational videos. So we'll go to YouTube and over here I'm going to type as Steve


04:43:37
Harvey motivation. We'll try to look for longer videos because the larger the transcript is, the more accurate your data is going to be. So, this is pretty long. So, in order to download the transcripts, what you have to do is just copy the link of this video. Copy. I'm going to mute it. Not this one. And then we need to go to the site YouTube transcripts. Then over here we need to paste a URL. Go and over here you have the access to the complete transcript. So what we're going to do is just copy the transcript from


04:44:31
here and then we need to save it to our system. So I've already prepared a folder called transcripts and you can see two transcripts I have already written. So you can see the transcripts in any of the documents. I have put it in text documents. You can even prefer as PDFs or docs or anything. Now this was for the previous chatboards that I have prepared. So let's delete those because I don't need them. And we'll prepare a new one. So, new text document I need and let's save


04:45:07
it as Steve Harvey part one. Yes. So, over here I'm going to paste the entire transcript. Control S. It got saved. Yeah. And we're ready to go. For this particular video, I'm going to take one more video of Steve Harvey. So, let's search one more video. This already took any other. Yeah, this one looks good. Just pause it for now. Copy link again to the same site. We're going to paste the URL. Yeah. And we're going to copy the entire transcript. And again we're going to go to our


04:46:03
system new text file. Let's give it Steve Harvey part two. Open Ctrl + VR S. Yeah. And we're ready to go. So after downloading this transcript, let's go back to our flow wise. So here we have flow wise. Then I'm going to click on add new and here is the canvas where we are going to make our chatbot. So in order to start that go to add nodes go for chains and here we have conversational retrieval QA chain just drag and drop it to the canvas. So this conversation retrieval QA chain


04:46:51
is a tool that is used in creating chatbots which helps the chatbot to find and provide answer to the questions in a conversation. You can think of it like a library system inside the chatbot that helps in understanding what you asking and then search through its knowledge to give you best possible answer. This chain is a sequence of steps that the chatbot follows to make sure it understands the questions correctly and retrieves the right information to help you. And over here in the conversation


04:47:18
retrieval QA chain you can see a chat model vector store retriever and memory as the inputs and the final output is a conversation retrieval QA chain. So let's find the input of these. Okay. So our first input is chat model. So we are going to go to add nodes and we'll look for chat models over here. And for this chat model we are going to choose chat open. Again drag and drop. We going to put it here. Now we can check out the output as chat open. We are going to connect its output to the input of conversational retrieval


04:48:00
QA chain especially to the chat model section. Yes. Now let's check the second input which is vector store retriever. So we're going to go to plus nodes. Then we're going to look for the vector stores. And where is it? We need inmemory vectors too. This one. So drag and drop over here. Now this inmemory vectors too is actually a database that our chatbot is going to follow through. In this document section particularly we are going to put our transcripts. So let's connect its output to the vectors to


04:48:41
retrie input of conversational retrieval Q. So as I already told you in this document section we are going to store the transcripts that we have downloaded. So how to do that? Again add notes. Go to document luders and then select folder with files. Drag and drop. Put it over here. Output to the input. Now in the folder path section we have to put the path of our transcript. So over here Ctrl + C and again to our flow wise folder path Ctrl + V. Okay. Now we can also see one input as text splitters. So let's also take care of


04:49:28
that. So we go down here we have text splitters and we're going to take cursive character text splitter. Drag and drop over here. The chunk size is already 1,000 which is good enough. And chunk overlap let's put it to 200. Again it's output to a text splitter input of folders with files. Yeah. Now we have one more thing called embeddings. So we are also going to add embeddings. So okay over here we have embeddings and we have open AI embeddings. We're going to put it over here. Yeah.


04:50:16
So our chatbot is almost ready. So over here you can see this connect credentials part. So let's also check that we have to add a credential and for that we are going to take the API keys of the open AI. How to do that? Let's check that out. So we are going to go at open website. So I'll take this platforms dot openi accounts and ei this one. Now for the API keys. So we are going to go to platform openai. So this particular link I'm going to paste it to the description and get it from there. And here we have the


04:51:05
access to the API keys. So just click on create new secret key. You can type anything like test key or anything like that. Create secret key. And it's going to generate a secret key for you. So, I'm just going to copy that and then I'll come back to the blue wise. Yeah, I'm going to save it. Let's save that as AI Steve Harvey. Save. Okay, our chat flow got saved. Back and go to credentials. Let's set this one. Just go to add credentials. put here as open AI API. No. And here it ask for


04:51:51
credential names and open AAI API key. So just paste your API key and let's put here as open AI add. Yeah. So here we have our credentials. Now again we are going to go to our chat flow and over here in the connect credentials spot we're going to paste as open yet one in our chat model and another one we have in our embedded section. Again we have open. So our chat flow is fully ready. So again we're going to save it and then again go to the section of chat. So it already shows a message. Hi there,


04:52:35
how can I help you? Firstly, I'm going to say hello. Now it's going to take some time and it's going to show error. Okay, it showed how hello can I assist you? I'll tell him to introduce yourself. Okay, I'm an AI assistant. I'm here to provide you with information assistant. So our chatbot is kind of working, but we want our chatbot to have a personality of Steve Harvey. How to do that? Let's check that out. Now, in order to give that personality of Steve Harvey, let's go to the additional


04:53:09
parameters of the retrieval QA chain. And over here, you have a space. Now, put a prompt similar to like this. So, this is a prompt like your Steve Harvey and American television host. All this I've gathered from different sources like Wikipedia or like Steve Harvey personality blogs or anything like that and get it from there. So, I'm going to select it all. I'm going to copy and I'm going to paste it over here. You can also pause the screen and read the full prompt. But what I want you to read is


04:53:40
is the last line because it's important. So, the last line it's like if you have no relevant information within the context, just say, "Hm, I'm not sure." Or don't try to make up an answer. Never break the character. Again, the same thing. Answer the user question as you are Steve Harvey and only answer questions found in the context. If you don't have any answer, just respond with something like I have no idea or could you please rephrase that or h I'm not sure. So yeah, so this prompt will help


04:54:10
a model to stick to the relevant information and answer it as Steve Harvey. So again we are put Okay, so firstly we need to save it. Yeah, we saved it. chat again we're going to put something like hello okay hi there and again I will say introduce yourself okay so here you can see now I'm Steve Harvey and television host producer actor comedian I'm known for my hosting shows like family feud and all like whatever we have given in the prompt so now I'll tell him to you know just tell


04:54:51
me joke. Okay, sure thing. Here's a joke for you. Why did the scarecrow win an award? Because he was outstanding in this field. Okay, let's tell him. I need a motivation. What is I need a motivation? I need some motivation. I'll tell as I am feel very lazy. So here we have our answer. Don't worry, I've got you covered. Remember, success requires a tremendous work ethic and faith. Imagination is the key to achieve your dream. So dream big, believe in yourself and take action to make those


04:55:31
dreams a reality. You have the potential within you to achieve great things. Keep pushing forward. Okay. So it is working properly. So our chatbot actually got a personality of Steve Harvey. [Music] Imagine how it would be for you if you got a virtual site pair alongside you while you're coding. The site pair is helping you to automatically generate code and improve your code by making it faster and more efficient. You can chat with him. You can ask questions, ask suggestions and much much more. Sounds


04:56:06
good, right? Well, GitHub copilot is that AI site pair of yours that makes it all possible. It is an AI coding assistant that helps you to write code faster and with less efforts, allowing you to focus more energy on problem solving and collaboration. GitHub Copilot is proven to provide 55% faster coding and also proven to increase developers productivity and accelerate the pace of software development. If we take a look at the industry standards, then we can see that around 50,000 plus businesses have adopted GitHub copilot.


04:56:41
One in three Fortune 500 companies use GitHub Copilot and 55% of developers preference GitHub Copilot. Now, if you want to learn more about this and you want to know how to use this amazing AI tool, then you just clicked on the right video. In this GitHub Copilot tutorial, you will learn how to use GitHub Copilot and how it can be your trusty navigator guiding you on your flight path to a coding career that really takes off and goes up high. So now let's open up our code editors and get started with the


04:57:12
GitHub copilot. Now very firstly let's check how you can install the GitHub copilot into your system. So very firstly we have opened our visual studio code and in order to install the GitHub copilot go to the extensions and as an extension type GitHub copilot you can see over here. Now I have already installed GitHub copilot in my system. But over here you can just click on the install button and it will start installing in your system. After that once you have installed GitHub copilot you have to create an account in the


04:57:43
GitHub. So over here you can see the GitHub link. Click on that and will redirect it to a page of GitHub. Now I've already installed GitHub. Now one more thing comes is that this GitHub copilot needs a paid subscription. So if you want to check the paid subscription for the GitHub copilot, we can visit this particular website. I'll be putting the website in the description. You can check it out from there. Now once you come to this website which is the GitHub copilot official website, you can scroll


04:58:12
down and over here you can check the subscription plan for GitHub copilot. For the $10 you can start a free trial over here. Then according to usage you can check whichever plan you want. So once we have installed gup copilot and we have subscribed to the gup copilot it's time for us to use the getup copilot for our coding. So once we are done with the installation part of github copilot you can check this icon over here of github copilot. This icon basically portray that your github copilot is active and it's ready to run.


04:58:46
And in case if you don't see this icon just restart your visual studio code. And once you restart you're going to see this icon over here. Now I have already created a folder of GitHub copilot in which we are going to store our files in which we are going to check the capabilities of GitHub copilot. So before we jump into the coding part, let me show you something interesting. So for example, let me create a Python file. So I'll put edurea python. Now if I put a comment over here, comment and if I put a question


04:59:17
like what is inheritance in Python? So as you can see that it is already predicting my question. I'll put enter and it's going to generate an answer for me. You can see the answer over here. If I want to accept that answer, I have to just click on tab button. And for the convenience, I'll put alt + z. And over here you can check the answer for this particular question. Inheritance is a mechanism in which one class requires a property of another class and has also given us an example.


04:59:49
Now if I check another question. So see over here it's also predicting my next question. It's already asking example of inheritance in Python. So if I accept that I would enter and it gave me the example of inheritance. It also gave me a piece of code. So for now I won't get into the coding part. So this is how you can check that copilot is also good in predicting our next questions and predicting our next moves. So this is how you can also put a Q&A with getup copilot. So now let's move to the coding


05:00:22
part. Let me make a index.html file. I'm going to put the tags over here. Then I'm going to put the title as let's say edure and in the body let's say if I make a container. Now over here you can also see the h1 and par that GitHub copilot is suggesting us for the title edure. So the text which is appearing over here is called as a ghost text. Now if I want to accept this ghost text I have to click on the tab and if you just want to reject just click on escape button. So for this it has already given an h1 and


05:01:03
a par for this. I'm going to put alt + z to put it in a better format. Now let's add a stylesheet into that. So if I put style dot CSS and in style dot CSS I want H1 tag as let's say blue in color. Now I have to link the stylesheet into our HTML file. So for that it's already giving a code for us. So if I click on tab button it's going to link the stylesheet. And then if we run this whole file you can see it has applied the stylesheet. It has given the heading and blue color welcome to edera and also


05:01:46
given us the text. So here you can see how getup copilot is helping us with HTML and CSS. Now let's check some other capabilities and features of GitHub copilot. So after playing with the HTML and CSS using GitHub copilot let's check some more interesting features in GitHub copilot. So again we'll go to the py file which we created in the initial stages of our tutorial and you can see this option oftrl + i. Now if you press control + i it's going to open an option of inline chat feature in github copile.


05:02:20
So let's say we give it a prompt of generate a code for calculating the number of days between two dates. Let's keep it simple. Once you press enter, it will start generating the code snippet for you. Now if you are satisfied with the code snippet, just click on exit. I can see the code snippet is appearing in your code editor. Now let's say there is some error in this particular code. So let's say if I remove this end date from here and I don't know what is the error in this particular piece of code. How to


05:03:00
find it? Now in order to find this G copilot helps you with this particular feature of fix this. Now if I select the whole code right click I'll go to copilot and if I click on the fix this option it's going to fix the whole code by giving you the suggestion and giving you the solution for this particular problem that you are facing. So it already says that your problem is with the function is expecting one argument but two are provided. So if I accept this you can see again it puted the last


05:03:34
argument over here which is of end date. Now get a copilot sometimes gives the accurate code snippet which you want and sometime it might differ from what you actually want. Now in order to do that just select the whole code and putt controlless enter. Now once you press control plus enter you can see GitHub copilot coming up with a lot of suggestions for this particular piece of code. Now if you're satisfied with any of the suggestion you can just accept the suggestions and it's going to


05:04:02
implement the particular suggestion for your code. Now let's check one more feature. Now you can even ask a copilot to explain the whole code separately. Now in order to do that again go to the c-ilot and go to this option of explain this. Now once you click on the explain this option. Now you can see in the chat box it comes up with the whole explanation of this particular code. Now this chat box is very much similar to the other AI chat boards that you use like chat GPT or Google Germany or


05:04:30
Google board. Now you can even ask for more questions to this chat box. So let's say if I ask him write a code for bubble algorithm. Let's see. So here you can see that is coming up with a code snippet of bubbles algorithm. Now this is very much similar to what we do in chat GPT or other AI boards like Google Germany and all. But the main feature is that this chatbot is appearing in your code editor and you can just directly copy paste this particular piece of code. You don't need to go to the separate browser then you


05:05:06
have to copy paste the whole code from there. So in short it makes your job more easy. Now to make it visually more appealing you can even drag this chat box anywhere in the code editor. For example if I put it over here you can see the chat box appearing over here. or you can make it appear anywhere in the code editor according to your convenience. Now after checking these basic features, let's check how these features of GitHub copilot actually help us while we build something. So for now,


05:05:35
let's try to make a simple chatbot using Python with open AI API and let's see how quickly we can complete building this chatbot. So very firstly we're going to start by importing OpenAI library. Now to install this open AAI, you have to go to your terminal and type pip install openai. So after that let's put our API key. And now you can see that GitHub copilot is already ready with its suggestions in the form of a ghost text. So we're going to type tab and let me change my API key. Now in case if you


05:06:12
don't know how to generate the open AI keys then you can just simply visit the openi website and over here in the platforms you can generate your API key. I'll be putting this link in the description. You can go to this website and make your API keys. So I'm going to copy my API key and then I'm going to paste it over here. Then let's make a function of def chat with GPT. And you can see once we write the function it's already ready with the code and in this case we exactly want the same code


05:06:49
response equal openti completion create engine 3.5 instruct exactly prompt is from max tokens is 150 decent enough now what is this code about what is this function about let's understand now this function will generate text based on the input prompt using openi's language model. The parameters for the completion model include the engine, prompt, and the max token. This engine basically specifies which version of the language model to use. In this case, it's using the GPT 3.5 Turbo Instruct engine. And


05:07:21
then it comes to the prompt. Now, this prompt is a text provided to the model to generate a response. It's what the user wants to chat about or to get information on. Then we have the max token. Now, this parameter limits the number of tokens in the generated response. Here it is set to 150. It means that response won't be too long. Now once the response is received from the open API, the function returns the generated text. The dot choice.extrip part extracts the generated text from the response object and removes any


05:07:53
extra white spaces from beginning and the end. Now let's move forward and create the main function. So we going to put if name do main. And you can see again it came with the host text which is exactly I want. Now what is this main function? Let me explain you. Now over here the code first check if it's being run directly. Then it enters a loop where it continuously prompt the user for input. If the user inputs an exit command the loop breaks otherwise it prints the user input and the bot


05:08:24
responses. So now let's go and run our chatbot. We're going to run it in the terminal. So over here I'm going to ask who are you? So over here you can see the generated response. It's like I'm opening HGB3 language model program to assist you with the variety of this. Okay, great. So you can see that our chatbot is working. Let's say one more question of like what is the difference between a class and a object. Let's see again it came with a answer. A class is a template or blueprint that defines the


05:09:09
characteristics and behaviors of a type object where an object is an instance of class. So it typically answers the question which you want. Now we already witnessed that how easily we build this chatbot with just a few clicks by accepting the recommendations from the GitHub code. Now after the chatbot let's also try to make a linear regression model for us. So we'll start off by importing uh required libraries. So firstly I'm going to start by import mattplot lip.pipplot as plt. Then I'm going to import numpy.


05:09:42
It is already showing me over here. I'm going to click on tab. Then I don't need torch but from scikitlearn I need to import the data sets and the linear model and then from scikitlearn I need mean square error. Yes I do need mean squed error. So I'm going to accept that and then I will start by loading the diabetes data sets which it's already showing me over here. So I'm going to tab and then it's going to give me a code. So again I'm going to click on tab. So again I'm going to click on tab. Now


05:10:14
what I want is yeah this is exactly what I want. I just wanted to use only one feature of it. So again I'm going to click on tab enter. It's going to give me a code for that. It's already did it slicing and then I'm going to print it. So print diabetes X and let's check the data set. This is the exact data set which we wanted or this one feature we are taking to train our model. So I'm going to delete that. I'm going to move forward by building our model. So after that we have to split the data


05:10:50
into training and test. It already knows what we are planning to do. Put enter. Yes. So over here it is taking the first 80 elements for the training data set. So I was like so I'm okay with that. And then for testing it's taking the last 20. I'm even okay with that. Then for target I need training and testing. Yes. Again I'm going to split the target into training and testing. So diabetes by train diabetes test. Exactly what I want. And then I'm finally going to create the linear


05:11:29
regression object or the model. So as you can see I'm just writing the comments and it is already showing me the code for that. So for this particular model I don't even have to code a single line or rarely maybe a line or two I have to code but everything get a copilot is suggesting me. So I'm going to click on tab then I have to train my model exactly what I wanted to do enter. So it's going to fit diabetes extreme diabetes by train enter and after that I'm going to make some


05:12:02
predictions again exactly enter. So, diabetes by predict good. I'm going to predict the X testing set. So, this is exactly what I want. And then I also want the coefficients. It's giving me the code to print the coefficients. And then again the mean square error I want. It's giving the code for mean squed error. Tab enter. Tab enter. And then I'm going to give the plot outputs. For that we need the plt.scatter exactly. And then plt.plot plot for that one straight line and then I want plot to show plot show. So a


05:12:41
linear regression model is ready and as you can see how much coding I did it by myself or manually get up copilot made the job as easy as it can be. So I'm going to run the python file in the terminal. So you saw how conveniently and quickly we made this projects using github copilot. However, these were just the basic projects we built in order to give you an idea about GitHub copilot. [Music] Now, let us understand what lang is and why it is a valuable tool for building AI applications. You must be aware of


05:13:21
popular applications such as GPT and Gemini. These applications utilize APIs and GPD uses OpenAI's API while Gemini operates through the Gemini API to process prompts. They leverage models like GPD 3.5, GPD4, Palm and Gemini 1. Additionally, these are other advanced models such as Llama, Gemini, Cohair, Cloud version 1, Falcon, Palm, GPT4, and GPT 3.5. Langchain is a framework designed to help developers build flexible and powerful AIdriven applications by integrating and utilizing these diverse models


05:14:00
effectively. But why exactly do we need lang? You must be thinking if langchain is this important then why do we need lang? So let's break down this question using some real world examples. So imagine simply asking an LLM a prompt and getting an answer. That's easy. But what happens when the complexity increases? For example, let's say you're working with data from SQL databases, CSV files, PDFs or Google Analytics, and you need the model to write code, perform searches, or send emails.


05:14:34
Handling such intricate workflows manually can get overwhelming. This is where Lankton steps in. It simplifies the process by offering components like document loaders, text splitters, vector databases, prompt templates, and tools. So this helps you assemble tasks such as document summarization, question and answer systems or even advanced workflows like Google searches or customer support automation. Let's visualize this process with a diagram. Here's how it works. So first you load a document like a CSV file using a


05:15:07
document loader. Then use a text splitter to divide it into a smaller chunks and then store those chunks into a vector database and add a prompt template to guide the model. And finally, use a LLM like a GP4 or LMA to perform tasks like searching the web or automating workflows. And lang chain also offers chains that will help you assemble components to achieve single task such as summarization and an agenda to figure out what each component must do like password, customer services, etc. Now that we understand Langchain's


05:15:40
core components, now let's explore how it streamlines the LLM application life cycle. So it typically involves three key stages. First is the development where you build and test your application. Then productionization where the system is fine-tuned for a real world use. And finally deployment where the final product is launched for users. So Langon simplifies this life cycle allowing you to focus on building without worrying about the underlying complexity. Now let's take a step back and understand


05:16:11
the role of APIs in powering these LLM applications and how Lankton effectively integrates them. In all these applications and models, one thing is common that is they use API. So now let's discuss APIs. APIs act as an intermediaries that enable different systems to communicate with each other. For example, they allow apps like Swiggy or Blinket to display your delivery driver's location in real time. So now let's look at the steps to explain APIs and API keys. So apps like Zipto, Swiggy


05:16:44
and Blinket use APIs to show the location of your delivery driver. So these apps don't communicate directly with Google Maps but follow a layer process involving servers and security mechanism. First the app sends a request to Google Maps API. Then the API forward the request to Google servers. Then the servers validate the request with the system. So once approved the response follows back through the servers, APIs and finally to the app. So previously apps like Swiggy allowed login using phone numbers. Now they use API for


05:17:19
login via platforms like Google or Facebook. So this demonstrates the versatility of a APIs in enabling seamless user interactions. To prevent misuse, APIs require API keys which are unique identifiers for secure access. So these keys authenticate request and ensure that only authorized users can interact with the APIs. Next, security systems closely monitor API usage to detect and prevent misuse. This ensures that APIs remain safe and functional for their intended purpose. And these steps simplify the explanation of how APIs and


05:17:55
API keys work in real world applications. So this is how Lankchain leverages APIs to connect your LLM applications with external tools making them versatile and secure. Now that we understand the role of APIs, so let's explore some real world applications of lang chain. So what can you build with lang chain? Here are few applications. First application we have is customer support. So customer support for your shopping websites to interact with customers. Next, conversational chat bots for helping you study, content


05:18:27
generation tools for blogs or social media. We also have question answering systems for knowledge bases and then document summarizers for legal or academic content. Langin simplifies AI development by integrating LLM with various data sources and tools. Its applications are vast from chat bots to document summarization. So, let's examine a practical example to see Lchin in action. All right. In today's datadriven world, understanding and effectively using SQL queries is crucial for managing and


05:19:00
analyzing large data sets. However, beginners and even experienced users often need help with complex SQL queries, their syntax, and how they work. This creates a barrier to efficiently interacting with databases and limits their potential to solve real world problems. To address this challenge, we propose a SQL query fetcher application that leverages the Gemini AI, Python, and Streamlit to simplify SQL learning and usage. The application allows users to input or select a query, generates the SQL


05:19:33
syntax, and provides a detailed explanation of its components and functionality. This tool bridges the gap between technical understanding and real world database operations, empowering users with an initiative and interactive SQL learning experience. Let's jump right into the code. So the first step is setting up your dependencies. Here we import streamlit for the user interface and then Google generative AI for using Gemini. So first import streamllet as SD and next import Google dot generative AI as gen AI.


05:20:16
So to get this API you have to go to the Google Gemini API key and here click on get a Gemini API key in Google AI studio and then once you scroll there is a button on the left called create API. Now click on it and select your model here and let's copy it. And now let's go back to our VS code editor and paste it here. So to paste let's type Google API key and inside the double quote let's paste it. And now let's type genai dot configure and inside the bracket let's keep it as


05:21:01
api key equal to and give it as google_i key. Now let's type model equal to genai dot generative model and inside the bracket let's keep it as gemini pro. So we use the Google Gemini API to generate SQL queries dynamically. So make sure to configure your API keys securely. Now let's display the streaml layout code. Now let's set up the app's user interface. So we use streaml to create an interactive page where users can input plain English queries and get SQL code in return. So we write st dot set


05:21:50
page_config and inside the bracket let's give it as page title and inside the double quotes let's give a title as edureka sql query generator and give a comma and let's type it as page icon equal equal to and inside the double quote let's keep it as robot. Now let's put some images. So I'm using edurea image and SQL logo and also to make them center we will type it as column 1 comma column 2 comma column 3 equal to st dot columns and inside the bracket let's keep it as


05:22:38
1a 2a 1. Next, let us type with column 2 colon and let's type it as st dot image and inside the bracket let's give the image address and then width is equal to 200. Now let us add another image. So let's copy the same and give the other image address. Our layout includes a title, logo, and text input box to keep the interface simple and initiative. So here's where the magic happens. So when a user clicks the generate SQL query button, we format their input into a prompt for the Gemini


05:23:19
model to generate SQL code. So let's create template by writing template equal to and inside the triple quotes let's type it as create uh SQL query snippet using the below text. Next let us also give text input and we'll also type I just want a SQL query. Now let's type the response. So type response equal to model dot generate generate content and let's keep it as template dot format and inside the bracket let's give text_input equal to text_input and next let's type the SQL query. So


05:24:14
give SQL query equal to response dot text dot. So let's give the strip function dot lstrip function dot r strip function. So the AI generates the SQL query and we clean up the output for display. So once the SQL query is ready, we take it a step further by generating a sample expected output and a clear explanation of the query. Now let's type the logic for showing explanation and output. So let's type st dot markdown and inside the bracket we will give HTML tags. So first div style is equal to we


05:25:02
will align the text and center. So text align center and next let's give H1 tag and inside H1 tag we will write SQL query generator and let's close the H1 tag. Next, let's open H3 tag and write I can generate SQL queries for you. And let's close the H3 tag. And inside the H4 tag, let's type it as with explanation as well. Now close the H4 tag and let's open the paragraph tag which is the P tag. And inside the P tag, let's type it as this tool allows you to generate SQL queries based on your data. Now let


05:25:55
us close the P tag. Also close the div tag. Now to make the markdown visible, let us type unsafe allow HTML equal to true. Now let's write text input equal to st.ext area and inside the bracket let's give it as enter your query here in plain English. Now let us give a submit button. So for that let us type submit button equal to st dot button and inside the bracket let us give generate SQL query. Now if submit button colon write it with st.spinner and inside the bracket let's keep generating SQL query


05:26:50
and then let's create a template and inside the trible quotes let's type it as create a SQL query snippet using the below text. Now using the about template we will write three more templates for SQL query which are text input and then SQL query which will include expected output and also explanation output. Now to merge all the templates together we will make a container. So we will write with st.container. So it's a function and let us also write it as st dot success and inside the bracket let us give it as


05:27:36
SQL query generated successfully also we will give here is your query below next st.ode code and inside the bracket let us give SQL query and comma language equal to SQL. Now let us give once again st dots success and inside this let us keep it as expected output of this query will be and now let us keep it as st dot markdown and inside the bracket a output once again st dots success and inside this let us keep it as explanation of this SQL query. Next let us give st.mmarkdown and inside this function let us keep it


05:28:35
as explanation. So over here this shows a green success message indicating the SQL query was generated successfully. And next the show SQL query. This displays the SQL query is a formatted code block highlighting it as a SQL. Next is the display expected output. So this provides a success message for the query's expected output followed by st markdown e output which displays the expected output in markdown format and followed by st.m domarkdown e output which displays the expected output in


05:29:09
the markdown format. So now this line of code introduce an explanation and st.m domarkdown explanation displays that it is in markdown format for clarity. So this makes the tool valuable for both learning and debugging SQL. So now let's see it in action. So open the terminal and let us type streaml run and give your file name. Now as you can see the screen your SQL query generator is ready to go. Now let's test it. So for that here I will input a prompt asking for a query which is give me the query for create table.


05:29:52
Now let's click on generate SQL query and as you can see it's running. So let's wait for it to generate. So as you can see on the screen the app generates a SQL query expected output and even a plain English explanation in seconds. So how cool is that right? And that's it. Our SQL query generator powered by lack chain, Gemini API and Streamlate is complete. So this project is perfect for simplifying SQL learning and enhancing productivity. [Music] Now let's begin by understanding what


05:30:30
RAG is and how it works. Rack is a hybrid approach in artificial intelligence that combines retrieval systems with generative models to produce highly accurate contextually relevant responses. It brings the gap between fractual accuracy and natural language generation. Now let's understand it with the help of diagram. So it's a hybrid approach involving artificial intelligence that combines a retrieval system with a generative system to produce highly accurate responses. Now that we know what rag is,


05:31:01
so let's explore why it is crucial for large language models and see a real world example. So rag addresses several limitations of traditional LLMs. It mitigates illusions by grounding responses in fractual retrieve data by dynamically accessing up-to-date information. RA stays relevant in rapidly challenging domains. It improves accuracy and relevance by fetching specific relevant documents during inference by outsourcing factual knowledge retrieval. RAG enables smaller more efficient models and it can adapt


05:31:34
to domain specific knowledge basis for specialized applications. Additionally, rack provides explanability by showing the retrieved documents or data sources increasing trust and transparency. Now let us see some of the use cases. So without rack the sentence would be when was the last Mars rover launched. So this is just the incorrect response. So with rag the sentence would be dynamically retrieved from NASA's database and it would be the perseverance rover was launched on July 30, 2020. Now that we have seen why RAG


05:32:08
is important. So let's dive into how it works. Well, RAG operates in three-step process. A user submits a query which triggers the retrieval stage. Here a retriever searches a database or knowledge base using tools like BM25 to fetch the most relevant information. The retrieve data is then fed into a generative model like GPT or T5 which process it and generates a coherent contextually grounded natural language response. Now let's take an example here. The query is who wrote 1984? Retrieve would be fetching a document


05:32:46
containing George Orwell wrote 1984. Now generative response would be the author of 1984 is George Orwell. This hybrid approach makes rack ideal for real world applications like chat bots and knowledge systems. Now that we understand how rag works, let's explore some of its real world applications. Rack's versatile applications span various domains. In knowledge management, it can summarize large databases or documentation, aid in corporate teams. Legal and compliance task benefits from RA's ability to


05:33:19
answer queries based on case law and regulations. While in healthcare, it can support medical professionals by summarizing research papers and guidelines. Education and e-learning can leverage rack for virtual tutotoring providing detailed explanations based on the textbooks and research papers. Interactive virtual assistants like Alexa and Siri can utilize rack to generate accurate and informative responses to user queries such as news headlines or product recommendations. Rack's unique ability to combine


05:33:51
retrieval and generation makes it essential for task demanding both factual accuracy and fluent natural language responses. Now let's compare retrieval augmented generation with traditional AI model across three features. First we have fractual accuracy. Rack provides highly accurate responses by using realtime data whereas traditional models may give less accurate answers and may give errors. Next is the context adaptability. So here DAG adapts quickly to new queries using live data whereas traditional


05:34:23
models offer fixed answers based only on pre-trained knowledge. Next we have knowledge updates. Rank is easy to update. Just change its data source. Whereas traditional models need retraining which takes time. Then we have scalability. Whereas traditional models are limited by day size and training data. And then we have use cases. Rag is great for task like legal advice or customer support. Whereas traditional models work well for creative rating or casual queries. So here I want to conclude that rag is


05:34:55
ideal for knowledge based task needing accuracy and flexibility while traditional models are better for creative users. While rag offers significant advantages, it's essential to acknowledge its limitations. So let's discuss the challenges and future of rack. So the first challenge is the latency. Rack systems can suffer from latency issues especially when dealing with large data sets or complex queries. Next is the data quality dependency. The quality of the generated responses heavily depends on the quality of the


05:35:25
underlying data. The next challenge is complex integration. Integrating RA systems with existing applications and infrastructure can be challenging due to the need for data synchronization, query optimization and model management. And finally, scalability issues. As RAT systems becomes more complex and are deployed at scale, they can face scalability issues. This includes handling increased query loads, maintaining data freshness, and ensuring model performance. Now, while rack faces limitations, its potential is


05:35:56
undeniable. So now let's discuss Rag's future. The future of Rag holds immense potential. It will power dynamic real-time applications like new summarization, financial analytics, and live sports commentary. Rag will be customized for specific domains like healthcare, law, and science through integrations with specialized knowledge bases. Advances in retrieval models and compression techniques will reduce latency to enhance efficiency. Rag will expand to handle multimodel data enabling use cases like multimedia


05:36:29
question answering. Additionally, RAG will facilitate personalized AI assistant and improve transparency and explanability by attributing sources and providing clear explanations. Now let us move on to generative AI project using rack. So imagine you're working with a massive library of documents. You need a way to quickly search and answer question based on the content. So manually flipping through pages takes time and effort. Wouldn't it be great to have a system that retrieves relevant information and answers your


05:37:00
questions directly within those documents? So that's where our Streamlit app comes in. This app utilizes the power of natural language processing and advanced retrieval techniques to turn your complex document collections into a powerful question and answer system. So let's take a look at the code behind this app. This app will allow users to ask questions about a collection of PDFs and get answers directly from the documents using the power of natural language processing. Now, first let's


05:37:29
create a virtual environment. Now, in the terminal, let's type the command for setting up the environment in your editor. For that, let's type create p. Let's type v E Nv Python and its version give equal equal to 3.10 - Y. Okay, let's enter. In this command, the hyphen PV ben specifies the path and the environment name while hyphen Y skips the prompts for a smoother install. Now, while that's setting up, let's create a few essential files. So let's activate your new environment with the command cond


05:38:13
activate v e n d and forward slash. So as you can see our environment is ready. Now let's import libraries. So let's start by importing the libraries we will need. In the first line we will import stream as st. This gives us access to all the functionalities of the streamly for building our web app interface. So next we will import OS for various operating systems functionalities. After that now we will import libraries from lang which is a framework for building NLP pipelines. So we will use


05:38:51
these for task like text splitting for document chain creation, prompting, retrieval and more. So we will explain each library in detail as we use them. So let's type from langchen_group import chat group that is gr and next we will type from langin.extplitter import recursive character text splitter. So let's type recursive character text splitter. Again let us type from langchain.tchains dot combine documents import create staff documents chain again from lang chain code.prompts prompts. Let's


05:39:41
import create retrieval chain. Next, import f import fis from langchain community dot vector stores. So this will help us to create a vector index for efficient document retrieval. So let us type from lchain community.vector Vector stores import f ais similar imports will follow for other functionalities like document loading and generating embedding but we will introduce them as they appear in the code. But before this go to the group cloud website and on your left you have API key option. So select and create


05:40:29
your API key and copy this. And if you want to check your model then go to the playground and at the top right corner click on the llama model and check there are so many of them latest also. So choose your model and generate your free API. Now go to the terminal and paste it inv file using variable group API key. Now again go to the Gemini AI studio. On your right you have the create API option. So select your model and create your API key. Now now copy the key and paste it into your environment variable that is the env


05:41:18
file using a variable Google API key and paste it here. Now we will load environment variables from a env file that will securely store our API keys. So for that use env to achieve this. Let's type from env import load env. Also let's type load env function. Next we use OS to retrieve the gro API key and Google API key from the environment variables get env. So let us type gro API key equal to os dot get env. And inside the function, let's type it as G OQ API key. And inside the single


05:42:15
code, let us type gr OQ API key. And in the next line, let us type OS dot envir and and inside the bracket under double quotes, let us type Google API key and equal to OS dot get env let us type Google API key. So here these keys are the required to use specific NLP services. Now let us write code for displaying app title and images. So for that load your image. Since I'm using Edureka image name Edureka.png along with the app title edureka document question and answer we will use ST. image and stitle for this


05:43:09
purpose. So for that let us type st dot image. So inside the double quotes let us keep the image name and comma width is equal to 200 and let us also keep the title. So for that stitle and let us type edure document question and answers. Now the next step is to initialize chat group and prompt template. Now it's time to interact with the langchain group API. So initialize the chat group object using group API key and specify the llama 38B 8192 which is the language model we will be using for our NLP task. So for that let us


05:43:47
type lm equal to chat group and inside the bracket give the group API key equal to group API key. We will give the model name as well. So for that type model name equal to and inside the double quotes give the model name. So here we are using llama 3 - 8b - 819. All right. Now let us define a prompt template using chat prompt template. So this template ensures that AI responses are based on the context provided and user questions. So keeping answers accurate and concise. So for that we will type prompt equal to chat prompt


05:44:34
template dot from template and inside the bracket let us paste the prompt. So here we have the prompt which says please answer the question strictly based on the provided context. Also ensure the response is accurate, concise and directly addresses the question. Now let's create function for embedding vectors. For that let's define vector embedding function. So type def vector embedding function and give colon. Next in the next line give if and under the double quotes give vectors not in


05:45:10
stession state colon then type st dot session state dot embeddings equal to Google generative AI embeddings give equal to Google generative AI embeddings AI caps and inside In the bracket give the model equal to and inside the double quotes let us type models forward slash embedding -001. Now make a folder where you will load your PDF. So I am creating ed PDF and paste your PDF here. Now set the session. So that is let us type stession state.loader loader equal to py pdf directory loader. Here inside the double


05:46:08
quotes let us paste the path of the pdf. Next is the data ingestion. For that let us type st dot session_state dod docs equal to std dot session state dot loader dot load function. So this particular line of code is for data injection and here this particular line is for document loading. Next let us type st. session state dot text spplitter and give equal to recursive character text splitter and inside the bracket let us give chunk size and mention the size here I'll give equal to,000 and comma chunk_lap


05:47:02
is equal to 200. Now here these are for the chunk creation. Now let us type ST dot session state dotfal documents equal to st dot session state dot textsplitter dotsplit documents. Now inside a bracket let us give stession state dot docs. Let us give bracket col 20. So this line of code is for splitting. Now let us type st dot session state dot vectors equal to fis dot from documents and inside the bracket let us again type st dot session state dotfal documents comma type st. dot session state dot embeddings.


05:48:05
Okay. So this line of code is for vector openi invarings. Now to input field for question let us type prompt one. So give prompt one equal to st.ext_imp import and let us type here enter your questions from any document. Now to create a button to load embeddings let us type if st.button button and inside the function let us give under the double quotes load edure DB give colon and in the next line let us type vector embedding function next type stuccess and the message would be edure db is ready for queries


05:48:50
if the question is asked then if prompt one is true then type document chain equal to create underscore star document chain and inside the bracket let us give lm comma prompt and in the next line to retrieve let us type retriever equal to st dot session state dot vectors retriever function and in the next line let us type retrie equal to createore retrieval_chain and inside the bracket let us give retrieval comma document chain. Now to measure response time let us type start equal to time dot process


05:49:43
time function. So for response type response equal to retrieval_chain dot invoke and inside the bracket give input prompt one and in the next line let us type response time equal to time dotprocess time function start. Next let us write code to display the response. So for that let us type st dot markdown and inside the bracket let us keep it as AI response. Now in the next line let us type st dots success and inside the bracket give response and give answer inside the single quotes and in the next line write st dot write.


05:50:35
Let us type f inside the bracket and and inside the flower bracket let us type as response time colon 2f and seconds. Now moving on let us write the code to display similar documents in an expander. So for that let us type with st.expander expander. Inside the bracket under double quotes, type document similarity search results and give colon and in the next line let us type st dot markdown and inside the bracket let us type it as below are the most relevant document chunks. So type below are the most relevant document


05:51:17
chunks. Give colon inside the bracket. Close the double quotes and come to next line. Here let us type for i, comm, dot in en in enumerate and inside the function give response dot get context. Now in the next line let us type st dom markdown and inside the bracket keep f and let us give the html tag which is div class is equal to card and open the p t tag and inside the p tag let us keep doc dot page content and now close the p tag. Now let us close the div tag as well. Now let us come outside the triple code


05:52:08
and give comma and type unsafe_all allow html equal to true. So you can also add inline styles and HTML tags and also icons and emojis to make your application fabulous for the user. Now it's time for testing. For that open your terminal and write streamlit run and give your file name. So once you enter and there we go. Here's our document question and answer loader. Now select the question from the PDF you have loaded in the file and ask your loader. So as you can see this is my PDF. So I'm


05:52:49
going to copy some question from here. So let me just copy this. Okay copied. So I'm going to paste it here. So I'm going to click on the load edure DB. So guys as you can see it provides an answer in context given in the PDF. So this is our answer that it has generated. So that's all we have used simple Python code and languin techniques of rack and some inline HTML and size. [Music] So over here what we're going to do is we're going to take download the mnest data set from keras.datas data sets and


05:53:27
we'll just take the X data set that is the features that are present only the extra part and what we're going to do is we are going to create a generator and a discriminator model and then we're finally going to combine them in order that they can work together and as GANs are being a pretty intense topic I would really recommend you people to check out or have a good understanding of you know the real basics of what deep learning is and how machine learning works although you won't be using any concepts from the


05:53:51
machine learning so if you want the playlist for our deep learning or the full course link The description for that will also be provided in the description box below. So talking about the problem statement as I mentioned earlier first off we going to load our data set and source that I'm going to use over here will be kas dod data set. Apart from that we are supposed to create two types of model here. One would be the generator model. Another one is going to be the discriminator model. Generator model it is there for


05:54:15
performing sampling and the discriminator model is basically there for downsampling. And then we are going to combine this up. It's very important for you to know how GANs work. And the reason for this is because we have to write our own loss function over here. So moving ahead, let me quickly just show you what are the different tools and frameworks that we are going to use. As GANs are really intense when it comes to training them. We're going to need a good performing GPUs. So what I'm going


05:54:39
to do is we all know Google Collab gives me a good performing GPU for free. So let me quickly use that up and we're going to use Google Collab. Apart from that talking about the different frameworks and libraries that we're going to have the generic ones would remain the same we are going to use numpy mattplot lab and pandas. Apart from that deep learning framework that is tensorflow and kas that's a framework that we are going to use here to train a model and also to develop our model. All


05:55:03
right guys so now that we know brief of what we going to do now let me quickly jump to my code editor that is Google collab and show you how we can get started with this project. So this is the interface of a Google collab. First thing that I'm going to do over here is come down to the runtime and change the runtime type to GPU because this is because training GANs requires huge amount of compute power. So once we're done with that, let's connect this to the server. So it will take some time.


05:55:27
So once this is connected, let me zoom in for you guys. First off, what we'll do is we'll load our libraries. So load libraries. So let's do one thing. Let's uh import from kas dot data sets. import mnest and let me execute this. It's okay. We are getting this error. Let me quickly I think I might have done some typo here. So first off kas dot data sets import mnest. So this is where I'll be having my data sets. Then we also need some layers. So it will be kas dot layers import. We


05:56:05
know we have different types of layers. So for our simplicity sake, I'll import all our layers over here. And then we have from kas dot layers dot activation or advanc activation. We'll import this later. So let me just have something like models import sequential model and then also import functional API that is model. Now we also need some optimizers that is from kas do opoptimizer import and as you can see I've done a small typo here. Let me fix this as well. And let's also add this advanced model again


05:56:45
from kas dot layers dot advanced model. It's going to be advanc activation import leaky rail. Usually the activation function that we would use in normal CNN is going to be like ReLU or DNH. But here we're going to use leaky ReLU. ReLU is just that you know in the lower level. So let me show you what this means. So if I take this thing ReLU goes something like this over here. So this goes here and it ends at zero and then there's exponential growth. So either it is at this point or at this


05:57:16
point. But when it comes to leaky relu the graph over here it's going to be same over here but on this coordinate it'll be a small dip. So we have a small space here. So this is called as leaky relu. And apart from this let's also get some generic ones. So import numpy as n and also import mpodl as plt. So let's execute this. That's great. What we'll do is you know sometimes we do get the warnings. So let me ignore the warnings. So import warnings. So now warnings let me execute this. Then we have warnings


05:57:50
dot filter warning and just pass here ignore. This looks great. So now we are supposed to get our data set. So for getting a data set I just have mnest which we have loaded and then we just have to do load data. We all know that this gives us a two values. So that's x train y train and again x test y test. So what we're going to do is we're going to have something in this format. And now we'll just have x train here. So now let me quickly execute this app. And now if I show you extra dot shape it's going


05:58:22
to be 60,000 and 28 28 is the picture size. And now what I'm going to do here in order to normalize my image what we'll do is extra is equal to extra minus 127.5 divide this by the same 127.5. So print we want the maximum value and the minimum value that is extreme dot minimum and similarly print extra dot max and now you can see it would be ranging from minus1 to 1. If I divide this by like 255 then everything over here would be from 0 to 1. So this is what we have done and now we supposed to


05:59:05
create the batches. What we going to do? Yeah, we just going to use a train images or the images present in this train to learn and then we are going to generate our own very own M& data. So it'll be total epoch. Let's keep it as 50. Then we have the batch size. Batch size basically means in one go how many images can be there. So it will be 256. Then we have half batch. That's going to be half of this. That's 128. And then number of batches. This should be in extreme do.shape.


05:59:36
shape which is present in index zero divide this by bat size. And then let's also add some noise here. So noise dimension so it's going to be 100. And finally for our optimizer what we're going to do here is Adam we just change our learning rate here. So L R this would be equal to - 4 and then beta 1 would be 0.5. So this basically means 0.00002. And now let's execute this. We are done. So what we're going to do now, we going to create a generator model. So generator model. And here we are


06:00:15
basically trying to do upsampling. So first off, as this is the model, right? So we'll give the name here as generator. We'll keep this as a lower case generator. And this is going to be the sequential model. This is pretty similar to what we going to have. So we'll have no generate. Let me just execute this. And now we'll have generator dot add. Now we'll add dense layer here. And now we'll call units. Number of units that I need over here. It will be 7 * 7 * 128. And then we have


06:00:47
input shape and this would be noise dimension, so basically this thing noise dimension is 100, right? So this is going to give me the bat size. And then comma is basically telling me the values. And then again we have generator dot add and we'll do the reshape here. So it will be 7A 7A 128. Then again generator do add. We're going to add a leaky relu. And the value that I'm going to give here is going to be 0.3. That's the alpha value or 0.2. It's totally up to you. And then we'll do


06:01:20
some batch normalization just to prevent overfitting. So this would give me like the size over here is going to be 7 + 7 cross 128. Now we'll try to decrease this and increase these values. So let me write that down. 7A 7A 128. We will try to do 14A 14A 64. So this is going to be generator add. So we'll have convolution 2D transpose. Number of layer that we going to add is going to be 64. Then we have kernel size that will be 3 + 3. Stride will take two. And even after that padding would


06:01:55
remain the same. So now we'll do generator dot add dku and we'll give the beta value here would be 0.2 and again we'll try to add batch normalization. Now again we'll try to do the upsampling. What we'll do is we'll change this. We'll change this to 28 28 1. This is the actual image that we need. So this is the size of the image that we have over here. So for this we'll do generator add. It's going to be the same thing that's over here. is just that we'll change the dimensions. So instead


06:02:29
of 64 it's going to be one and rest everything remain the same. And just a small change that I'm going to do here add an activation function tanh. This looks great and finally we are going to compile our model. So generator do compile. So we'll have loss this will be kas. So what we'll do here we'll also import kas. So import just kas. So now we'll come here kas dot losses dot binary cross entropy and then we have optimizer we'll use our custom optimizer that is Adam we have used this Adam over here


06:03:06
somewhere in the top and finally we'll try to do general dot summary that's our model dot summary and let's execute this now so as you can see here first off we have the batch size we have 7 + 7 and finally we'll do the upsampling and we get this image 28A 28A 1. Now we're going to do discriminator model or you can say downsampling. The input size over here is going to be 28 28 1. If you're unable to understand what's happening here, I would highly recommend you to check out any of the


06:03:36
Edure's GAN tutorial projects. The reason for this is because GANs are pretty high level models that requires you to have some fundamental knowledge over here. So finally, what we're going to do now, we are going to do down sampling. So the input shape here is going to be 28. Let me just copy this from here. And we're going to downsample this to 14, 14, 64. So this is basically we are trying to do the reverse of this model generator model. So here we'll have discriminator and this would also be a


06:04:05
sequential model and let me execute this so that you know I start getting the recommendations here. So now that we have done with this, so we'll have discriminator dot add let's say cont. And now the number of layers I need here is going to be 64. Then I have kernel size 3, 3. Then I have strides. This will also remain the same. 2A 2. We have padding. We want the padding to be same. And as this is the first layer, we are going to add our input shape as well. And this is going to be 28, 28, 1. So


06:04:36
now that we are done with this, let's also try to add some other layers. So we have discriminator is equal to dot add. And now we just going to add leaky relu and give an alpha value here to be 0.2. Try this with a different alpha values like 0.3 and whichever suits you. And now what we'll do, we'll again do the down sampling. So let me give this as a comment. And we're going to down sample this thing to 128 and this should be 7. We're going to perform the same step. I'll just copy this for now. And as we


06:05:07
all know there's slight changes that we need to do. First off instead of 54 this is going to be 64, right? Not 54. And instead of here 54 we going to have 128. And we don't have to provide the input shape over here as well. Rest everything remains the same. And finally we will down sample this again from this vector to a scalar value. That is scalar here is going to be 6,272. And now we'll do discriminator dot add. So this is going to be flatten here. We'll do some discriminator dot add


06:05:42
dense and we'll pass here 100. And we'll also give some leaky relu here. Let me just copy this. And finally, we'll do discriminator add dense layer. And we just want the output for this to be one. And the activation as this is just one, it's going to be sigmoid. So this is it about the model. And now we are going to compile this. So it's going to be discriminator dot compile. So the loss function that I'm going to use here is binary cross entropy again. So it's going to be kas


06:06:14
dot losses dotbinary cross entropy optimizer that I'm going to use will be Adam the one which you have defined Adam and finally let's compile this this is not going to be compile this is going to be summary and let's execute this now so as you can see here we are doing the reverse process of what we did in our generator it's usually 28 28 and then every time I pass this through a convolution layer obviously there's change in size and as I move down you know the size decreased ES and we are


06:06:42
presented with one. So this is about like building our generator model and even our discriminator model. So fine in the next stage or the final stage of model creation we are going to combine it. Combine model. So over here we have describer describer trainable uh we'll give this as false. Then we have GAN input. So this is a name of a model that's a GAN input and I have input and this will be shape and this is going to be noise dimension. And then finally we have generated image. This will be generator of GAN input. And


06:07:19
finally we have GAN output. This would again be discriminator. And we're going to pass here generated input or image. And now we'll use a functional API. We are using this to create a final model. So it will be model this will be model we supposed to part the inputs right so it will be GAN input and the output will be GAN output and finally just like others we are supposed to compile a model it will be model compile so here loss we are going to use as binary cross entropy so kas dot losses not binary cross entropy and


06:07:51
optimizer will be Adam looks great let me execute this the reason why getting this error is because of this and let's try to execute ute this and now as you can see we don't have any errors. Let's quickly see how this model would look like. So model dots summary and let's run this. So as you can see we have input shape and this is the output and then finally this input over here. So now we are going to come down come back to our data set that's train. We'll reshape this. So


06:08:20
it'll be train dot reshape. We're going to have minus one then 28 that is a no on the x axis 28 on the y axis and one is a channel. And if I run this and add here extreme dotshape you will see it looks like 60,000 28 28 and 1. This is the batch size and then we have 28 28 is the size of the image and one is the number of channels. So this is all there is about building a model. Now finally we are supposed to create a training loop. Let me give a comment again. Training loop. So we'll


06:08:51
have discriminator loss. At the same time we'll also have generator loss. We'll have a for loop here for epoch in range total epochs. Here we'll have one value that is epoch discriminator losses there will be 0.0. Similarly epoch generative loss and even this will be 0.0. So now we're going to have mini batch gradient descent for step in range number of batch. Let me give some comments over here just so that it's easier for you guys to understand. So in the step one we're


06:09:27
going to train discriminator. So for this what we're going to do is we're going to have discriminator trainable is will be equal to true that is how do I do this is by this part. Okay discriminator. So first off we need to get the real data. So this will be index this will be equal to np.trandom rand. So we'll have 0 comma 60,000 comma half patch and then we'll have real images this will be extreme present in that index idx and now get the fake image or you can say data so we'll have


06:10:03
noise this will be equal to np dot random dot normal so we'll have 0 comma 1 noise should range size this will be half batch comma noise dimension so now it will be fake image This will be generator dotpredict noise. And now we'll do a labels. So now we need that label. So it will be a real y is equal to np do.1 once. So this would be half batch comma 1 *.9. So time 0.9 and then fake y this would be np.0. Then we have half batch comma 1. So now it's time to train D. D loss_real. This would be equal to


06:10:53
discriminator.train on batch. So here we're going to pass real image and then real y. And then for the fake d loss fake this would also be the same. It's that instead of real it's going to be a y. So let me do this fake images and then this is going to be fake y. And finally d loss this would be 0.5 * d loss real plus 0.5 * d loss fe and then finally we have to do epox loss that is epoxy loss plus equal to d loss. So this is the stage one. In the stage one, we are going to just train this


06:11:39
part over here. And now coming down to the stage two that is let me give a comment here. Train generator. And when I'm training generator obviously my discriminator has to be false or disabled. So here we have noise. So there will be np.trandom dot normal. So here we are going to pass 0 comma 1 comma size. uh this would be bat size comma noise dimension and now we have ground truth so this will be np1s and let's say let's provide a batch size I mean the size for this comma 1 and then we have epo


06:12:20
gloss plus equal to gloss so finally we have to print this out so what we're going to do we're going to write a function to display the images that we have created but before that let me quickly write here print so it'll be epoch and we'll pass this data here. Then we have this clause that is discriminator loss. We'll provide this over here and generator loss. We'll provide this over here. So here we'll have epoch + one. And then over here we are going to have epoch_d


06:12:52
loss divide this by number of batches. And finally for our loss over here that is generated loss. I'm going to add something similar. Let me give this on the next line. just so that it's easier to read. So here it's going to be the similar thing but instead of D it's going to be G. So let me copy this and paste this over here. And finally we'll have D losses dotappend. We're going to append the same value. So we going to append this for the discriminator loss. Me copy and paste here. And then this should be


06:13:28
D losses.append. It's going to be G not D. And we're going to add this value here. And finally, if generator, it's going to be like this. If the number of epochs + 1 percentage 10 is equal equal to zero, then save it. Give a extension as generator.h5. And now we also want to display the images. So what we'll do display image, we'll write a function display image. And we'll just have to display this image here. So what's going to happen is for every 10 images. So for every 10


06:14:05
image it'll display the epochs losses and then the images. For us to display the image we have to write another function here. We'll do that as well. This we'll have a small function def display image images and we'll give samples is equal to 5 25. Now we have noise. This would be np do.trandom dotn normal. Here we'll have 0a 1 comma size. The size would be number of samples that's 25 common noise dimension. So generator image this would be equal to generator dotpredict noise.


06:14:40
And finally we'll just plot the figure here. So plt dot figure we'll give you a figure size. Let's say 10. And I want the images to be plotted next to each other. So for i in range samples as it's 25. You know it's going to be five in row and five in columns. So plt dots subplot 5 comma 5 that's five rows five columns and then we just have to provide the index that's i + 1. Then we have plt do im show generated image which is present in index i dot reshape 28 28 and I want the cap to be gray. I


06:15:18
can either keep it gray or binary totally up to you. I'll give it as binary. Then plt do.axis we'll keep this as off. nt.show and yeah this is it. So let me quickly give you a walkthrough of what we are doing here. Let me zoom out. First off we try to get our data. First off we are trying to ignore the warnings. Then we import the libraries. Then we get our data. Try to reshape it. We over here I want my data to range from minus1 to one. I hope now you understand why we using leaky relu. And now we are trying


06:15:48
to create the bat size noise dimension and customize our function. Over here there are two models that are involved. the generator model where we are basically trying to do the upsampling and then we have also done downsampling using discriminator model. Both of these models are combined over here. As you can see we are combining them to create a GAN model. That's our final model. And the way we train our model is quite different. Like usually we use fit and provide all the values. But here we are


06:16:13
trying to write a customized training loop. For every 10 epochs this function would be called and this would actually generate the images. So let me do one thing. Let me quickly run this up now. Before you start, make sure you remove this comment from here. And first off, we are going to train our discriminator as I mentioned earlier. And then we are going to give this a false, right? Discriminator trainable would be false. And let me quickly run this and see what happens here. This should take quite


06:16:38
some time as training generators adversarial neural networks are pretty intense. This should take quite some time over here. And let's see what happens. All right guys, so as you can see here we have successfully created our images and now if I tell you this right this is not the image that which is present in our data set here. This is the beautiful concept of GANs where this image over here we have generated this by ourselves and this is what GANs does. So let's wait for another batch over here. Let's


06:17:12
wait for another 10 batches and let's see what happens and how that thing would look like. All right guys, so as you can see here now this is the second batch of the image that we have received. Let me zoom out a bit. Let me zoom out over here. So as you can see the images over here are better. So we can see this 5 6 3 6 7 and all of these are the images that was generated by our model. And as you can see all this is pretty simple and short but there's a huge amount of complications that go in


06:17:40
for developing this particular model. So if you have this kind of project in your arsenal, you know, you can crack out any interview or questions that has been posed to you. [Music] Midjourney is a generative AI tool that turns your text description into stunning images. Now let's see how it industry can use Mjourney. Users can generate mock-ups with simple text form and visualize idea before the development. Marketers can create eye-catching visuals for the campaign, test multiple concepts and quickly


06:18:12
personalize content for their targeted audience. Mjourney even help create illustrations, build fictional worlds, visualize data for content creator and help them go beyond. Mjourney is a gamecher streamlining design, marketing and content creation within the ID industry. The ID before we go and explore it, let's see some of its products. A realistic portrait of a woman holding a white cat. An anime-l like character with powerful effect drawing of an ideal scenery and cinematic view of soldier in middle of


06:18:43
war. Let's see why use major. I would just say three points. It's easy to use. Boost creativity. Explore different style and find the one you like. So to get started, let's see how to sign up for Mjourney. Go into your favorite search browser. Then go to search bar and type mjourney. Let's go ahead and click enter. So we'll see the Mjourney's official website. Go ahead and click on that. There you'll see a landing page that will look something like this. To join Mjourney, we'll go ahead and click


06:19:12
on join the beta. After clicking this, this will take us to their discord invite. For this, you have to have one discord account. Since we have already created one, we'll go ahead and just sign into it. As soon as you join, you will see something like this. Mid journeys icon you will find on left hand side of your screen. That will look something like a sailing boat. Now to go ahead, we'll just scroll down and join any of the newbie channels. There you can see so many creation of so many


06:19:41
people. This is an open channel. Anyone can come and write a prompt. They'll get their image generated right over here. But in this you'll lose your image as soon as you create it because there's so much content over here. So to go ahead and add Majourbot to your server or directly message it, we have to go on left hand side and click on symbol that will look something like this. Here you can see Mjourney bot. Right click on it. There you can see the profile. Click on profile. It will open a menu like this.


06:20:12
If you click on send message, it will directly take you to the Moneybot. There you can message directly and get your results. So let's go ahead and see our information. To do that we have to type / info then just hit enter. Here we can see user ID a subscription and even the fast time remaining. Here we can see how many images we have created and fast images that has been generated by my journey. To go ahead and join my journeybot to a personal server we have to create a server first. Let's go ahead


06:20:44
and create a server. On the left hand side of your screen you can see something like this. Click on it. Then you can select create my own. You can choose for me and my friends and go ahead and name it. For now, we'll name it Edorica server 3. We'll go ahead and create. Here we have landed on our page. Now to add bot to a server, we'll just go ahead and click over here. We can see my journeybot. When you click on it, you can see an option of add app. Click on it and then you can choose a server you


06:21:18
want to add it to. We'll go ahead and choose Edrica server 3. Then click continue. After that you would have to select the permission you want to give Mjourney bot. Then go ahead and authorize it. Here we got success. After adding Mjourney bot to your server you can see a message like this. Let's go ahead and click on server 3. So we'll go to Edora server. We can see the Mjourney bot has landed. Let's go ahead and create a first image. To do that we would have to type in / imagine then hit


06:21:50
enter. Here you can see a prompl like option. Now following prompt you have to give the description of the image you want. For now we'll go ahead and type this a bustling city street in modern era. Let's go and see what does mourney generates. So the image has been generated. Let's go ahead and click on it to get a better view. The images will be generated in the grid of four in order 1 2 3 and four. Below the generated image, you can see multiple option like this with U and V where the number would correspond to


06:22:29
the image. Here we can see a roll button that will look something like this. The U stands for upscaling and the V stands for varying. If you like a image, for instance, I like the image too. We'll go ahead and click U2. As we can see the image has been generated. This is the exact image that I have selected the second number. If you like it, you can just go ahead and download it. To do that, click on open browser. You can zoom in, zoom out to get a better view and right click on it to save it. We'll go ahead and save it as


06:23:05
an image. Just click on save and it will be downloaded to your disk. Let's go back to our discord. Here below the image we can see multiple options again but this time the options are different. Here we can see upscale settle and upscale creative. Both are used to upscale the current image but in a different manner. It will slightly upscale your image whereas creative will change something in the image. Vary vary has multiple options subtle and strong. Strong will generate a totally different


06:23:37
image but with similar reference. Whereas vary region if in the image you want to change something you can select it by the region and then change it. We will dive into that later. So let's go ahead and see zoom out. There's option of zoom out to zoom out 1.5 and custom zoom out. If you want to zoom out you can click on any of these. And if you want to customize your zoom you can click on this. We'll dive into that later. Moving ahead, we can see multiple arrows pointing in different direction.


06:24:08
What does this mean? Let me tell you. If you click on the arrow, it'll pan out the image in the direction which the arrow is pointing. If the arrow is pointing left, the image will pan out on the left side. If the arrow is pointing at right, the image will pan out on the right side. Similarly, on the top and the bottom. Now, let's go ahead and test out our zoom. We'll click on zoom out 2.0 and see what it generates. The image has been generated. We'll go ahead and click into it to get a better


06:24:40
view. We'll upscale all of these. This is our original image. Let's go ahead and see our zoomed out images. In a fourth image, these parts have been added which were originally not here. In a third image, we can see a different scenario over here. There's a zebra crossing whereas in fourth image it is not. In every scenario we can see a different vehicle or a different building. Nothing has changed from the original image. But to zoom out it had to create new objects that we can see. Now let's go ahead and


06:25:20
see pan out. To go to our original image we can click on this and it'll take us to our original image. Now let's go ahead and see pan out. Now I want to see what's up there in that image. To do that we'd have to click on pan out with top arrow. We go ahead and do that. It'll give us a prompt window. Now you can choose what you want. As we are talking about sky, we can have moon or sun over there or a plane flying. Let's go ahead and do a sun in cloudy weather and click on submit.


06:26:00
As we can see in our newly generated images, there is sun, there are clouds and even both of them. Now to create perfect prompt, you need to use proper keywords and even give description as much as you can. For that we'll go ahead and generate this image in painting format. To do that, we'll type / imagine and then prompt. Our prompt will be same as previous one and we have just added oil painting. Now let's go ahead and hit enter. Here the M journey has used vibrant color in oil painting. All the images


06:26:38
look stunning. Now we'll see some of its advanced feature. One of those was released recently by Mjourney that was describe. Describe feature of Mjourney allows you to use a image to get its prompt. You can use the prompt to generate new images similar to that one. For that we'll go ahead and type /escribe. Now there are two option image and link. You can go ahead and click on image to upload image from your system. If you go ahead and click on link, you can provide link from anywhere that can be used by


06:27:11
mourney to generate description for that image. For now we'll go ahead and click on image. You can either drag or drop your picture or you can go ahead and select it. Now I have selected the image. Just go ahead and hit enter. As a reply to your image, Mjourney bot will generate several of its prompt that can be used to generate images similar to that one. Scrolling down, you can generate any of the images based on the prompt number given or you can imagine them all. Let's go ahead and imagine


06:27:47
all. Here we can see all the images generated by all four prompts. Those are similar to the one we had given. Now let's go ahead and talk about settings in the mid journey. To check those type setting here we can select the model that we want for our M journey and so many other like stylus. style is low, medium or high. Controls the amount of Mjourney's creative influence on a pictures. Next is public mode. Public mode allows your picture to be seen in Mjourney's public collection. Then comes the remix mode.


06:28:29
Remix mode allows you to give extra prompt during variation in image. If you have turned it off, it won't ask you for prompt again. Just vary your current image based on its prompt. Then there is variation mode. When we choose variation, it'll go ahead and check our preferences. Either it is low or high. Based on that, it'll give us a varied image. Then we have different modes. Relax mode, fast mode, and turbo mode. All these three modes define the speed at which mid journey will generate your


06:29:02
images. For relax mode, it will generate very slow. But for fast mode, it will use your fast but will generate faster. And for turbo mode, your results would be instant. but consume more fast hours. You can click on reset setting and set your setting to default. At last, raw mode. Raw mode access a more literal interpretation of your prompt. Now, let's go ahead and design some logos. I have chosen logo for a bakery. Here the midjourney has given us some amazing logos. We can select any one of those


06:29:47
and use it our own. Now let's go ahead and explore different parameters that mjourney allows. For starters, we use a new prompt. To add a parameter, we just need to add double slash followed by the parameter. For the first parameter, we would select aspect ratio. Now aspect ratio is the ratio between height and width of a image. Let's go ahead and type in 16x9 and hit enter. Now as we can see the aspect ratio of the image has changed from the previous ones. Those images were in square and this is a widecreen


06:30:26
image. Now let's go ahead and discuss a different prompt that is negative prompt. For that we'll go ahead and type the same prompt. You can add multiple parameter in a single prompt. For negative prompting, you use n o no and followed by the object you want to remove. We want to remove clouds. We'll type in clouds and hit enter. The clouds have been removed from our original image here. Cloud cloud and cloud even here. But in our newly generated images, there are no clouds because it has taken a unique approach


06:31:10
of making our dolphin underwater. So let's go ahead and check out the parameter that controls imagination of a majour that is stylus. Now we'll go ahead and type in same prompt. For stylus, the range is from 1 to,000. And to use stylus we would type single s followed by the value we want to zero being the lowest and th00and being the highest. We would go and type in 1,000 and see the result. This is a final result and it is stunning. I am amazed. Moving to another parameter chaos. Higher value of this parameter lead to


06:31:52
unexpected and unique outcomes. So let's go ahead and try it. with the kos value one we get results something like this. Now let's move to using reference images to create our own images. For that we need to go ahead and upload one of our photos. So let's go upload a file. I'm going to use this picture. We'll go ahead and open in a browser. copy its link. For our next one, we would go ahead and type imagine paste the copied link. Then type the prompt we want to near a lake with rocky


06:32:39
mountain and cloudy sky. Here are generated images. Now let's go ahead and see another parameter image weight. It controls the weight of referenced image on our new regenerated image. Values ranging from 0 to two. 0 being the lowest and two being the highest. Image weight parameter is given by IW and value I have given as zero. Let's hit enter. With the image weight being zero, the reference has not been taken into consideration. Now let's try it with value two. Our reference image is the first


06:33:38
priority and the prompt is second. With that, let's see how to create stock images. To create stock images, we need to use words like realistic and describe as much as we can in that image. Photography of adult Caucasian female wearing a white shirt and black tie, smiling, crosshanded, studio shot, cinematic look, clean background and white background. Adding all these now we'll just hit enter. You can see there are multiple keywords that represent stock image. So here are generated images. Let's go


06:34:19
ahead and upscale first one. It's a stock image. It's a real looking. Let's create a second stock image in a group. Photo of a diverse team. Remember the keyword photo. Photo of a diverse team celebrating a successful project in a co-working space. Let's hit enter. We'll go ahead and upscale the fourth image. It's a stock image. Definitely one. With that, we'll conclude our video. And now I'll show you in my journey how you can use. When you enter my journey website,


06:35:01
you can see collection of photos like this that has been generated by multiple midjourney users. So you can find all your generated images in archive. You can see all of these images right over here. You can join the rooms. Those are based on themes or read the images as you like. Mjourney has recently introduced Mjourney's web creation alpha that allows you to create images on the web. This is currently available only for the user those have created more than 100 images already. You can type


06:35:35
your prompt directly here without giving any command like / imagine or anything. Let's just go ahead and type shark in mechanical suit. Now our image is being generated in create. We'll go ahead and check it out. These are the images of shark and mechanical suit. You can even vary both of these subtle or strong or you can use rerun button to reroll the images. You can even see the prompt that you have given for previously generated images by you. [Music] This is something I was going through


06:36:18
recently and I found it so interesting that I wanted to share it with you. Now most of you all must have heard of chat GPT the super trending AI text generator that generates responses based on user prompts. This technology was developed by a company called as open AI and along with chat GPT they released a few APIs that are super convenient to use in your code. You can find these models if you search for openai.com/appi/pricing. Once you're in this page, if you scroll down a bit, you will be able to see a


06:36:57
section called as language models. Now, this section has a collection of base models that you can directly use in your code. There are currently four options available, which is ADA, Babage, Cury, and Dainci. Now, each of these models have slightly better capabilities than the previous one. And depending on these capabilities, prices can also differ. Now the ADA model which is the least capable out of these four has the fastest response time and Dainci is the most capable while being the slowest out


06:37:34
of all these options. Now coming to the pricing, you can see that the prices are mentioned per thousand tokens. So what is tokens? You can think of tokens as words and like it is mentioned over here 1,000 tokens is approximately 750 words. You will be charged based on how many tokens you use in your program. If you want to look at the differences then there is a separate page for that too. Over here you can see the details, capabilities and restrictions of each of these models. All these models are


06:38:14
developed on the GP3 models and the Dainci model which is the most capable has a max request limit of 4,000 tokens and the training data was taken up to June of 2021. The query model is slightly less capable than the Davinci model, but it is faster and costs way less than Dainci. It has a max limit of 248 tokens. And you can also see that it was trained only until October of 2019. The Babbage model is another option which is capable of straightforward tasks and is very fast while giving your answers at a fraction of the cost. And


06:38:57
the ADA model is super fast while being restricted to very simple tasks. Now just because the capabilities differ doesn't really mean that the ADA is worse than Davinci. It all depends on your use case. If your use case is very simple and straightforward, you would be better off using ADA or Babbage. And if you have some complex mathematical tasks, then you can use a text generator like Dainci. While using these models in code, you will also have to insert some parameters. Based on this parameter, the


06:39:34
prices can also heavily differ. To use the OpenAI APIs, you will first need to head over to openai.com. Once you are in here, you will find an option called as API on the top. Click on that and then log to your account. If you don't have an account yet, then I highly recommend that you make one. Now, once you log in to Open EI, you will find yourself in a web page similar to this. What you need to do right now is head over to your account and then click on view API keys. Once you're here,


06:40:11
click on this button that says create new secret key. Once this key has been generated, make sure that you copy this key. Let's just name this key as open AAI do API key. Now before we execute this code, we will need to install a library called as open AAI. So let's just do that by using the pip command. As you can see, I have already installed OpenAI in my system. But if you haven't already, then this command will install OpenAI library in your system. Once that is done, you can get rid of that cell


06:40:47
and import the OpenAI library into your code. Now let's run this. Yes, your key needs to be passed as a string. Now let's run it again. And you can see that it has executed successfully. Now let's define a simple prompt. We'll say variable prompt is equal to and then a message. Let me just type hi, how are you today? Now this will be our prompt. Now that we have access to open AI and we have defined our prompt, the only thing left is to generate a response. Now to generate the response, let's


06:41:30
start by typing variable response equal to open AI dot completion.reate. Now this is the function that you use to generate responses. Inside the create method, we will start defining our parameters. The first parameter that we are going to define is the type of engine that we will use. So you can just type engine equal to and then you can specify the model that you will use. If you want the details of the models, you will find it over here. These are the models that you can use in your code. Let's first start with the ADA


06:42:08
model. The next parameter that you will need to specify is your prompt. This parameter will be passed to your model to generate a response. Since tokens are expensive, it's best practice to set a max limit to your tokens. You can do this by typing the max tokens parameter and setting a value for it. Let me just set it to 50. And one more interesting parameter is the number of responses that you want to generate. So we can set it as n equal to one. This will tell the model to generate only one response.


06:42:46
Another interesting parameter is the temperature parameter. This parameter will tell the model how flexible it can be with the response. This value can be anywhere between 0 and 1. A temperature of one will make the generated response quite formal while a value of 0.1 will keep the response extremely creative. Keep in mind that lesser temperatures can also make your responses actually incorrect. And when I say one, it will show one. When I say zero, it will show zero. And then 0.1 for now. Let's keep the temperature


06:43:26
value at 0.5. With this we have generated the response. To view this response you will just need to write one more line of code. This can be done by using the print function and then typing response dot choices. Now choices is a list of generated response. Since we have set n as one, we'll be getting only one output. So our responses choice index is zero. We also need to see the text format of the response. So we type dot text at the end. Once we execute this code, you can see that our model has generated a


06:44:07
response. You can also change the model or the number of responses if you don't find the answer satisfactory. And now you can see that our response is more friendly. So guys, this is how you use OpenAI API in Python. [Music] What if you could create personalized eye-catching ads in minutes instead of hours? Or generate dynamic content that speaks directly to your audience no matter where they are. Sounds like the future, right? But the future is now. Take Starbucks for instance. They are using generative AI to tailor marketing


06:44:50
messages based on customer preferences and behaviors, creating personal and relevant campaigns. Sephora has jumped on board too with AIdriven product recommendations that engage customers in a way that feels like they are speaking to a personal beauty advisor. In this video, we are diving into how generating AI is not just improving marketing campaigns, it's transforming them, making them smarter, more efficient, and more impactful. From targeted emails to hyperpersonalized ads, AI is changing


06:45:19
the game. So, want to know how? Stick with us to learn how your brand can ride the AI wave and boost engagement like never before. So, let's get started. Now, without any further delay, let us get started with the generative AI use case. So for this use case I have used the popular company Amazon and we will see how Amazon uses generative AI. Amazon started as an online bookstore and has since become a global leader in e-commerce, cloud computing, streaming and artificial intelligence. The everything store is among the big five


06:45:52
United States tech companies along with Alphabet, Apple, Meta and Microsoft. But what if I told you that Amazon has also faced challenges because of its extensive reviews make it difficult for customers to extract insights restricting decisions and satisfaction. Now let me tell you in details what challenges they faced. First is in terms of the volume and diversity of reviews. With approximately 1.5 billion reviews and ratings contributed annually, distaining meaningful information from such a large and diverse data set is


06:46:25
complex. Next, due to its identifying common themes, manually summarizing prevalent sentiments and frequently mentioned product features across numerous reviews is impractical. Next is the maintaining authenticity. Ensuring that summaries accurately reflect genuine customer opinions without introducing bias or misrepresentations is crucial. And to address these challenges, Amazon explored advanced technologies capable of processing and summarizing extensive textual data. Generative AI emerged as a promising


06:46:56
solutions due to its ability to analyze large data sets and generate coherent summaries. The company saw tools that could efficiently process vast amounts of review data, then identify and extract common themes and sentiments, then generate concise summaries that are both informative and reflective of authentic customer opinions. Now let us see what are the tools implemented. Amazon implemented a generative AI powered features designed to analyze customer reviews and produce concise summaries. This AI model was trained to


06:47:28
evaluate the frequency and context of specific product features mentioned in reviews. Then assess customer sentiment associated with these features and generate a brief paragraph highlighting key insights which is displayed directly on the product detail page. And the solution is the AI generated review highlights provide customers with a concise overview of common themes and sentiments expressed in reviews. For example, a customer interested in a product's ease of use can quickly see if this aspect is frequently praised or


06:47:59
criticized. Additionally, customers can tap on specific product attributes to view all reviews mentioning those features, allowing for a more targeted and efficient review reading experience. And this implementations not only streamlines the decision making process for shoppers but also enhances the confidence in purchasing by providing quick access to relevant insights. By leveraging generative AI, Amazon has improved the accessibility and utility of customer reviews ultimately contributing to a more satisfying


06:48:28
shopping experience. But why do we need generative AI to enhance marketing campaigns? Well, generative AI enhances marketing campaigns by improving personalizations, automating creative task and optimizing targeting strategies. It saves time, boost engagement, and ensures higher ROI by creating tailored content, streamlining workflows, and delivering datadriven insights. But what exactly is generative AI? Generative AI is a subset of artificial intelligence that focuses on creating new content or idea rather than


06:49:02
simply analyzing existing data. Unlike other types of AI that rely on pre-programmed rules or patterns, generative AI uses algorithms to learn from data and generate new outputs. This technology is often used in tasks such as image generation, task generation, and even music composition. But how has AI impacted marketing? AI is transforming the marketing industries partly because of its vast applications. AI tools can automate routine task freeing up valuable time and allowing you to focus your efforts on


06:49:34
strategizing. But task automation is just one example of AI use cases in marketing. AI improves you to analyze vast amounts of data better to understand your target audience, consumer preferences and behaviors. You can use this data to predict customer behavior, optimize campaigns in real time and deliver highly personalized content to mold an exceptional customer experience. Beyond analytics and reporting, generative AI tools can support content creation. And this dual impact of enhanced personalization and


06:50:06
increased efficiency is transforming business decisions and how brands engage with customers. Now let us see what will happen to marketing in the age of AI. First let us discuss about the hyperpersonalization in marketing. AI helps businesses analyze vast amount of data to understand customer behavior, preferences and needs. This allows for personalized marketing strategies such as targeted ads, tailored messages based on browsing history, purchase patterns and demographics. Next is the dynamic


06:50:38
content such as emails, websites or app interfaces that adapts in real time for each user. Then comes the customer segmentation. Identifying specific groups of user for highly relevant marketing efforts. Next is the enhanced efficiency through automation. AI automates repetitive marketing task such as ad placements and binding. AI tools optimize budgets by selecting where and when to display ads for maximum ROI. Next is the email campaigns. Tools like Mailchimps use AI to determine the best time and content to engage users. Next


06:51:17
is the chat bots. AI powered assistants provide instant roundthe-clock customer support and can upsell or cross-ell products. Next, AI as a creative partner. AI tools like chat GPT d and midjourney assist in creative task writing engaging ad copy or scripts designing visuals or creating mockups and producing ideas for campaigns based on data insights. Next let us talk about the ethical and privacy challenges. AI in marketing comes with a risk and responsibilities such as data privacy where companies must handle consumer


06:51:53
data ethically adhering to regulations like GDPR. Next is the bias in AI. AI might perpetuate biases if trained on biased data sets. Next is the transparency. Marketers must ensure AIdriven decisions are clear to consumers. Next, let us see the future implications. First is the emotion AI tools that analyze emotions via facial expressions or voice to tailor experiences. Next is the voice search and assistance. AI like Alexa and Google Assistants impact SEO and product discovery. Next is a customer centric


06:52:31
focus. Despite AI, brands must remain authentic and human in their storytelling to build trust. Now let us see the benefits of generative AI for digital marketing. Well, generative AI offers several benefits for digital marketing. So let us see what are the benefits. Generative AI is transforming digital marketing by improving efficiency and personalization. It automates task like content creation, enabling businesses to produce highquality content at scale while saving time and resources by analyzing


06:53:04
customer data. AI tailor marketing messages and offers increasing engagement and conversion rates through personalized interactions. Additionally, generative AI enhances creativity and customer experiences. It helps businesses explore innovative strategies and solutions such as new marketing approaches or product ideas. AI powered tools like chatbots also deliver engaging interactive support ensuring better customer satisfaction and stronger connections. Now let us have a look at the impact of generative AI on


06:53:35
SEO and search marketing. Generative AI is transforming SEO and search marketing by analyzing SERPS to uncover patterns and optimize website elements like metatags and content. This enhances search engine ranking driving more organic traffic. AI also generate highquality keywordri content based on search data improving visibility and engagement. In addition, generative AI automates landing page creation for SEM campaigns by analyzing campaigns data and optimizing for conversions. This streamlines marketing efforts, increases


06:54:10
efficiency and ensure businesses attract and retain more visitors effectively. The future of generative AI in digital marketing is promising. So let us see how generative AI is shaping the future of digital marketing by enhancing voice assistants and chatbots to make interactions more natural. It also plays a key role in predictive analytics helping businesses make informed decisions based on the data insights. This advancements can improve customer experience, optimize marketing strategies and predict market trends.


06:54:42
Moreover, generative AI can transformize user generated content by allowing users to create richer, more diverse media such as videos and virtual reality, helping brands build stronger and more engaging communities. What is agentic AI? Agentic AI denotes artificial intelligence systems capable of autonomously executing actions to attain designated objectives. Unlike reactive AI which only responds to the inputs, agentic AI is proactive, capable of planning, adapting and making decisions autonomously. So let's explore


06:55:20
deep into agentic AI and see its capabilities. Agentic AI is a type of artificial intelligence that exhibits autonomous behavior enabling it to take actions and operate without continuous human guidance. It is goal-driven, actively working towards achieving specific objectives rather than passively responding to inputs like reactive AI. And with advanced decision-m capabilities, it can evaluate multiple options, select the optimal course of action based on current conditions and acquired knowledge and


06:55:49
adapt its strategies dynamically in response to unforeseen changes in its environment. Moreover, agentic AI demonstrates proactiveness by taking the initiative to act rather than waiting for external triggers making it highly effective in dynamic and complex scenarios. Now let us see its relevance in the current AI market. When AI systems can act autonomously to accomplish predefined objectives, we call that agentic AI making it highly relevant in the current AI market. Its autonomy allows it to operate without


06:56:21
continuous human guidance, making decisions and adapting dynamically to achieve objectives. This capability is complemented by its advanced problem solving skills, enabling it to evaluate complex situations, strategize and respond effectively to challenges. However, the growing adoption of agentic AI also rises important ethical considerations such as ensuring responsible behavior, minimizing unintended consequences and maintaining transparency in its decision-m processes. Now that you know about


06:56:51
agentic AI, so let us discuss how it differ from other AI systems. Agentic AI differs significantly from other AI systems in its autonomy, decision making and adaptability to achieve long-term goals. Unlike reactive AI which performs predefined task only when prompted such as spam filters or image classifiers, agentic AI takes the initiative and operates independently. It also contrast with the generative AI which focuses on creating content like child GPT generating text but it is not goal-driven by combining autonomous


06:57:26
behavior, strategic decision-m and the ability to adapt dynamically. Agentic AI stands out as a powerful system designed to achieve specific objectives in evolving environments. Now since we know a bit of differences, let us see the comparison between generative AI and agentic AI. Generative AI and agentic AI differ in several key aspects that define their functionality and applications. Generative AI is primarily focused on creation, excelling in output focused tasks such as generating text, images or other form of content. Its


06:57:59
adaptability is limited as it relies heavily on prompts for guidance and lacks the ability to operate independently. In contrast, agentic AI emphasizes autonomy, making it goal-driven and capable of dynamically adapting to changing environments. Unlike the prompt dependent nature of generative AI, agentic AI is self-directed, enabling it to take the initiative and execute strategic task effectively. These differences highlight the complimentary roles of both AI types in addressing distinct challenges. Now


06:58:29
let us see the impact of agentic AI on various industries. Agentic AI has had a profound impact across various industries transforming operations and solving long-standing challenges. Autonomous logistics systems such as those in Amazon warehouses have significantly improved operational efficiency by 30 to 40%. In healthcare, AI enabled surgical robots like the Davinci system have performed over 10 million less invasive procedures worldwide, enhancing precision and patient outcomes. Scientific advancements have also been transformed


06:59:03
by systems like Deep Minds Alpha Fold, which successfully solved the decades old protein folding problem. On a global scale, the World Economic Forum predicts that by 2025, AI will displace 85 million jobs while creating 97 million new ones, reshaping the labor market. And in the energy sector, AI powered smart grids can reduce electricity waste by up to 10%. Promoting greener energy solutions. Additionally, over 90 countries are investing in AI enabled military technology to modernize their defense systems, showcasing the


06:59:37
strategic importance of agentic AI in global security. Now, let us see the applications of agentic AI. Agentic AI is transforming various industries by enabling systems to make autonomous decisions, adapt to changing environments, and achieve specific goals. Autonomous vehicle power self-driving cars and drones to navigate roads, avoid obstacles, and make real-time decisions as seen with Tesla autopilot and autonomous delivery drones. In robotics, agentic AI allows industries healthcare and exploration


07:00:08
robots to perform complex task independently as demonstrated by Boston Dynamics robots used in logistics and rescue operations. Personalized virtual assistants like Google Assistant and Amazon Alexa leverage agentic AI to predict user needs, manage schedules, and execute task without direct commands. And in gaming, adaptive AI agents enhance the experience by creating challenging humanlike opponents such as Alph Go and AI boards into realtime strategy games. In healthcare, Agentic AI supports personalized


07:00:40
treatments, accurate diagnostics, and surgical assistance with examples including AIdriven surgical robots and systems for remote patient monitoring. These applications demonstrate the transformative potential of agentic AI across diverse domains. Agentic AI is making a significant impact across various industries by enabling autonomy, adaptability, and efficiency in diverse applications. In finance, it powers algorithmic trading systems and fraud detection tools, optimizing financial operations such as managing investment


07:01:11
portfolios and identifying fraudulent activities. In smart cities, AI systems manage energy consumptions, optimize traffic flow and enhance public safety with examples like smart traffic lights adapting in real time and autonomous energy grid optimization. In space exploration, autonomous spacecraft and planetary rovers such as NASA's Mars rovers perform exploration task independently. In education, AI powered tutors like Carnegie Learning provide personalized instruction by adapting to individual learning styles. In military


07:01:44
and defense, autonomous drones and surveillance system improves situational awareness and decision making such as AIdriven surveillance drones in defense applications. Now let us see the challenges and risks associated with agentic AI. While agentic AI offers tremendous potential, it also faces several challenges and risk that must be addressed to ensure its safety and ethical deployment. So one key concern is misalignment with human goals where AI system may pursue objectives that conflict with human intentions due to


07:02:17
poorly defined parameters or intended unintended consequences such as autonomous robot prioritizing efficiency over safety. Ethical questions arise regarding accountability and decision-m demonstrated by the challenge of determining who is responsible when an autonomous vehicle causes an accident. The complexity of decision-m in agentic AI can also lead to a lack of transparency making it difficult to understand or explain its actions particularly in sensitive fields like healthcare or finance. Ensuring safety


07:02:48
and reliability is another challenge as AI systems must operate effectively in unpredictable environments such as autonomous drones encountering extreme weather or medical failures. Additionally, agentic AI systems often require substantial computational resources making their deployment costly as seen in advanced robotics and self-driving cars. Security vulnerabilities pose further risk as autonomous systems could be targeted by cyber attacks potentially leading to harmful consequences like the


07:03:17
manipulation of autonomous vehicles. Lastly, overdependence on AI may reduce human oversight or lead to skill degradation in critical areas such as relying too heavily on autonomous systems for medical diagnosis without human validation. These challenges highlight the need for robust design, rigorous testing and ethical frameworks to mitigate risk and maximize the benefits of agentic AI. Now let's see the future of agentic AI. The future of agentic AI is set to be transformative with advancements across various domains


07:03:49
influencing its deployment. Future systems will exhibit increased autonomy and adaptability, enabling them to make a complex decisions in real time and operate effectively in dynamic environments without human intervention. The integration of agentic AI with advanced technologies like quantum computing, IoT, the edge computing will further enhance its capabilities allowing for faster decision making and realtime processing at the edge. These systems will have the widespread applications in sectors such as


07:04:18
healthcare where they will enable autonomous medical diagnostics, personalized treatment plans and robotic surgery. Climate action with advanced systems for environmental monitoring and response and space exploration where smart rovers and spacecraft will carry out missions on their own. As these technologies evolve, ethical concerns and accountability will need to be addressed. promoting the development of regulatory frameworks to ensure responsive AI usage. Additionally, agentic AI will foster human AI


07:04:48
collaboration, enhancing productivity and creativity in the fields such as education, engineering, and research. Imagine asking Chad GPT for a poem and it writes one instantly. Now think about an AI assistant planning your entire day, booking meetings, and even handling emails without your constant input. That's the difference between generative AI which creates content and agentic AI which acts with autonomy making decisions. In 2025, as AI becomes more than just a tool, understanding the shift is very critical. Are we heading


07:05:27
towards just smarter chatbots or truly independent digital agents? Let's break it down through this video. Let's see the key differences between generative AI and agentic AI. Generative AI and agentic AI serve different purposes, each with unique strengths and applications. The key distinction comes down to creativity versus decision making. As previously discussed, generative AI focuses on producing content, whether it's text, image, or code. It enhances creativity by assisting writers, designers, and


07:05:59
developers. But it lacks true autonomy. It only works when prompted and doesn't make any decision on its own. Agent AI on the other hand is designed for interactions and execution. Instead of just generating responses, it can analyze situations, make decisions, and take actions. While it may not create content like generative AI, it can manage workflows, automate task, and adapt to real world conditions. Another key difference is user dependency. Generative AI is entirely reactive, meaning it requires human input to


07:06:31
function. It waits for prompts before generating anything. In contrast, agentic AI is proactive. It can initiate actions independently, setting reminders, optimizing schedules, or even solving problems without human intervention. The applications of these AI types also differ. Generative AI is widely used in content creating, marketing, entertaining, and software development. and agentic AI powers autonomous system like self-driving cars, AI powered customer service and personal assistant that can handle


07:07:02
complex workflows. Both AI types are transforming the industries. But when they work together, they unlock even greater potential. Imagine an AI that not only generates a marketing campaign, but also launches it, tracks engagement, and refine the strategy automatically. The future isn't just about choosing between generative AI and agentic AI. It's about combining them two to build truly intelligent systems. Now that we understand the key differences between these two, let's explore the future of


07:07:31
AI by asking, will generative AI be replaced? As AI continues to evolve, one big question arises. Will agentic AI replace generative AI? Right now, generative AI is everywhere, helping people write, design, and code faster than ever before. But it has one major limitation. It relies entirely on human input. Agent AI on the other hand takes things further. It doesn't just generate, it decides, plans, and even acts. It's the next step towards the true autonomous intelligence. Does that means generative AI will be obsolete?


07:08:06
Not necessarily. The future of AI isn't about one replacing the other. It's about coexisting. Generative AI will keep getting more creative and even sophisticated, producing even higher quality content. Agentic AI will become even more autonomous integrating deeper with industries like healthcare, finance and robotics. But this shift does comes with some risk. As AI takes on decision-making power, we face new challenges. Ethical concerns, unintended consequences, and the need for accountability. If an AI agent makes a


07:08:38
bad decision, who is responsible? And how do we ensure it aligns with the human values? The answer lies in balance. The real future of AI is hybrid approach where generative AI fuels creativity and agentic AI drives intelligent action. Imagine an AI system that not only writes a research paper but also submits it to generals, responds to reviews and refine it automatically. And this is where we are headed. Not just smarter AI, but AI that truly works with us as both a creator and an agent. The question isn't whether


07:09:12
agentic AI will replace generative AI. It's how we'll harness both to shape the future of intelligence. Now that we have explored the differences between generative AI and agentric AI, let's move on to building an intelligent AI agent that can interact with our database using natural language. This means you can simply ask a question like show me all the students who have scored about 80 and the agent will automatically convert it into an SQL query, fetch the data and return the exact result from the database. No need


07:09:41
to write complex SQL queries manually. Just ask and the AI response. Let's dive in and build this powerful system. First, we need to set up a environment to manage our project dependency. To do this, we open the terminal and run the following command. We'll write create p vv python equals to 3.10 10 hyphen y. So creates a new environment and hyphen pvnv specify the environment path as vv. python equals to 3.10 installs python version 3.10 inside the environment and hyphen y automatically confirms the


07:10:29
installation without asking for approval. Once the process is complete, our virtual environment is ready and we can move forward with setting up our agentic AI project. Next, we'll create a file name requirements.txt where we'll list all the necessary libraries for our project. This will help us easily install dependencies in one go. Additionally, we'll create a NV file to securely store our Google generative AI API key, keeping sensitive information separate from our main code. With these files in place, we ensure a


07:11:06
well structured and organized setup for our agentic AI project. First, we will work with SQLite, a lightweight self-contained database engine to create and manage a student database. Let's break it down step by step. So we'll create a file named SQL. py and import the SQLite 3 module which allows us to work with SQLite databases. We'll write import SQLite 3. This module provides all the necessary functions to create a database, insert records, retrieve data, and manage connections. Next, we create a


07:11:42
connection to an SQLite database file named student. DB. We'll write connection equals to SQLite3 doconnect connection equals to SQLite3.connect in the bracket in double inverted comma student db. If this file doesn't exist, SQL light will automatically create it. The connection object will allow us to interact with the database. Now we create a cursor object which is used to execute SQL commands in Python. We'll write cursor equals to connection cursor. Think of the cursor as a tool that helps


07:12:22
us send queries to the database and retrieve results. Now we define a SQL command to create a table named student with four columns. We'll write table_info equals to triple inverted commas. Next we'll create a table. For that we'll write create table then student we'll write in the bracket name type vcar and we'll have 25 characters class type vcar and the same 25 characters section type vcar with 25 characters and marks type integer. Then we'll write cursor.execute in the bracket table info. The name


07:13:07
stores the students name string up to 25 characters. The class store the class's name and the section stores the section of the student. And lastly, the mark stores the marks obtained as integer. Executing this commands creates the table in the database. Next, we insert five student records into the student table using SQL insert statements. I've already created and inserted five values in the table. You can create as much as you can. Each insert commands adds a new role with the students name, class,


07:13:38
section and marks. Now we retrieve and display all records from the student table. For that we'll have to write print in the bracket print in the bracket the inserted records are in the next line we'll write data equals to cursor do.executed executed in the bracket three single inverted comma select star from student closing the inverted commas in the bracket then we'll write for row in data colon print in the bracket row the select star from student query fetches all the data from the table the for loop iterates through


07:14:16
the records and prints them one by one and finally we commit our changes and close the database connection for that we'll Type connection dotcommit and then connection.close. The dotcommit function ensures all the changes are saved in the database. The dot close closes the connection freeing up the system resources. And that's it. We have successfully created a student database, inserted records, and retrieved them using SQLite in Python. Now, let's build an interactive streamllet app that converts natural


07:14:49
language questions into SQL queries using Google's Gemini model. It then retrieves data from an SQLite database and display the result. Let's break it down step by step. But before we start, we have to activate the environment. For that, we'll write activate venv forward slash. And here our environment is activated. First, we'll create a file named app. py and load environment variables using env. For that we'll write from env we'll import load env. Next we'll write load_.env.


07:15:28
It will load all environment variables. This ensures that sensitive information such as API keys is securely stored and accessed. Next we import the necessary modules. For that we'll write import stream lit as ST. Then import OS, then import SQLite 3 and then import Google.generative AI as genai. Streamlight here powers the web interface. OS helps access the environment variables. SQLite 3 allows us to interact with the database and Google generative AI enables the conversion of natural language into SQL


07:16:06
queries. Now we configure the Google Gemini API key. But before that we'll have to create a API key through Google studio itself. I've already generated one. You can create yours through Google studio itself. Then we'll write genai.configure in the bracket API_key equals to os dot get env key. This allows the app to use Gemini 1.5 Pro to generate SQL queries. Then we define a function to generate SQL queries from natural language input using gemi. For that we'll write def get gemi


07:16:49
response in the bracket question, prompt. Next we'll write model equals to genai, generative model in the bracket we'll write models/jna version 1.5 pro. Then we'll write response equals to model.generate rate underscore content in the bracket and in square brackets prompt in the square bracket zero and comma question and then we'll write return response text. The function initializes the Gemini model. It takes a question and predefined prompt as input and the AI model generates an SQL query as output. Next,


07:17:30
we define a function to execute SQL queries on the database and retrieve results. For that we'll write def read_sql_query in the bracket sql comma db. Next we'll write con equals to skite 3 dot connect in the bracket db. Then cur equals to con.cursor and then cur equals to execute in the bracket sql. Then we'll write rows equals to cur do fetch call. then con do. And then con.t close and then we'll create a loop by writing for row in rows and then we'll print it and then return rows. The function


07:18:18
connects to the student db database. It executes the given SQL's query and it fetches all the retrieve records and prints them. Now we define the AI prompt that instructs Gemini on how to convert the questions into SQL queries. As you can see, I've already created a prompt for my own and you can create yours according to how you want your model to function. If you want the prompt which I've used over here, you can just comment on the video and I'll send it to you. This prompts ensures the Gemini AI


07:18:47
generates SQL queries accurately without unnecessary text. Now we'll set page configuration with a title and icon. For that we'll write st set_page configuration in the bracket page title equals to SQL query generator edurea comma page icon. Then we'll display the edureka logo and header. For that we'll write st dot image in the bracket 123.png png comma width equals to let's keep it as 200 st dom markdown in the bracket logo plus ederica's gemini app/ your AI powered SQL assistant


07:19:33
next we'll write next we'll write st.mmarkdown then the logo and ask any questions and I'll generate the SQL query for you the page title and the icon are set a logo is displayed at the top and the app's purpose is to introduce to the user and before we import the logo just make sure that you have the logo in your folder we take user input for a natural language query for that we'll write question equals to st.ext_input text_input in the bracket enter your query in plain English colon comma key equals to input.


07:20:12
This allows users to type their questions such as show all students with marks above 80. A submit button triggers the SQL generation process and for that we'll write submit equals to ST dot button in the bracket generate SQL query. When clicked the app processes the query and retrieves the result. Now we define what happens when the submit button is clicked. For that we'll write if submit in the next line response equals to get gemini response in the bracket question, prompt. This is to convert the question


07:20:50
to SQL and then we'll print the response. Then we'll write response equals to read_sql_query in the bracket response, student db and this is to execute SQL on the database. Then we'll write st.s subheader in the bracket the responses brackets closed. Next we'll include a loop for then row in response. Then we'll write st dot subheader in the bracket the responses and then we'll include a loop for row in response. Then we'll print row and then st do header and in the brackets row.


07:21:35
The user's question is converted into an SQL query using Gemini AI. The SQL query is executed on the student DB database and the retrieve records are displayed on the streamllet app. And that's it. The AI powered stream app allows users to ask natural language questions which are automatically converted into SQL queries and executed on a student database. Now let's open the terminal and run our streamlet app. To do this we simply type streamllet run app. py and hit enter. It's running and as you can


07:22:10
see our agentic AI is up and running ready to interact with our database. Let's test it by asking a simple question. We'll ask, "Give me the names of all the students." The AI processes our request, converts it into an SQL query, and retrieves the student names from the database. Perfect. As you can see, the response is generated. Now, let's try another query. We'll say, "Give me the average of marks." And just like that, the AI calculates and returns the average marks. The


07:22:43
response which is provided is 72.2. two. So in this video we successfully built an agentic AI that can understand natural language, generate SQL queries and interact with our data seamlessly. [Music] What skills do you need to succeed in generating AI? Here's a breakdown of the key technical skills that are essential. First, we have programming skills. Mastery of programming languages like Python is a must. Python is particularly popular in AI due to its versatile and the availability of libraries like


07:23:20
TensorFlow and PyTorch. Next, machine learning expertise. You should have a strong understanding of machine learning concepts and algorithms. Familiarity with different machine learning frameworks is also beneficial. Next, data handling skills. Comfort in working with large data sets is crucial. This includes data cleaning, pre-processing, and visualization. Next, deep learning knowledge. Understanding neural networks, convolutional networks, and recurrent networks is vital. As generative AI heavily relies on these


07:23:54
concepts, next mathematics, and statistics, a solid foundation in linear algebra, calculus, and statistics will help you understand how algorithms work and optimize them effectively. Finally, creativity and critical thinking. Don't forget the importance of creativity. Being able to think outside the box will enable you to come up with innovative solution in generative AI. By building these skills, you will not only prepare for technical challenges but also become a valuable asset in the job market. Now,


07:24:28
if you eager to jump into generative AI, here's a road map based on what I have learned along the way. First, start with the basics. Let's kick things off by building a strong foundation in AI. I recommend Edureka's AI certification course as a great starting point. In this course, you will get hands-on with core tools like Python and data handling, plus set up basic AI tools like the OS module and NLTK for natural language processing. These essentials gives you a solid base for a more


07:25:00
advanced AI skills. Next, get certified. certifications really help you stand out and make sure you're covering industry relevant topics. A few key ones to consider are the responsible AI certification where you will dive into the ethical AI and tools for fairness like IBM's AI fairness 360. Next, the prompt engineering course. If you're interested in making AI charts more intuitive with optimized prompts using Open AI's APIs, these certifications build your skills while highlighting


07:25:34
your knowledge to potential employers. Next, gain hands-on experience. Next up is a real world practice which is a must. Edureka's mastering generative AI tools course is a great option here. You will learn popular tools like TensorFlow and PyTorch and work on projects like image synthesis and text generation. This gives you the confidence to apply AI in real world scenarios. Networking is super valuable in AI. It opens doors and keeps you updated. Through Edureka's Chant GPT training course, you get more


07:26:07
than just skills. You connect with other AI learners and share insights. Plus, you will work with tools like open AI API and learn to build customer support bots which is great asset. Next, specialize yourself. To stand out, consider specializing in a specific area. Here are some courses to consider. First, AI for software developers, which is focused on using GitHub copilot and deploying models with AWS. Next, AI in retail. This one covers personalized recommendations and other retail focused AI solutions using TensorFlow


07:26:45
recommenders and Google's Vortex AI. Next, GitHub Copilot training. Learn how to use this AI powered coding assistant for enhanced productivity. Now let us move on to the job opportunities in generative AI. As you prepare, consider these various roles you can pursue in generative AI. First, we have AI research scientist specializes in advancing AI technology by developing new methodologies and exploring cuttingedge innovations. Next, AI product manager manages the development of AIdriven products, ensuring they meet


07:27:20
user needs and align with business objectives. Next, NLP engineer focuses on natural language processing, enhancing human computer interaction through language based technologies. Next, AI solutions architect designs AI based solution tailored to address specific organizational goals and challenges. Next, AI ethics researcher investigates the ethical implications of AI technologies, ensuring responsible and fair AI development. Next, AI consultant provides expert guidance to businesses on effectively implementing


07:27:57
and utilizing AI strategies for growth. Now, let's talk about a question that's on many people's minds. Is generative AI taking over jobs or is it just reshaping them? The answer to the question is no. It's not eating our jobs. Instead, generative AI is reshaping jobs by automating repetitive task in areas like customer service, design, and content creation. While it handles routine work, jobs that require creativity, emotional intelligence, and complex decision making remains secure as AI struggles


07:28:31
with all these complex task. Generative AI boosts productivity as a partner, helping us focus on strategy and creativity. [Music] Let's learn what Nvidia is all about. Nvidia is a multinational technology company that specializes in graphics processing units and other products for gaming, automotive, data center, and professional visualization markets. Nvidia's role as a leader in AI and graphics technology features its iconic headquarters and cuttingedge hardware products like the RTX and series GPUs.


07:29:12
Nvidia's innovations are transforming industries from gaming to AI, making it a key player in the tech advancement. Their commitment to powerful processing and AI solutions is reflected in both their infrastructure and product lineup. Well, Jenzi started their journey with Nvidia's gaming experience by powering popular titles with advanced graphics and AI technology. As you see, Nvidia's GPUs are often used to optimize these games, enhancing visual quality and performance, which creates a seamless


07:29:45
and stunning visual experience for players. For Gen Z, who played games and used PCs containing Nvidia elements, the company is like a friend to them. So let's see what the news is all about Nvidia. Nvidia recently announced several key advancements including the launch of its large language model cloud services to accelerate AI applications across industries. The Nemo LLM service empowers developers to customize large models while the bio service is specifically designed for biological and


07:30:17
genetic predictions. Additionally, a new Nemo framework has been introduced to improve training efficiency for large language models and the powerful H200 supercomputer is set to enhance LLM training capabilities, further positioning Nvidia as a leader in AI innovation. These announcements position Nvidia as a top innovator in AI infrastructure. So now let's explore one of their global collaborations. 2025 predictions AI finds a reason to tap industry data leaks. Every industry, every company, every country must


07:30:52
produce a new industrial revolution, says Nvidia CEO Jensen Hong at the AI summit in Japan. Now, let's look at what Nvidia exactly achieved. Nvidia's next generation AI hardware Hopper and Blackwell offers unmatching performance enabling faster training of complex models. Now let's look closer at what makes Hopper a gamecher in AI processing. The Hopper chip built with 80 billion transistors and manufactured with the TSMC's 4Nm process delivers unprecedented processing power and high


07:31:29
clock speeds. It is specifically designed to accelerate large language models and generative AI. And the Blackwell's chip containing 104 billion transistors and a peak performance of 1.4 4 extra flops is equipped with 30TB of high bandwidth memory making it ideal for handling large AI workloads. Moving forward, let's discuss Nvidia's software innovations in large language models. Nvidia has launched Nemo LLM, a framework that supports diverse NLP task and BNIMO LLM, which specializes in


07:32:05
biological and genetic research. Together they bring AI's power to life science. Now let's discuss the first model that is Nemo LLM. Nvidia's Nemo LLM powered by the RTLLM engine is a robust 12 billion parameter model with a 128,000 token context window enabling optimized performance for diverse NLP task. Transitioning to life science, Biono is transforming the field with its unique capabilities. Nvidia offers a diverse range of AI models on its website likely under Nvidia. You can explore these models and their


07:32:45
capabilities. Now let's take Nemo AI demo as an example. On the Nemo AI website, you will find information about how to interact with the model through APIs, potentially using open AI standards and a temperature setting to control its creativity. You can even input code like see out hello world in Python and the model will translate it into different programming languages showcasing its capabilities. The second model is Biono LLM for biological research. It is trained on molecular protein and DNA data. Bioneo


07:33:25
LLM accelerates drug discovery and advances genetic research driving breakthroughs in biology. Now let's see the Bionmo impact on science. Biono is a breakthrough tool tailored for biological data revealing hidden patterns that could revolutionize drug discovery. Now let's move on to the Biono's research applications. Biono identifies complex biological relationships, transforming genetics research and advancing research through collaborative human AI insights. Bionimo analyzes protein and DNA sequences to


07:34:00
fuel new scientific insights enabling researchers to uncover critical discoveries faster. So following the above news, let's see what happened in AI summit in India. At the Nvidia's AI summit India, CEO Jensen Hong shared his vision for AI's potential in India. He highlighted the country's promising future in AI innovation supported by Nvidia's technology. He stated the world's digital future will be written in AI and India has a lead role. Jensen Hong and MKkesh Amani discussed India's


07:34:33
potential as a global AI hub. Hong envisions AI enabled super employees driving future advancements. He stated AI will amplify human capabilities and create super employees of the future. Nvidia's global reach connects innovators and leaders worldwide. With 306.995 trillion metrics of computational power, Nvidia leads the AI revolution. Nvidia's ongoing innovations in AI hardware and software are reshaping industries and pushing the limits of what's possible. And with this, we have come to the end of this video on


07:35:10
Nvidia's breakthrough in AI chips. I hope you enjoyed about Nvidia and its new advancements through this video. [Music] Did a Chinese AI model just shake up the entire US market? Let me tell you what happened. On January 27, 2025, the stock market took a serious hit. Tech stocks drop hard and the biggest shock was that the Nvidia a prominent player in the AI hardware sector saw its stock crash by 17%. And that's a 590 billion loss in a market value and it was because of an AI model called Deepseek R1. Yes, you heard


07:35:52
that right. A Chinese AI model just sent shock waves through the industry, raising big concerns about China's growing AI dominance and what it means for companies like Open AI, Google, and even Nvidia. So why is DeepSync R1 such a big deal? Well, it's not just another AI model, it's a gamecher and with models like DeepSc R1, DeepSc V2, and DeepScoder. and it's going head-to-head with the top players like Open AI and Google offering powerful AI at the fraction of the cost. Here I have added


07:36:24
a screenshot for your reference and this table compares the large language models based on their accuracy and calibration error. And among the models mentioned, Deepseek R1 has the highest accuracy with 9.4%. Or performing 01 with 9.1%. Gemini thinking with 6.2% 2% and other models such as GPT40 with 3.3% and group 2 with 3.8%. Furthermore, DeepSync R1 has the lowest calibration error with 81.8%. indicating improved confidence calibration over other models with errors greater than 88%. This demonstrates that DeepSc R1 not only


07:37:06
produces the most accurate results but also has a higher forecast reliability. And this benchmark graph will show the DeepSc R1's expectational performance across a variety of evaluation task solidifying its position as a top tire LLM. Notably, DeepSc R1 achieves the highest scores in AIM 2024 with 79.8%. Code forces with 96.3%, math 500 with 97.3% and MMLU with 90.8%. indicating superior reasoning, problem solving and coding skills when compared to OpenAI's 01 models and DeepSc V3 DeepSc R1 consistently outperforms or


07:37:50
equals top models particularly in domains requiring precise logical reasoning and mathematical skills. Furthermore, its highbench verified score is 49.2% demonstrates its suitability for software engineering applications. And this finding supports DeepS R1's advancements in AI research, establishing it as a formidable competitor in the LLM space. First, let's talk about DeepSc R10 and its successor, DeepSc R1. So, let's break it down. In reinforcement learning, there are two main components, the agent and


07:38:23
the environment. The agent interacts with the environment and based on its actions, it receives rewards or penalties. The goal of the agent is to maximize these rewards by learning from its mistakes and improving over time. Now let's talk about the Deepseek R10. This model was a pioneering attempt to use reinforcement learning without supervised fine-tuning. And the idea was to let the model learn entirely through interaction with its environment without any pre-labelled data. However, this


07:38:54
approach had some challenges. Deepsec R10 faced two major issues. First is the poor readability. The models outputs were often hard to understand and next the language mixing with Chinese. The model sometimes mixed languages especially Chinese which affected its performance in English task. And to address these issues the team introduced DeepSeek R1. This new model not only solved the problems of readability and language mixing but also achieved remarkable performance. In fact, DeepSync R1 matched the accuracy of


07:39:27
OpenAI's GPD01 model, especially the OpenAI 01217 model on reasoning task. That's a huge milestone. But that's not all. Deepsec R1 is also 24 to 28 times cheaper to train compared to other state-of-the-art models. And this makes it not only highly effective, but also cost efficient, opening up new possibilities for research and applications. So to recap here we have the DeepSc R10 was an ambitious attempt to reinforcement learning without supervised finetuning but it faced challenges with readability and language


07:40:02
mixing. Deepsec R1 address these issues achieving top tier accuracy and being significantly most cost effective. Now as far as GPD is concerned, CH GP combines unsupervised learning, supervised fine-tuning and RLHF making it more aligned for text based reasoning and safe AI interactions. Now we will look at the model comparison. Deep Seek and GPT are both pushing the boundaries of what AI can do, but they take a very different approaches. So let's break it down. We asked the GPD01 model and Deepseek R1


07:40:38
model to generate a Python code where a ball bounces inside a rotating triangle. Sounds cool, right? Well, let's check out the result. First up, here's what GPD01 came up with. It works, but the physics seems a bit off and the moment isn't as smooth as you would expect. Not bad, but it's not quite there yet. Now, let's look at what DeepSync R1 generated. Wow, this one looks way better, right? The ball's movement feels more natural and the rotation of the triangle is much


07:41:11
smoother. The overall gameplay experience is just more polished. So, if we compare the two, Deepsec R1 definitely outperform GPD01 in this challenge. Of course, both models are impressive in their own ways, but when it comes to designing this specific game, DeepSc R1 takes the win. Now, we will distinguish the differences in detail. So first let's talk about the architecture. Deepseek uses a mixture of experts design. Think of it like a team of specialists. Only the relevant experts are activated for each task. So


07:41:44
for example, DeepSc V3 has 671 billion parameters but only 37 billion are activated per token making it super efficient. And on the other hand, GPD model uses a dense transformer architecture where all the parameters are active at once. GPD4 for instance has 175 billion parameters all working simultaneously and this makes GPD powerful but also computationally expensive. Now let's talk about cost and efficiency. Deepseek is a gamecher here. It was developed on the budget of just $5.5 million due to its efficient design


07:42:23
and that's a fraction of what other models cost. GPD models like GPD4 requires massive computational resources. Training GPD4 cost over hundred billion dollars making it a heavyweight in terms of both performance and expense. And when it comes to performance both models shine in different areas. Deepsec is a powerhouse in tasks like coding translation and solving complex math problems. And in fact deepseek R1 has been shown to match the performance of advanced systems from open AI and Google despite its smaller


07:42:56
budget. GPT models like GPD4 are known for their natural language understanding, creative writing and complex reasoning and they are incredibly versatile and can handle a wide range of task with ease. Next, accessibility is another key difference. Deepseek is an open-source meaning its code is available to the public and this promotes transparency, collaboration and innovation within the AI community. GPD models on the other hand are primarily proprietary. While open AI has released some tools and models, many advanced


07:43:28
versions are restricted and accessed through APIs. Finally, let's talk about the ethics and censorship. Deepsec implements strict content moderation, especially for politically sensitive topics. This ensures compliance with regulatory standards, but can sometimes limit its responses. GPD models also have moderation mechanism to prevent harmful outputs. But they strive to balance open access to information with ethical guidelines. Now we will install the DeepSync R1 and run a short demo on it. So let's see how the DeepSync R1


07:44:01
model can be installed. First let's open up our browser and head over to the ola.com. And once you're there you will see a download button. So go ahead and click on that. Now select download for Windows to start the download. And keep in mind it's a pretty big file. So it might take a minute to download. So let's give it some time. Once the download finishes, go to the download folder and find the installation file. Now here, double click on the file to open the installation window. And you will see an


07:44:34
install button. So simply click on that and Ola will start installing on your system. So once Ola is installed, let's head back to the Ola website. Now click on the models tab at the top of the page and here you will see a list of available models. And for this video we are going to use the DeepSync R1 model. So we will select the 1.5B model but if you want you can also choose the latest 7B model too. Now once you have selected the model you will find the installation command here. So go ahead and copy that


07:45:09
command. Now open your terminal on your Windows system and paste the command we copied earlier. So this is the command and this command will start pulling the model. So depending on your internet speed, this might take a little time. So be patient and that's it. Once the process is done, your DeepSync R1 model will be all set up and ready to use on your system. All right. Now let's try out some commands here. So let's say hello. Okay, we got some response here. Now let's ask it to tell something about


07:45:44
himself. All right, so it responded saying that I am DeepSync R1 and AI assistant. Next, let's ask it to design a Python code to list all the files in a directory. So as you can see, it has provided up the code we asked for. Great, right? So, what does all of this means for the future of AI? Well, AI is becoming more accessible. Like, for years, AI development was mostly controlled by big companies with massive budgets. But now, with open-source models like Deepseek R1, anyone whether you have a small


07:46:25
startup or you are an independent developer or a student, you can build AI solutions without paying huge API fees. This means faster adaption and innovation worldwide. Next, the global AI race is heating up. The AI race between the United States and China is getting even more competitive. While the United States tries to limit AI chip exports, China is finding ways to keep up. No matter which side you support, one thing is clear, AI is evolving rapidly, and staying informed is more important than ever. Next, AI is


07:46:59
becoming more sustainable. Training AI models consume massive amounts of energy. But with advancements in optimization, we are seeing a shift towards more efficient and eco-friendly AI. This means lower CO2 emissions and a reduced environmental impact. Something that was once a major concern in AI development. Next, career opportunities are growing. And if you're a developer, AI engineer, or a data scientist, this is your moment. Companies will need skilled professionals to build and deploy AI solution at a faster pace than


07:47:32
ever. So if you have been thinking about getting into AI, now is the time to start. Well, DeepSync is making big moves, but can it really compete with open AI in long run? Which one do you think will dominate the future of AI? So let me know your thoughts in the comments below. [Music] Did Alibaba just do the impossible? Their latest AI model has outperformed both GP4 and Deep Seek in some key benchmarks. But how did they manage to do it and what does this mean for the AI race? Stick around as we dive into the


07:48:10
shocking details behind this breakthrough and what it means for the future of AI. Alibaba just dropped a bombshell during the Luna New Year, a new AI model called Quinn 2.5 Max, and they say it outperforms OpenAI's GPT40, Meta's Llama, and even China's own rising star, DeepSync. Is this the new benchmark in AI? Let's break it down. First off, what exactly is Quen 2.5 Max? Developed by Alibaba Cloud, this model is being hyped as a major rival to GPD40. According to their benchmarks, it


07:48:43
crashes competitors in reasoning, coding, and multilingual task. So, let's look at the numbers. In Arena Hard, a benchmark for complex problem solving, Quinn 2.5 max scored 85.3%. Beating GPT4's 80.2% and Deep 6 V3 is 77.5%. But here's the twist. It's not just about the raw power. Alibaba built this model for businesses. Think customer service bots that speak 10 languages or AI coders that debug Python faster than your engineering team. And unlike OpenAI's premium pricing, Alibaba's


07:49:19
offering Quinn 2.5 Max at a fraction of the cost. But why drop this during the Lunar New Year when half of the China's on vacation? Well, that's where the discussion begins. Meet DeepSc, the 20month-old startup that's being shaking up Silicon Valley. Three weeks ago, they dropped DeepSync V3 and the R1 model. And the secret is insanely low cost. We are talking 0.14 million per tokens. That's like charging pennis for a Lamborghini. Deep cheap open-source models triggered an AI price war in


07:49:52
China. Alibaba reduced price by up to 97% overnight. But Deep Six founder Young isn't sweating it. In a raid interview, he said, "We don't care about the price. AGI is our goal. AGI that's artificial general intelligence. AI that can overthink humans. And here's the twist. Deepseek isn't some corporate giants. They are a tiny team of a grand students and researchers working out of Alibaba's hometown, Hongu. Meanwhile, Alibaba's got 200,000 employees. So, how does Quen 2.5 Max actually stack up?


07:50:27
Let's compare. First, let's talk about reasoning. Quen takes the lead here. Next, when it comes to coding, Quinn continues to shine. And if multilingual support is what you are after, Quinn speaks Mandarin, English, Spanish, or whatever you name it. But Deepsec3 still holds the crown for affordability. And JP4, it's holding on to its reputation. But even open AI Sam Oldman admitted deep progress is impressive. But Alibaba's timing is strategic. Releasing Quinn 2.5 Max during Lunar New Year when


07:51:01
everyone's distracted is a power move. It's like dropping a diss track on the Christmas day. No one's looking, but everyone will hear it. So, here's why this matters. Deepix R1 model wiped $1 trillion of US tech stocks in a day. Nvidia, Meta, Microsoft all dropped. Why? Because if a tiny Chinese startup can match GPT4 at 100 the cost, inventors wonder, are we overspending on AI and China's giant aren't sitting still. Bite Dance updated its AI model days after Deep Six launch. 10 cent and


07:51:37
BU are in the price cutting frenzy. Meanwhile, Alibaba is betting big on Quinn to dominate enterprise AI. Think hospitals, banks, and mega corporations. And the real question is who's closer to AGI? Deepseek's agile team or Alibaba's corporate powerhouse? Share your thoughts in the comments. So, is Quinn 2.5 Max the new AI champion? Maybe. But this isn't just about benchmarks. It's a glimpse into the future. A future where AI isn't just built by Silicon Valley giants, but by startups in Honguk and


07:52:11
opensource communities worldwide. [Music] A game-changing development has taken the tech world by surprise. I think we should take the development out of China very very seriously. A free open-source AI model emerged seamlessly out of nowhere. It not only matched but surpassed some of the most advanced systems on the market. What made this even more remarkable was its origin. It wasn't a new release from OpenAI nor a breakthrough from Anthropic. It was a deep sick an AI model developed in China


07:52:47
and its development left top AI researchers in the United States in amazement especially when they learned about the staggering cost behind it. It's opened a lot of eyes of like what is actually happening in AI in China. The training cost for DeepSeek version 3 was just $5.576 million and in comparison, OpenAI spends a massive $5 billion annually. While Google's capital expenditures are projected to exceed $50 billion by 2024, Microsoft on the other hand invested over $13 billion just in OpenAI. And


07:53:24
yet, Deepseek's model outperformed these highly funded AI models from leading American companies. And the contrast is truly mind-blowing. to see the deepseek um um new model. It's it's super impressive in terms of both how they have really effectively done an open source model that does what uh is this inference time compute and it's super compute efficient computer deep didn't stop at the success of its powerful open-source AI model. Instead, it quickly introduced R1, a next generation


07:53:54
reasoning model that has already surpassed the advanced OpenAI W1 model in several third party benchmarks. This rapid innovation highlights Deep Seek's ability to surpass even the most well-funded United States AI giants, proving that agility and creativity can disrupt the established leaders in the race of the AI dominance. As we dive deeper into this, let's hear from Martin Wishop, the director of Bulgarian Institute for Computer Science, Artificial Intelligence, and Technology. He recently made some interesting


07:54:25
statements about the AI industry, and they are shaking things up. So he pointed out that a Chinese AI startup claimed to have developed its R1 LLM model with less than $6 million while other companies are pouring in billions. That announcement alone caused Nvidia stock to drop. And according to Martin, this models are built by strong researchers and engineers in the field, many of whom actively publish their work. But developing these AI models can be incredibly expensive. Just to give you an idea, running 2048 H800 GPUs


07:54:59
could cost anywhere between $50 to hund00 million. And he also mentioned that the company handling the data center is backed by a massive Chinese investment fund with far more GPUs than just those 2048 H800 units. As for the architecture behind Deep6 R1 and V3 models, Martin explained that they use a mixture of experts ME approach. Simply put, this means that at any given time only a small percentage of the model is active, making it much more efficient in real-time use. This rises a lot of question about the cost efficiency, AI


07:55:35
development strategies and how companies are competing in this space. Now the question is if deep 6 development is being reported to have cost only5 to6 million how does this figure align with the extensive infrastructure data center operations and substantial backing from Chinese investment funds. Could there be more to the story that isn't being disclosed? Let us know in the comment section below. As far as the research indicates, DeepSc V3 has been utilized as the base model for DeepSc R1 and this


07:56:07
progression highlights DeepSc strategic approach to building on its existing architecture while pushing the boundaries of AI capabilities. Deepsec R1 distinguishes itself by relying entirely on reinforcement learning fine-tuning, a focused and efficient method that constructs sharply with OpenAI's GPT infrastructure. Open AI's GPT generative pre-trained transformer framework employs a combination of supervised learning, unsupervised learning and reinforcement learning to train its models. While this multiaceted


07:56:38
approach has proven effective, it also requires significant computational resources and time. In contrast, DeepSync R1's exclusive use of reinforcement learning fine-tuning demonstrates a more streamlined and targeted methodology which not only reduces cost but also enhances performance in specific task. This differences in training strategies highlights Deepseek's remarkable ability to innovate efficiency and by building on its foundational V3 model, it developed R1, a reasoning model that has


07:57:07
already surpassed OpenAI's advanced systems in some key benchmarks. Deeps focus on reinforcement learning has allowed it to carve out a unique position directly challenging the dominance of United States AI giants. This approach demonstrates that with strategic resource conscious innovation groundbreaking results are not only possible but they are already happening. So what do you think? Does DeepSync opensource approach give it a long-term advantage or will OpenAI's heavy investment in research and proprietary


07:57:38
models keep it ahead? Share your thoughts in the comments. Let's explore the real life rewards of generative AI. The first one is it automates and innovates the content generation. AIdriven tools create highquality text, images and videos, revolutionizing the content creation across the industry. It optimizes the product designs. AI enhances the product development by stimulating the refining designs producing cost and time to market. The next one is strengthens the cyber security efforts. AI detects and


07:58:15
analyze the threats against the cyber attacks. The next one is advances healthcare research. AI accelerates the drug discovery, medical imaging analysis and personalized treatment plans improving the patients outcome. And the last one is drives digital transformation. AI integrates with business's system to automate the workflows, enhance the decision making and optimize the operational efficiency. So now let us look at some real life example. Recently Walmart in collaboration with Microsoft announced a


07:58:47
major AIdriven initiative. Walmart has partnered with Microsoft to integrate AI powered solution into retail operations. This collaboration aims to enhance both customer experience and employee efficiency by leveraging the advanced AI tools. So what are the main benefits of AIdriven initiative? Here are some key points. The first one is personalized shopping experience. The next one is AI powered associate support. The next one is content creation automation. Next on we have improved customer service. And


07:59:18
last but not the least operational efficiency. So this partnership showcases how AI is revolutionizing retail making businesses smarter, faster and more customercentric. So now let's look at another exciting real world application of AI but this time it is the field of medicine. Scientists at Stanford had developed a tool called synthe which is helping to create new antibiotics to fight drugs. This is a huge breakthrough because antibiotic resistance is a major global health challenge. So what has this AI


07:59:50
tool achieved so far? Here are some key points. Scientists use syntheal to design antibiotics that can fight resistant bacteria. It generated 25,000 possible antibiotics candidates, giving researchers a massive head start. It provided detailed recipes for making these drugs efficiently in the lab. Out of all candidates, scientists identified six compounds that actually work against the resistant bacteria. This technology isn't just limited to antibiotics. It's now being used to explore the treatments


08:00:20
of heart disease and even fluorescent module design. So this is a gamecher in medicine. Providing that AI can do much more than just chat bots and automation, it can literally help to save a life. So now we have seen how AI is transforming the industry. But it also comes with an ethical challenges and risk. Let's take a look. One big issue is output quality. AI can produce incorrect or unreliable result. It can also generate madeup facts and hallucinations creating false information. Another concern is biased


08:00:53
output. Where AI reflects biasness from its training data, there are also copyright and other legal risk. AI generated content might violate the intellectual property laws. AI is also vulnerable to abuse, meaning it can be misused for misinformation and harmful purpose. Lastly, the cost of expertise and compute makes AI expensive and less accessible. These challenges highlight the need for responsible AI development. So now that we have seen the ethical challenges of generative AI, let's look


08:01:24
at some real life example where these risk have had significant impact. Recently a report highlighted that Deep Seeks AI app is openly sending users data from US to China. This has raised major concerns especially in the light of past controversies surrounding the data security and privacy. This incident had drawn attention to ethical and security risk. This situation brings a several key challenges. The first one is data transmission to China. User data being directly sent raises concerns about the privacy and the control. The


08:01:57
next one is lack of transparency. Users may not be fully aware of where their data is going and how it is being used. The next one is government scrutiny. Authorities may impose regulations or restrictions due to national security concerns. The next one is global backlash. countries worldwide might be stricter against AI platforms handling the sensitive data. The next one is potential exploitation of third parties. There's a risk that collected data could be misused or shared with the other


08:02:24
entities without consent. So now let's look at another major challenge data leaks and their consequences. Recently a report from business insider revealed that Amazon warned its employees not to share confidential data with charge GPT. The concern arose after some AI generated responsive were found to closely match Amazon's internal information highlighting the risk of sensitive data leaks. Such incidents bring several serious risk for example data privacy concern, intellectual property exposure, unintended data


08:02:54
retention, regulatory compliance issues and erosion of customer trust. So this case with Amazon is just one example of why companies must carefully manage the AI interaction to prevent the potential data leaks and security violations. So now let's look at another major risk of generative AI that's making headlines worldwide. That is deep fake. Deep fakes are one of the biggest risk of generative AI. AI generated fake videos, audios and images can be misused in dangerous ways. Scammers use them for


08:03:26
fraud, identity theft and misinformation leading to financial losses and manipulations. Fake videos of public figures spread false narratives. While AI generated explicit content violates the privacy. As deep fake become more advanced, they are eroding trust in media and creating serious ethical challenges. While deep fakes highlights the dangers of generative AI, it's very important for us to focus on how we can develop and use AI ethically to minimize these risk. Ethical generative AI ensures that the technology benefits the


08:03:57
society while preventing the harm. To achieve this, we must focus on key principle. The first one is transparency and accountability to ensure that the AI systems are open and responsible. Bias and fairness to prevent the discrimination. Privacy protection to safeguard the user data. Additionally, AI should prioritize avoiding harmful content and give users consent and control over how their data is used. By implementing these principle, we can build a future where generative AI is both powerful and ethical. So now that


08:04:27
we have explored the ethical aspect of generative AI, let's look ahead and discuss what the future holds for the technology. The future of generative AI is filled with exciting possibilities. We can expect wider adoption across industries transforming the field like healthcare, finance and entertainment. AI will continue to enhance creativity and content generation, making it a valuable tool for artists, writers, and businesses. With the advancement, better customization will allow AI to generate


08:04:55
more personalized experience. At the same time, human AI collaboration will grow, enabling more seamless interaction. A key focus will be on responsible AI development, ensuring stronger ethical frameworks and regulations to create the safer and more trustworthy AIdriven future. Before [Music] we dive into AI and culture, let's play a little game. I like games and I hope you do as well. So, the name of the game is guess the title of the movie. So, I'm going to show you four different movie


08:05:29
posters and your job is to guess the name of the movie. I hope that was clear, the instructions were clear. Guess the name of the movie as you see the posters. You can pause the screen and then go down to the comment section below and leave your answers there. So, are you ready? 3 2 1. So, I hope that you left your answers in the comments and I'm going to give you the titles, of course. So, there they are on the screen. The point of this game is to see how many of you recognize the movies. And I think most of my viewers will at


08:06:00
least recognize two out of the lot. Yeah. And all of these movies are based on artificial intelligence. And if you haven't watched any of these movies, I recommend that you find some time for them. They're mindblowing. The Matrix is my personal favorite from the ones you see on the screen. But why did we play this game? Well, this is the easiest way to show you guys that people get excited when they hear the term AI thrown about because it is cool and it's everywhere and there's so much to talk about.


08:06:27
People have so many different opinions and perspectives on the matter. So I think it is safe to agree that it's deeply embedded in not just the western culture but cultures all over the world. Wouldn't it be cool to look at some of the factors or elements that led to such a widespread adoption of artificial intelligence in cultures? Well, first up we're going to take a look at advancement in science. Movies like the one you see on the screen came out before World War I and they inspired a


08:06:55
whole generation of scientists who did some amazing work in the field of computer science and AI. So what were the advancements that led to AI? The first computer was made in 1946 and was called Aniac. It was enormous. It occupied a space of 50 by 30 ft but could only do simple calculations. But the benefit was that it was reprogrammable. Following which in 1950 who is regarded as Alan Turing who is regarded as the father of modern computer science wrote in his paper how to build intelligent machines and how to


08:07:30
test their intelligence. But as you know computers weren't advanced to hold any instructions or data. But Turing's test for intelligence is still used today over a half a century later. Isn't that amazing? Next in 1956 at the Dartmouth summer research project on artificial intelligence is where it all began. I mean for artificial intelligence. As a matter of fact, this is where the term artificial intelligence was first used. From 1957 to 1974, AI flourished. It became the buzzword. And also the


08:08:06
computers could store more information became faster, cheaper, and more accessible. Yet they were too slow to exhibit any kind of intelligence. Remember the first movie and the game we played earlier that came out in 1968. Since the computers were too slow and needed to catch up, the AI buzz started to quiet down. And that was until 1997 when IBM supercomputer called Deep Blue beat Gary Kasparov at chess. And mind you, Gary was regarded as the grandmaster of chess. Here's a picture of Deep Blue on the right. So, you get


08:08:42
an idea of how compact the computers got by the year 1997. So, I hope you had as much fun as I had by giving you a little bit of a history on AI. So, this was the first element advancement in science and technology that influenced the culture around AI today. What's the second one? It is books and literature on AI. Literature can range from the ones that deal in fact such as academic, scientific or research papers to the ones that are very imaginative and completely fictional like comics. Inspired by the advancement in science


08:09:15
and technology, written literature is where writers take the most complex and evolving ideas and present them in a way of stories so common folks can understand and relate to them. But the next element that we are about to see does this better than books in my opinion. movies and television shows. Movies and TV shows have a wider audience than books. And like I said earlier, the right ones inspire the future scientists and the advancements that they make inspire the next generation of movies. So it's like a


08:09:45
cycle, you see. Let's move on to the next one. Different events such as conferences, seminars, webinars, trade shows and expos, awards and competitions, workshops and others provided a platform to exchange ideas on artificial intelligence and connected like-minded people. They range from highly academic events like the one we saw earlier, dark mouth summer project on artificial intelligence to things like Comic-Con where people attend as their favorite characters and meet movie stars. Next one is mainstream media like


08:10:19
newspaper, news channels and radio across the world. They have provided coverage on artificial intelligence and brought on experts to talk about it. The next one is normal everyday conversations. Yeah, if AI is going to be everywhere, then it will be part of everyday conversations. All of these different elements and not to forget social media have made AI incredibly famous in cultures across the world. But in the past few years, the buzz around AI is back, especially with companies like Amazon, Google, Facebook, Apple,


08:10:52
Tesla, and two or three dozen research organizations making incredible progress in this field. So naturally, that will make you wonder what is AI like right now. In this section, I have another surprise for you. But before we get to it, let's check out what are the different types of artificial intelligence. So there are three main categories of artificial intelligence. A NI, AI and ASI. So obviously the first thing that popped in your mind if you don't know about these categories is what are


08:11:25
they? Well, let's briefly take a look. Artificial narrow intelligence is what ANI is. It is also known as weak intelligence. Artificial narrow intelligence refers to AI systems that can only perform a specific task on their own using humanlike capabilities. They can learn from past experiences in regards to that specific task. Even the most complex AI that uses machine learning and deep learning to teach itself falls under AI. This type of artificial intelligence represents all existing AI, including even the most


08:12:01
complicated and capable AI that has ever been created to this date. Let me give you some examples. Google, Alexa, and Siri voice assistants use AI to detect speech and carry out commands. Today's security and surveillance systems uses facial recognition, which is a type of narrow AI. Social media platforms use it to learn about preferences and show you ads and content that you will enjoy. E-commerce websites like Amazon use it to learn about your shopping activities, where you are located, and so much more


08:12:38
to recommend similar products. It also helps them figure out inventory for warehouses for different locations and their unbelievable 2-day delivery. banking and financial sector use it for fraud activity detection, loan approval and so on. Last but certainly not the least, autonomous vehicle use it to navigate the roads on their own. So I hope that you got a little bit of an idea of what AI is. Let's move on to the second category which is AGI and it stands for artificial general intelligence and it's also known as


08:13:13
strong AI. So you're probably wondering what it is. First we talked about artificial narrow intelligence. Now we're talking about artificial general intelligence. Does it give you any idea? If you're thinking that this type of artificial intelligence is good at general tasks, meaning all tasks instead of a specific task like we saw in the previous category A and I, you would be right. So AGI will be able to better understand the humans it is interacting with by discerning their needs,


08:13:45
emotions, beliefs and thought processes. It will be able to learn things and apply a broad range of areas just like human beings can. And unlike narrow artificial intelligence, right now AGI is the goal of the field of AI, a place where AI will become part of the physical world and will navigate it like we do. Quite a bit of leading AI researchers think we'll get there in a few decades. There's been a lot of research and development happening on AGI by organizations like Open AI, Deep Mind, Apprent, and many more. So, I hope


08:14:21
you got an idea about this one. Let's move on to the last type which is ASI and it stands for artificial super intelligence. Judging by the name super, you probably already got an idea that this will be the pinnacle of AI. ASI is where machines will become self-aware like us humans and they will be overwhelmingly superior than humans at everything. That is they will have greater memory, faster data processing and analysis and decision making capabilities. The potential of having such powerful machines at our disposal


08:14:57
seems appealing. But these machines may also threaten our existence or at the very least our way of life. We don't have any examples of AI. Thank God for that because we aren't even ready for artificial general intelligence, the one before this one. Okay, let's switch gears and play a little game because the section after this is going to be a little tense. So, let's disperse some tension before we get into dangers of AI. The name of the game is guess the type of AI. AI which is artificial


08:15:30
narrow intelligence good at specific tasks. AGI is more like human capabilities but it is not self-aware. And ASI which is artificial super intelligence which is where the machines become self-aware and get way better than human beings. So I will put up a few pictures or videos and you will have to guess the type of AI. And if you want, you can comment your answers below. Let's see how many of you get them right. Sounds good. Okay, let's begin. First one here is an autonomous vehicle that can drive from point A to


08:16:05
point B and even park itself without a human in it. If you guessed A and I, you'd be right. Second one is a Tesla bot. The robot will be able to perform basic repetitive tasks with the aim of eliminating the need for people to handle dangerous or boring work like getting groceries from Walmart. If you guessed AGI, you wouldn't be completely wrong, but its capabilities will be limited. So, it's A and I. These last ones are from Boston Dynamics. They've been making incredible strides in robots


08:16:39
navigating the world. And personally, the robots give me the goosebumps. I mean, look at them move. But remember, even though they can navigate the world and obstacles around them, they are not able to understand the humans. So, take that as a hint if you will. So, any guesses? If you guessed AGI, again, you'd be wrong. The point of this game was to show you that everything that we have today is artificial narrow intelligence. All three examples that we saw were artificial narrow intelligence.


08:17:13
But there are a lot of companies that are working on artificial general intelligence and it's truly it is around the corner. So I wanted you guys to watch these two clips before we move on to the next section. Freaking goosebumps, right? I mean imagine one day you go out for a stroll or a jog and you just see this thing just running loose. This is something from your worst nightmare. Robots taking over the world. That must make you wonder what are the dangers of AI, doesn't it? Well, then


08:17:42
let's talk about its dangers in this next section. So, what are the near to midterm dangers of AI? And when I say near to midterm, I mean from present day to 20 years in the future. Well, the first one we're going to take a look at is privacy. Imbalances of access to information has been exploited in the recent past and is probably being exploited as you watch this video. Let's see some of the examples to understand how everyone's privacy is always at stake. In 2018, news broke out that a


08:18:09
data analytics firm Cambridge Analytica had analyzed the psychological and social behavior of users through Facebook's likes and targeted them with ad campaigns for the 2016 US presidential election. Now imagine being able to influence US presidential election. That is crazy. Second example is Clear View face recognition. Clear View is a company that created a face recognition system to help police officers identify criminals. They claimed that it only used publicly available images on social media


08:18:41
platforms like Facebook, Instagram to identify criminals. But in January 2020, the New York Times reported that in a demo from Clear View, it scraped personal images from Instagram account of the show's producer. Next example is deep fake. And this is really concerning. Images and videos that are created using deep learning and contain a real person acting or saying things that they didn't do or say are called deep fakes. If you use it for entertainment purposes, deep fakes are fun, but people are creating deep fakes


08:19:11
for fake news and information and worse, deep fake porn. And the last example is mass surveillance in China. China uses over 200 million surveillance cameras and facial recognition to keep constant watch on their people and also mind their behavioral data captured on the cameras. China also implemented a social credit system to rate the trustworthiness of its citizen and give them ratings accordingly based on their surveillance. So on this system if they rate higher they get more benefits and if they rate lower well you're out of


08:19:43
luck. All of this was done without their knowledge or consent and that is what is most concerning. As you can see, people's privacy is a big concern right now and more so in the future years to come. Next one we are taking a look at is AI producing biases. Well, naturally, you're going to say, Kevin, how does AI produce biases? Well, AI and machine learning models use parameters and the data it was trained with to make yes or no decisions. There have been many examples in which the parameters don't


08:20:12
tell the full story and the labeling of training data could be done with some sort of bias. So when AI is used for serious tasks like filtering job candidates, giving out loans, accepting or rejecting insurance requests, or even for medical diagnosis, it can have a tremendous impact on somebody's life. Here's just one for example. This image shows how one of these thermometer guns gets classified as a gun when it is held by a person of dark skin and as a moninocular when it is held by a person


08:20:43
of salmon or white skin. This happened in 2015 where Google's image recognition software made this bias. Now imagine something like this being used by law enforcement in real life scenarios. I don't even want to think about that. The next one is centralization of AI. This danger deals with the fact that what if all the AI technological advancements always end up in the hands of a few people or groups. So we are already seeing this now to a certain degree. The amount of time that people spend on


08:21:15
platforms like Facebook, Instagram, Twitter, YouTube is so ridiculous and we're constantly giving data to these companies with each interaction. They all have gigantic oceans of data on their users which along with their AI enables them to keep making advancement in technology staying ahead of the curve and always on the top. But it is a very likely scenario that some of these companies will advance to extremely high-tech machines and the rest of the world wouldn't be able to keep up. Will


08:21:43
they then become all powerful like United States with atomic bombs in World War II or even worse because AI can unlock a whole bunch of threats to humanity? Of course, there's this problem of rich getting richer and also there's this issue of transparency where we don't know how much these organizations know about us and what they can do. Here's another scenario. What if one of these companies just like in the movie Iron Man where Tony Stark has his fancy high-tech AI powered suits


08:22:13
and there's people that are always trying to steal it so that they can reverse engineer and then there is bad people who want to use this technology to bring mayhem and destruction to the world. So it may not end up being that extreme but it is entirely possible that these companies may have good motive and good intentions towards the world but an entity with a bad intention can steal it and use it to cause harm to humanity. So centralization of AI in the hands of few people or group is a real threat and we


08:22:42
need to make sure that we always keep these threats in check. So the next danger is loss in jobs. It's also called AI dislocation. Use of AI in the workplace is expected to result in the elimination of large number of jobs. Though AI is expected to create and make better jobs, education and training will have a crucial role in preventing long-term unemployment. But initially it will lead to a lot of lost jobs. It is a common misbelief that AI dislocation will only hold true for labor to semi-skilled jobs. But the truth of the


08:23:17
matter is that it has already started displacing even high-skll jobs that require mast's and PhD degree. Let me give you an example. One such job is consultants for companies to help them make decisions. They are being replaced by AI and machine learning softwares. Let's now talk a little bit about the long-term dangers. And by long-term, I mean 20 to 50 years into the future. So what's the first danger? Well, the first danger that we will talk about is safety and security. AI applications that are


08:23:49
in physical contact with humans or integrated into human body could pose safety risks as they may be poorly designed and misused or hacked. Poorly regulated use of AI and weapons could lead to loss of human control over dangerous weapons. So the first example is neural link brain implants and the second one is autonomous weapon system. The second danger which is very real and it could happen and I think it is going to happen at a certain degree at least is the transformation of society. Remember the times when three four


08:24:21
generations would live under the same roof and since the silicon revolution of 20th and 21st century has broken down families into nuclear units of two to four people on an average. Today you look around and you'll see everyone with their heads down and eyes glued to their phones. Most of the people prefer to spend their time on their phone than the world around them. And if you want proof of this, you only need to look around. Go to a public space and just observe people. With the AI revolution that we


08:24:50
are in, we will see entire realities unfold in real time in augmented reality and we will be able to interact with an intelligent projection in them that behave like humans. Also we will be surrounded by robots that human beings will have relations and feelings towards. It is quite possible that on this course we might forget what it is to be a human. And that brings us to the last danger that is AI rise to power. I know a lot of people think that singularity or AI rise to power or taking over the world is not a likely


08:25:24
scenario. But super intelligent AIs with realworld traction such as an access to pervasive data centers and autonomous robots could radically alter their environment. Example, harnessing all available solar, chemical, and nuclear energy. If such AIs found uses for free energy that better further their goals than supporting human life, human survival would become unlikely. So these were some of the scenarios and dangers that we need to avoid while we go forward with the AI revolution. So let's


08:25:59
now find out in this last and final section what does the future hold for us. An important challenge is to determine who is responsible for damages caused by an AI operated device or service. In an accident involving a self-driving car, for example, should the damage be covered by the owner, the car manufacturer, the programmer, the person who trains the machines on data? It's really unclear right now. But with that being said, the future of AI is very promising. It's very bright and it


08:26:30
feels like a start of a revolution. World is also taking baby steps towards artificial general intelligence and it is going to be really really helpful for humankind. But that also means that we should tread really carefully and we should take all the dangers as very real because this isn't something that human beings could control once it gets out of hand as opposed to every other technology that we had previously. Okay. So how should we prepare ourselves for the future? Educating ourselves about AI


08:27:00
or other tech is going to be absolutely paramount if we are going to make this AI revolution of net positive effect for the whole world. So for that we need to have tough conversation and debates especially when it comes to developments in artificial general intelligence. Artificial narrow intelligence is not as concerning as what we are trying to achieve right now. And then lastly, we need to ask difficult questions. Keep the progress in check by establishing ethics and laws on AI. And if required,


08:27:30
there should be licensing and registration of every single tech that we make. So here's my closing remark. With great power comes great responsibility. Just as the world thought that the development of nuclear weapons will wipe out the entire planet. But we now know that if handled with great care, responsibility and universal cooperation, then it could not only lead to world peace but resolve energy crisis of the world with nuclear power. [Music] The first artificial intelligence project is chatbot. Now chatbot is an AI


08:28:08
software that can start a conversation or a chat with a user through messaging application, websites, mobile apps or even through calls. Chatbots are increasingly becoming popular. Many companies websites use chatbots to communicate with the customers. It's been used in almost all the fields be it education, medical, IT and even in banking websites. Now they're using chatbots. For example, EVA by HDFC Bank. Now if you're a beginner then you can program a simple version of a chatbot.


08:28:35
There are many chatbot available online. Just learn from them, identify the basic structure and then build your own chatbot using that structure. You can then enhance it using your creativity and make it better. So this was the first AI project. The next AI project idea is music recommendation app. Now due to AI, music recommendation app which can also be known as music recommended engine makes it quicker and easier to show the music recommendation that are tailored to each user's interest and preferences. Now how does


08:29:02
this work? So first it basically collects all the datas which is what the songs the user listens to the most what is the genre of the song which language that the user listens to and so on. Next it stores all this data and then analyzes it. It then recommends songs from the similar genres and the same language and the songs which have high ratings. You would have seen this in apps like Spotify or Wing where they have the entire section of songs recommendation for you. So using artificial intelligence, online


08:29:29
searching is improving as well since it make recommendation related to the user's visual preferences rather than a project description. We can program this music recommendation app by learning from some online blogs or watching some YouTube videos. The next project idea is stock prediction. Now many people invest in stocks and they need a stock predictor in order for them to know when to buy the stock. Now although it is impossible to predict the future, we can make an estimation or guesses and an


08:29:55
informed forecast based on the data we have in the present and the past regarding the stocks. This is known as technical analysis which is used to predict the stock's price direction. Will it increase or decrease after a particular time? So for your project, you can create an application that analyzes the trend of the stock market and offers datadriven insights. You can start off by keeping your stock prediction cycle small and then go on and try for higher values and insights. Also, if you design a good stock


08:30:21
prediction application, there'll be a great value and demand for such system and it will make your career. Now, moving on to our fourth AI project idea, which is social media suggestions. Now, artificial intelligence has been used in all popular social media networks that we use on a day-to-day basis. Like for example, Facebook uses AI and advanced machine learning to serve you all the content based on your preferences and to recognize people faces and photos. So you can tag them basically and also


08:30:48
target users for the right advertisement. Also, Instagram which is owned by Facebook uses artificial intelligence to identify visuals. Next, LinkedIn uses artificial intelligence to offer job recommendation based on your qualification and interest. It also suggests people should connect with. This also happens in Facebook. So these were just some of the example of how social media uses artificial intelligence. Now AI powered research platform analyze a variety of social media analytics to understand which


08:31:14
accounts can provide the most engagement, reach and influence for a specific industry. So for your project you can do any of the following task like suggest users to connect with people they might know or suggest them some content they might like to watch or suggest some product they might be interested in and so on. So this was the social media suggestion project. So now let us move on to our next project idea which is to identify inappropriate language and hate speech. Now this project sounds easy but it is quite hard


08:31:42
to identify all the hate speeches and inappropriate language. There are many companies like Facebook, Twitter and YouTube who are trying to create a system like this. So for your project you can use detection techniques which identify the characters in a context and then compares it to the content that has already been removed as hate speech. Now usually this would be used for identifying any hate speech in any post like Facebook or Twitter post. So design an artificial intelligence system that


08:32:08
looks into things like the text in a post, the reaction comment to the post and how closely it matches the common phrases of a hate speech. Also if it contains at least one appropriate word then identify those words and report them. So this could be one of your AI project. Now let us move on to our next AI project which is lane line detection. Now many of you know that self-driving cars are gaining a lot of popularity. Now as a beginner it would be very hard to design this but you can design a part


08:32:33
of it which is lane line detection while driving. This lane line detection technique is used by many self-driving autonomous vehicles as well as line following robots. So you can use computer visual techniques and AI to teach the vehicle to go in a particular lane. You can use computer vision techniques such as color thresholding to detect the lane. So usually the lanes are colored in white color and usually there are double lanes in the middle of the road which separates the direction the vehicle runs in. Then there is


08:32:58
usually one white line at the end of the road after which is the edge of the road. Using all this data you can design an AI powered system that detects the lane lines. Now let us move on to our next project idea which is monitoring crop health. Artificial intelligence has been increasingly adopted as a part of agriculture industry evolution. Using AI, you can perform predictive analysis to determine what is the right date for sewing the seed to obtain maximum yield after the previous harvest. You can also


08:33:24
get insights on the crop health, soil health, the fertilizer recommendation and also the next 7 days weather forecast. So you can create a project which uses artificial intelligence to monitor the health of the crop and check for disease by using various images of the plant that has the same disease. So when a user collects the image of a plant, it will be matched with the images that has already been stored and then diagnosed the particular disease and then even maybe provide a intelligent spraying technique and


08:33:48
treatment automatically. Our next project idea is using AI for medical diagnosis. AI has been used in medical industry for analyzing risk, identifying hotspot and chronic diseases and accounting for social determinance of health. So for your project, you can use artificial intelligence to develop a software that can be programmed to accurately spot signs of a certain diseases in medical images such as MRI scans or X-rays and CT scans. For example, you can design a system that uses artificial intelligence for cancer


08:34:19
diagnosis by processing photos of skin lessens. This project can be very helpful to diagnose patient more accurately and also prescribe the most suitable treatment. The next project idea is AI powered search engine. You can design a search engine which is powered by artificial intelligence which will scan billions of content available on the web and match the exact search sentence or keyword and will show the relevant information, images, videos, text and other documents. We can also use ranking algorithms that will rank


08:34:48
the content for a particular keyword based on various factors like engagement rate. that is for how long that the user spends on that website is the content from a reliable website and so many factors. To do this project you can refer some online blogs or watch some videos to get started. Also for this project you need to know a little bit about networks and how the data passes on the internet from one place to another. So this was about the AI powered search engine. Now let us move on to our next project idea which is AI


08:35:14
powered cleaning robots. Today's artificial intelligent powered robots possess no natural general intelligence but are capable of solving problems and thinking in a limited capacity. You can design a robot that uses artificial intelligence to clean a room by scanning the room size, identifying obstacles, and remembering the most effective route for cleaning. For starters, you can design a robot that does only one of these things. Then you can enhance it until it effectively cleans the entire


08:35:39
room properly. The next AI project idea is house security. Now this is a very interesting project. For this project you can design a system that uses artificial intelligence to scan and identify the face of the visitor. First the facial structure of the family members or someone who frequently visit the house can be scanned and stored. So every time a visitor comes near the gate the system can scan the face and if it matches the existing facial structure that is stored in the database it can open the door and allow the person to


08:36:06
pass. Else the gate can remain shut and the people living in the house can be notified that the person is waiting outside. The next project idea is handwritten notes recognition. Handwriting note recognition refers to the computer's ability to get alphabets and numbers. This inputs could be from various sources like paper document, notes on the phone, photos and other sources. Note that handwriting characters remain complex since different individual have different handwriting styles. So you can develop a


08:36:31
system that uses artificial intelligence to scan the handwriting notes and convert them into digital format. You can use the artificial neural network which is a field of study in artificial intelligence to design the system. The next AI project idea is loan eligibility prediction. Nowadays one of the major problem banking employees face in this everchanging economy is the increasing rate of loan defaults. So the employees are finding it difficult to correctly access loan requests and decide whom to


08:36:57
give loan and whom not to. So in order to determine whether an individual should be given a loan or no, you can create an AI program that will check a person's loan eligibility criteria by accessing certain attributes of an individual such as the salary, the previous loan details and so on and then make a decision to approve a loan or not. This program will make the process lot more easier by selecting suitable people from a given list of candidate who have applied for loan. So this was about loan eligibility prediction


08:37:23
project idea. So now let us move on to our next project idea which is AI powered voice assistant. So this is one of the interesting artificial intelligence project. Create a voice- based personal assistant using artificial intelligence. So for this you have to train the system to understand human language so it can understand and save the command in the database. So next time you give the same command it will identify the words and perform the necessary action. This can be very helpful to do vious searching for some


08:37:51
information or item on the web, setting alarms, taking down notes, calling someone, playing songs and many more. The next AI project idea is e-commerce recommendation engine. You can build an e-commerce recommendation engine using the similarities among the background information of the items or users to propose the recommendation to the user. So in this project you can build aation engine using the similarities among the background information of the items or users to propose recommendation to the


08:38:20
user. So for example if the user has searched for Apple phones then you can design a recommendation engine that recommends only Apple phones to the user. Now the other way to do the trends and patterns in the previous and other user item interaction and advise similar recommendation to the present user based on his existing interactions. So an example for this would be if a person has bought a formal shirt then you can design a recommendation engine toing an you can use artificial intelligence to recommend the user what


08:38:49
exactly they need. The next AI project idea is AI enabled maps with artificial intelligence you can create a project algorithm to determine the optimal route to take in order to reach the destination faster. Also to determine which mode of transportation is the best to go to the destination. It could be on foot or in a car, bike, bus. We can also use advanced artificial intelligence in the program by implementing voice assistant that will guide the users about the turns, the potential roadblocks, traffics and create


08:39:19
augmented reality map in real time. The next project idea detection. Now everything that's happening in a science fiction movie could be your future where artificial intelligence is used. One such area of interest is detecting human emotions. There are many top companies investing a lot of money in doing this. So you can design a facial emotion detection and recognition system to identify human facial expressions. So for this first the system would have to analyze the facial expression for some


08:39:48
time and then perform facial feature extraction and classify the facial expression. For starters you can design the system to identify only one expression maybe just then you can enhance it and try for different emotions. The next AI project idea is AI health engine. You can create a project that will use artificial intelligence to give the user must provide all the medical reports and based on that the artificial intelligence system will check for any pre-existing condition ongoing health concerns and gaps in


08:40:16
general health knowledge. Then the health engine could be programmed personal data of the users and the external health data to provide informed advice to the user. It can also help the users with prescription support, vaccination advice, recommended specific condition guidance. So this was about AI health engine project. Now let us move on to our next project idea which is trying on online clothes and accessories. Now you would have already seen this feature if you ever visited the lunchart app for your project. You


08:40:43
can design an artificial intelligence system that takes the input image and computes the person's body model which would represent the posture and their shape. The segments can the dresses are going to for example shirt on the body, gloves for hands and so on. And then when the user chooses a particular dress, the system can combine them with a body model and update the images shape representation. Our 20th is spam. Spam means detecting emails that are irrelevant to the user by understanding the text content of the


08:41:13
email. You can create a project that uses artificial neural network to detect and block spam emails or updates or any ad. You can also enhance it. Let's say the newsletters or updates or any ads which are received from emails can be liked by one person but disliked by another. So include this feature that on the individual user preferences [Music] number one we have increased automation. Artificial intelligence can be used to automate anything ranging from tasks that involve extreme labor to the


08:41:51
process of recruitment. That's right. There are any number of that can be used to automate the hiring or the recruitment process. Such tools help to free the employees from tedious manual tasks and allow them to focus on complex tasks like strategizing and decision. An example of this is conversational intelligence. This application focuses on automating tedious parts of the recruitment process such as scheduling, screening and sourcing. My is trained by using advanced machine learning algorithms and it also uses natural


08:42:27
language processing to pick up on details that come up for creating candidate profiles, performing analytics and finally shortlisting applications. It is a known fact that automating the recruitment process reduces time to hire by 50% and helps in finding key hires that impact profitability and growth. Our next benefit is increased productivity. Artificial intelligence has become a necessity in the business world. It is being used to manage highly computational tasks that require maximum effort and time. Did you know that 64%


08:43:05
of businesses depend on AI based applications for their increased productivity and growth? An example of such an application is a legal robot. I call it the Harvey Spectre of the virtual world. This bot uses machine learning techniques like deep learning and natural language processing to understand and analyze legal documents, find and fix costly legal errors, collaborate with experienced legal professionals, clarify legal terms by implementing a AI based scoring system on multiple scales. It also allows you


08:43:39
to compare your contract with those in the same industry in order to make sure that yours is a standard document. Moving on to our next benefit. AI helps us in making smarter business decisions. One of the most important goals of artificial intelligence is to help in making smarter business decisions. Salesforce Einstein has managed to do that quite effectively. Following Albert Einstein's dictim that the definition of genius is taking the complex and making it simple, Salesforce Einstein is


08:44:12
removing the complexity of artificial intelligence, enabling any company to deliver smarter, personalized, and more predictive customer experience. Salesforce Einstein is a comprehensive artificial intelligence for customer relationship management driven by advanced machine learning, deep learning, natural language processing and predictive modeling. Einstein is implemented in large scale businesses for discovering useful insights, focusing market behaviors, recommending the best possible solutions and also


08:44:43
automating tasks. Moving on to our next benefit. AI has been mainly used to solve complex problems that cannot be solved through other means. Throughout the years, artificial intelligence has progressed from simple machine learning algorithms to advanced machine learning concepts such as deep learning. This growth in AI has helped companies solve complex issues such as fraud detection, medical diagnosis, weather forecasting, and so on. Consider the use case of how PayPal uses artificial intelligence for


08:45:14
fraud detection. Thanks to deep learning, PayPal is now able to identify possible fraudulent activities very precisely. The company processed over 235 billion in payments from 4 billion transactions by more than 170 million customers. Along with so much data, machine learning and deep learning algorithms were used to mine data from the customers purchasing history in addition to reviewing patterns of likely fraud stored in databases. The derived insights and patterns were then used to predict whether a particular transaction


08:45:52
is fraudulent or not. Coming to the next benefit of artificial intelligence, AI is used in strengthening the economy. Regardless of whether you think AI is a threat to the world, it is estimated to contribute over $15 trillion to the world's economy by the year 2030. According to a recent report by PWC, the progressive advances in artificial intelligence will increase the global GDP by up to 14% between now and 2030. It is also said that the most significant economic gains from AI will be in China and North America. These two


08:46:31
countries will account for almost 70% of the global economic impact. The same report also reveals that the greatest impact of artificial intelligence will be in the field of healthcare and robotics. The report also precisely states that approximately 6.6 6 trillion of the expected GDP growth will come from productivity gains especially in the coming years. Major contributors to this growth include automation of routine tasks and development of intelligent boards and tools that can perform all human level tasks. Presently


08:47:06
most of the tech giants are already in the process of using AI as a solution to laborous tasks. However, companies that are slow to adopt these AI based solutions will find themselves at a serious competitive disadvantage. Moving on to our next benefit which is AI in performing repetitive tasks. So we all know that performing repetitive tasks can become very monotonous and timeconuming. Not to forget it's quite boring. So using artificial intelligence for tiresome and routine tasks can help


08:47:38
us focus on the most important tasks in our to-do list. An example of such an AI is the virtual financial assistant used by the Bank of America called Erica. Erica implements artificial intelligence and machine learning techniques to cater the bank's customer service requirements. It does this by creating credit report updates, facilitating bill payments, and helping customers with simple transactions. Erica's capabilities have recently been expanded to help clients make smarter financial


08:48:10
decisions by providing them with personalized insights. As of 2019, Erica has surpassed 6 million users and has serviced over 35 million customer service requests. Our next benefit of artificial intelligence lies in personalization. Research from Mckenzie found that brands that excel at personalization deliver five to eight times the marketing ROI and boost their sales by more than 10% over companies that don't personalize. Personalization can be an overwhelming and timeconuming task, but it can be simplified with the


08:48:46
help of artificial intelligence. In fact, it's never been easier to target customers with the right product. An example of this is a UK based fashion company called Thread that uses artificial intelligence to provide personalized clothing recommendations for each customer. Most customers would love a personal stylist, especially one that comes at no charge. But staffing enough stylists for 650,000 customers would be expensive. Now instead the UK based fashion company thread uses artificial intelligence to provide


08:49:20
personalized clothing recommendations for each of its customer. Customers frequently take style quizzes to provide data about their personal style. Each week customers receive personalized recommendations that they can upward or downward. Thread uses a machine learning algorithm called thimble that uses customer data to find patterns and understand the likes of the buyer. It then suggests clothes based on the customer's taste. This is how personalization is performed in threat. Moving on to our next benefit which is


08:49:51
artificial intelligence in global defense. The most advanced robots in the world are being built with global defense applications in mind. This is no surprise since any cuttingedge technology first gets implemented in military applications. Though most of these applications don't see the light of day. One example that we know of is the ANBOT. The AI based robot developed by the Chinese is an armed police robot designed by the country's National Defense University capable of reaching maximum speed of 11 mph. The machine is


08:50:26
intended to patrol areas and in the case of danger deploy an electrically charged riot control to the intelligent machine stands at a height of 1.6 6 m and can spot individuals with criminal records. The anbboard has contributed in enhancing security by keeping a track of any suspicious activity happening around its vicinity. Moving on to the next benefit, we have artificial intelligence in disaster management. For most of us, precise weather forecasting makes vacation planning easier. But even the


08:50:59
smallest advancement in predicting the weather majorly impacts the market. Accurate weather forecasting allows farmers to make critical decisions about planting and harvesting. It also allows airlines to maximize the use of their planes. It makes shipping easier and safer. And most importantly, it can be used to predict natural disasters that impact the lives of millions. Among companies using artificial intelligence to predict the weather, only a few have invested as heavily as IBM. After years


08:51:31
of research, IBM partnered with the weather company and acquired tons and tons of data. The acquisition gave IBM access to the weather company's impressive network of sensors and models, providing a massive pipeline of weather data it could feed into IBM's AI platform, Watson, in order to attempt to improve any predictions. In 2016, the weather company claimed that their models use more than 100 terabytes of third-party data every single day. The product of this merger is the AI based IBM Deep Thunder. This system provides


08:52:07
highly customized information for business clients by using hyper local forecasts at a 0.2 to 1.2 mile resolution. This information is useful for transportation companies, utility companies, and even retailers. Moving on to the last benefit of artificial intelligence is that it enhances our lifestyle. And we're all aware of how AI is actually enhancing our life and changing our life. In the last decade, artificial intelligence has gone from a science fiction dream to a critical part of our everyday lives. We use AI systems


08:52:42
to interact with our phones and speakers through voice assistants like Siri, Alexa, and Google. Cars made by Tesla interpret and analyze their surroundings to intelligently drive themselves. Amazon monitors our browsing habits and then serves up products it thinks we'd like to buy. And even Google decides what results to give us based on our search activity. Artificially intelligent algorithms are here and they've already changed our lives for better or for worse. But this is only the beginning. And one day we'll look


08:53:14
back at AI in 2019 and laugh about how primitive it was. Because in the future, artificial intelligence is going to change everything. [Music] Generative AI is reshaping industries and economics at the staggering pace. Reports by Goldman Sachs and PWC Global highlights that AI has the potential to significantly boost global GDP and labor productivity while driving economic gains of up to 45% by 2030. So let's dive into how you can prepare for a lucrative generative AI career. So to get started with Gen AI, start with the


08:53:56
natural language processing. Natural language processing is the branch of AI that focuses on the interaction between computers and human language. So here's what you need to learn. First, start with the basics of NLP. Learn how machines understand and process language. Next, parts of speech tagging where you need to understand how to label words with grammatical tags. Next, text processing. Here, learn techniques for cleaning and preparing text data for models. Next, named entity recognition,


08:54:29
where you need to learn to build models that identify entities like names and dates. Next, text vectorzation. Study methods to convert text into numerical data using techniques like bag of words and embeddings. Next, learn about large language models. Well, large language models are powerful AI systems trained on massive data sets to understand and generate humanlike text. Here dive into different types of LLMs like Llama, Falcon and Gemini. Next transformers where you need to learn about their


08:55:03
architecture including the encoder, decoder and attention mechanism. Next read the similar paper attention is all you need for the in-depth understanding. In this paper you can read about the transformers in depth. Next working with APIs. APIs that stands for application programming interfaces allow you to integrate and extend functionality in your projects and they can be paid or unpaid. So in this video we will mostly tell you about free API sources. Use free API platforms like Glitch, Postman,


08:55:38
Moi, Heroku, Rapping API and Firebase functions. Also build APIs with platforms like group to create generative AI integrations. And you can also use hugging face for free resources. Now as you can see here we are at the group platform and on the top we have this developer option. And once you hover on it you can see options like free API creation. And once you click on it it will redirect you to this where on your left you can see different options. Now if you want to check about the model then click on the playground then on


08:56:12
your right you have the llama model. So if you click on that you can see different model group website provides and these are all for free. Next click on create API key and here give any name to your API key and that's it. You have your own API key now. So from here you can copy it and use it. Also you can create as many API keys as you want. You can explore hugging face for pre-trained models and data sets. So now we are at the site of hugging face. You can see options like models, data sets, space,


08:56:47
post, docs, etc. Now suppose you want to code for your model then click on model and it will redirect you here where you can see options like libraries, data sets, language or license models etc. Now select the model according to your project. As you can see we have different options here. So select your model and use this website as much as you want. Next start building mini projects for practice. So here start applying your knowledge with hands-on projects such as named entity recognition models. Next translators and


08:57:19
summarizer, sentiment analysis systems also the texttospech or speechtoext applications. After building many projects you can move on to advanced topics. So to enhance your skills in AI, focus on techniques like quantization to optimize models for faster and more efficient deployments, particularly in resource constraint environments. Then dive into fine-tuning large language models to customize pre-trained models for specific task unlocking their full potential for tailored applications. Additionally, explore the lang


08:57:53
framework, a powerful Python library for building generative AI pipelines. practice using its comprehensive documentations and tutorial videos to deepen your understandings. So if you haven't already, watch our video featuring two lang project. It provides valuable insights and practical examples to get you started with this versatile tool. And this Edureka's generative AI certification and training will teach you Python programming, data science, artificial intelligence, natural language processing and many other


08:58:21
updated technologies that a beginner or advanced learner is seeking. Now after much practice you must know how to develop a backend and a front end. So start with the back end because when you start with the back end you will deal with real life problems and have a thorough knowledge of your coding language too. So for backend development you can leverage frameworks like fast API, Django and flask to create robust and efficient serverside applications. These frameworks are well suited for handling API creation, request routing,


08:58:52
and server side logic, making them essential for modern web development. To manage and store data effectively, consider using databases such as MySQL or MongoDB, which offer reliable and scalable solutions for structured and unstructured data, respectively. Combining these tools allows you to build powerful datadriven application with ease and efficiency. Edureka's Python developer masters program crafted by an industry expert will give you extensive knowledge about Python and its libraries. Also with a hands-on project


08:59:24
you will gain proficiency in data science, machine learning, deep learning and natural language processing. So if you are a beginner and want to upskill yourself, you can check out this certification. You can also consider a Django certification training course where you will be guided by industry experts and gain insights into Django rest framework. Django models, Django Ajax, Django query and many other top technologies. Also, if you're looking for particular database certification courses, you can check out the MongoDB


08:59:54
certification training course which will give you deep insights into back-end database technologies. When you're thorough with the back end, next moving on to front end. For front-end development, focus on creating userfriendly interfaces to complete your projects effectively. Start with basic web development by mastering foundational technologies like HTML, CSS, and JavaScript to build responsive and visually appealing designs. As you progress, explore advanced JavaScript frameworks such as ReactJS, Vue.js or


09:00:25
Angular, which provide powerful tools for building dynamic, interactive, and scalable web applications. This framework streamline the development process and enhance the user experience, making them essential for modern front-end development. Edurea's web developer master program will give you insights into HTML, CSS and other web development tools. So if you are a beginner and want to upskill yourself, you can check out this certification course. Also, you can check out the fullstack web development program which


09:00:54
will teach you front end and backend development with the core technologies you need as a beginner. Now having both front end and backend complete, you need one more thing for your journey that is version control with git and github. Version control is essential for tracking changes in your projects and collaborating efficiently with others. And by learning to use Git commands, you can manage versioning, maintain a history of your work, and seamlessly revert to a previous states if needed. Additionally, leveraging platforms like


09:01:25
GitHub enables you to deploy and manage projects, collaborate with team members, and showcase your work to a broader audience, making it an indispensable skills for developers. Edureka's Git certification training will give you insights into version control from scripting to Git workflow. Everything you need to know about version control. So if you're a beginner and want to upskill yourself, you can check out this certification training course. And by following this road map and practicing


09:01:55
consistently, you will develop a strong foundation and advanced skills to excel in a highpaying career in generative AI. So start today and remember hands-on project and continuous learning are the key to success. [Music] Let's first look at the Gen AI impact on the market. So here is the graph illustrating the potential impact of generative AI on the job market. It suggests that while generative AI will lead to the job losses, it will also create new job opportunities. The number of job influenced by generative AI is


09:02:32
projected to increase significantly over the next few years far exceeding the number of jobs lost. This indicates that while some jobs may be automated, generative AI is expected to drive overall job growth and create new economic opportunities. Next, the graph median salary of Gen AI professionals across years of experience in India. This shows a clear upward trend in salaries with increasing experience. Professionals with 0 to three years of experience earn a median salary of around 10 lakhs peranom. This figure


09:03:05
steadily increases to 12 lakhs for 3 to six years of experience, 16 lakhs for 6 to 10 years and 19 lakhs for 10 to 12 years. The highest median salary at 23 lakhs is observed for professionals with 12 plus years of experience in the genai field. This indicates a strong correlation between experience and earning potential in India's field of generative AI. And last but not least, the graph generative AI market size 2023 to 2033. And this illustrates the projected growth of the generative AI market. It shows a significant upward


09:03:40
trend with the market size expected to increase from 17.65 65 billion in 2023 to a staggering $83.90 billion by 2033. The market is projected to grow at a rapid pace with substantial increases anticipated each year. This indicates a strong demand for generative AI solutions and its potential to revolutionize various industries. Now the era of generative AI is leading in a new wave of career opportunities. From crafting prompts to managing AI products and ensuring ethical development, the demand for skilled professionals is


09:04:15
surging. As AI continues to evolve, roles like generative AI research scientists and synthetic data engineers are emerging, promising exciting career paths at the forefront of technological innovation. So, embrace the future and explore this top five career opportunities to unlock your potential in the world of generative AI. Now, the first one we have is AI prompt engineer. An AI prompt engineer focuses on designing prompts to optimize AI outputs, ensuring that models generate accurate, relevant, and high quality


09:04:48
responses. This role involves crafting prompts for tools like Chad GBD to effectively solve specific business problems or creative writing task. Now that you know about AI prompt engineer, now let us explore the skills and tools required for it. First talking about skills. Prompt engineering involves crafting effective instructions for AI models like chat GPT. A strong understanding of natural language processing is very essential for this as it helps in guiding models to process and comprehend human language


09:05:20
accurately. Creativity plays a vital role enabling the development of innovative and imaginative prompts that lead to unique and valuable outputs from AI models. Next, let us talk about tools. Python, a versatile programming language, is widely used in AI and machine learning, including for building and interacting with AI models. Hugging face, a popular platform, provides access to a vast collection of pre-trained AI models and tools for natural language processing and other AI task. Specialized prompt engineering


09:05:53
tools assist users in crafting and refining prompts, offering suggestions and optimization techniques. Additionally, lang chain can be used to integrate multiple AI models into more complex applications, further enhancing the capabilities of prompt engineering. Hence, by mastering these skills and utilizing these tools, AI prompt engineers can effectively harness the power of AI models to generate creative and valuable content. And with the rise of prompt based AI tools, the demand for this role is rapidly growing across


09:06:25
various industries. And the salary range for an AI prompt engineer in India is 6 to 10 lakhs per and in the United States they earn between $100,000 to $150,000 annually depending on expertise and location. Now we have the next career which is AI product manager. An AI product manager leads the development of AIdriven products aligning technical teams with business goals and user needs. This role involves managing the release of AI powered tools such as customer service solutions or recommendation systems for e-commerce


09:07:01
platforms. Now that you know about AI product manager, so let us explore the skills and tools required for it. First talking about skills. Product life cycle management involves overseeing the entire journey of an AI product from ideation to post launch support including market research, planning, development, testing and maintenance. A solid understanding of AI fundamentals such as machine learning, deep learning, and natural language processing is essential for managing AI product development effectively. Strong


09:07:32
leadership skills are necessary for guiding cross functional teams, including engineers, data scientists, and designers, ensuring clear communication, decision making, and the ability to inspire and motivate team members throughout the process. Next, talking about tools, agile development is a methodology that encourages iterative development and collaboration, making it ideal for AI product development due to its flexibility and adaptability in response to the rapid evolution of AI technologies.


09:08:04
JRA is a widely used project management tool for tracking task, bugs, and features throughout the development process. Miro a collaborative whiteboard tool helps team brainstorm, visualize and organize ideas facilitating effective collaboration during the product development journey. Hence by mastering these skills and utilizing these tools, AI product managers can effectively lead the development and lodge of successful AI products. And as organizations increasingly integrate AI into their products, there is a strong


09:08:36
demand for skilled professionals to guide the process. And the salary range for an AI product manager in India ranges between 16 to 29 lakhs peranom. And in the United States, they earn between$100,000 to $200,000 annually. In our list, next we have AI ethics specialist. An AI ethics specialist ensures that AI systems comply with ethical guidelines and regulations focusing on fairness, transparency, and accountability. This role involves auditating AI models to prevent bias in hiring tools or ensuring compliance with


09:09:11
privacy laws like GDPR. Now that you know about AI ethics specialist, so let us explore the skills and tools required for an AI ethics specialist. First talking about skills, a strong understanding of ethics in AI is crucial covering principles like bias, fairness, transparency and accountability in AI development and deployment. Knowledge of compliance standards such as GDPR and HIPAA ensures that AI systems are developed and used ethically and legally. Critical analysis is also vital as the ability to assess AI systems and


09:09:47
identify potential ethical risk and biases is a key skill for AI ethics specialist. Next talking about tools, GDPR that stands for general data protection regulation is a European Union law that governs the process of personal data setting strict standards for data privacy and security especially for AI systems handling personal data. HIPA that stands for health insurance portability and accountability act is a US law that establishes standard for protecting sensitive health information requiring AI systems dealing with


09:10:21
healthcare data to comply with its regulations. The OECD AI principles developed by the organization for economic cooperations and development offer a framework for ethical AI development addressing topics such as human rights, inclusivity, and environmental impact. Hence, by mastering these skills and understanding these tools, AI ethics specialists can play a crucial role in ensuring that AI is developed and used responsibly, ethically, and in compliance with relevant regulations. As governments and


09:10:53
organizations play a higher emphasis on responsible AI deployment, the demand for AI ethics specialist is increasing and the average salary in India for AI ethics specialist is 14 lakh 2,849 rupees annually. In the United States for this role typically falls between $80,000 to $120,000 annually. Next on the list, we have generative AI research scientist. AI generative AI research scientist develops and fine-tunes new generative models like Chad GPD midjourney or DALI to advance AI capabilities and this role involves


09:11:28
working on foundational models for text to image tools or improving large language models for tasks such as translation and summarization. Now that we know about generative AI research scientist, so let us explore the skills and tools required to be a generative AI research scientist. Well, first talking about skills, deep learning, a subset of machine learning, uses artificial neural networks to learn complex patterns from large data sets, and it's fundamental for developing generative AI models.


09:11:56
Understanding neural networks, including their architecture and functioning, is crucial for designing and training these models. Additionally, a strong foundation in mathematics and statistics is essential for understanding and implementing the algorithms that power generative AI models. Now talking about tools, Python is a versatile programming language widely used in AI and machine learning including for developing and training generative models. TensorFlow an open-source platform developed by


09:12:22
Google provides tools and libraries for building and training complex machine learning and deep learning models. PyTorch another popular opensource machine learning framework is known for its flexibility and ease of use making it a preferred choice for research and development. Hence, by mastering these skills and utilizing these tools, generative AI research scientists can push the boundaries of AI and develop innovative solutions for a wide range of applications. And due to the explosive growth of generative AI, there is a high


09:12:51
demand for this role in tech companies and research institutions. The salary range for a generative AI research scientists in India is 19 to 30 lakhs per anom. And in the United States, they earn between $158,000 annually with top companies offering even higher compensation. Finally, we have synthetic data engineer. A synthetic data engineer creates and manage synthetic data to train AI models when real world data is unavailable, incomplete or sensitive. This role involves generating synthetic data such as medical data to train


09:13:26
healthcare diagnostic AI systems without risking patient privacy. Now that we know about synthetic data engineer. So let us explore the skills and tools required for a synthetic data engineer. First talking about skills. A strong foundation in data science is crucial for understanding data, its distribution and the techniques required to generate synthetic data that closely resembles real world data and knowledge of various data generation techniques such as statistical modeling, machine learning


09:13:53
and generative models is essential for creating highquality synthetic data sets. Additionally, understanding data dynamics and how data evolves over time is important for generating synthetic data that accurately reflects real world trends and patterns. Next, talking about tools, Python is a versatile programming language widely used in data science and machine learning, offering a rich ecosystems of libraries and frameworks for data generation and analysis. TensorFlow, an open-source platform developed by Google, provides tools and


09:14:23
libraries for building and training generative models. Additionally, various AI tools including generative models and machine learning algorithms are used to create synthetic data that is statistically similar to real world data. Hence, by mastering these skills and utilizing these tools, synthetic data engineers can create highquality synthetic data that can be used to train AI models, test algorithms, and conduct privacy perceiving data analysis. Now that the demand for this role is growing in industries like healthcare, finance


09:14:54
and autonomous vehicles where data privacy and scarcity presents significant challenges. The salary range for a synthetic data engineer in India is 9 to 15 lakhs peranom and in the United States it is $16,000 annually. Now that we have a wealth of knowledge about career opportunities and you might be wondering if they will continue to grow in the future. So to answer that we have a few global reports from AI career growth. Well global AI market growth says that in coming years the global AI market is projected to


09:15:27
reach a staggering of $190 billion and this significant growth is expected to create millions of job opportunities worldwide. Also, the soaring demand for AI specialist report says that the demand for AI specialist has witnessed a remarkable search growing by 74% annually over the past 5 years. And this data is sourced from the LinkedIn emerging jobs report. And in industries driving AI demand, the report sees that industries such as healthcare, finance, and autonomous vehicles are experiencing


09:16:00
significant growth leading to a high demand for AI professionals to drive innovation and development in their sectors. Overall, this underscores the exciting future of AI careers with numerous opportunities for skilled professionals to contribute to the advancement of technology and shape the industries of tomorrow. Since you're now familiar with most of the skill set and tools, you might be wondering where to find certifications, courses, and resources to master them. So, we have resources to build skills. First, prompt


09:16:30
engineering with generative AI course offered by Edureka. This course is designed to take learners how to effectively use prompts to generate desired outputs from artificial intelligence models. The course highlights the growing importance of prompt engineering in the field of AI and its potential to transform problem solving and creative process. Next, machine learning course masters program offered by Edureka. This course is designed to take learners how to effectively use concepts like machine


09:16:58
learning, deep learning and data science along with some essential tools. Next, artificial intelligence course for beginners offered by Idurka. The course is designed by industry experts and focuses on helping learners improve their AI and machine learning skills. It covers fundamental concepts like classification and regression algorithms and prepares learners for roles like AI solution consultant, technical support engineer for AI products and AI research assistant. Now we have some recommendations for those who enjoy


09:17:28
reading and hands-on practice through books. So first book on artificial intelligence, a guide for thinking humans that provides a comprehensive introduction to AI making it accessible to a wide audience. Next on deep learning that is considered a foundational text in the field of deep learning. It covers the mathematical and computational underpinnings of deep learning techniques. Next, the Oxford Handbook of Ethics of AI that explores the ethical implications of AI addressing topics such as bias,


09:17:59
fairness, accountability, and transparency. It offers valuable insights into the societal and ethical challenges posed by AI. And these books collectively provide a solid foundation in AI covering both technical aspects and broader societal implications. They are valuable resources for anyone interested in learning more about AI and its potential impact on the future. [Music] So let's get started with our first question that is how traditional AI different from generative AI. Now let's take a look at the differences between


09:18:37
traditional AI and generative AI. Traditional AI relies on predefined algorithms and primarily works with labeled supervised data to solve specific problems. Whereas generative AI uses advanced learning techniques to understand data structures, train on data sets, reason and create new and innovative solutions. This ability to generate fresh output sets generative AI apart from traditional methods. Let's compare traditional AI and generative AI based on their functioning. So in traditional AI the process typically


09:19:09
involves data collection, model selection, training the model, evaluation, feedback and finally deployment. And on the other hand, generative AI starts with data prep-processing followed by AI model training, data generation, identifying patterns and iterative training that incorporates feedback and acknowledgement. Once defined, the model is then deployed. And this iterative and creative approach makes generative AI more dynamic in its application. Well, the key differences is that traditional


09:19:42
AI works on predefined rules while generative AI does not. So here are some examples to differentiate traditional AI and generative AI. Traditional AI is used in applications like image detection, stock market production, fraud detection, and voice recognition. Whereas generative AI is utilized for tasks such as content creation, predictive fashion trends, code generation and advancements in healthcare industries. Moving on to next question. How does generative AI help in scaling businesses? Well, generative AI


09:20:16
helps businesses scale by automating and optimizing process, creating personalized customer experience, and generating datadriven insights. Here's a breakdown of how it contributes. First, efficiency and automation. Generative AI automates repetitive tasks such as content creation, customer service via chat bots, and report generation, freeing up human resources for strategic activities. Next, pattern recognition. By analyzing vast data sets, generative AI identifies trends and patterns, enabling businesses to make proactive


09:20:52
decisions, optimize operations, and predict market shifts. Next comes the content creation. It generates highquality marketing materials, product descriptions and creative assets at scale which reduces the cost and production time. Next, personalized experience. AI customizes interactions and recommendations for customers improving satisfaction and loyalty which are critical for scaling. Next is the rapid prototyping and deployment. Generative models create simulations, prototypes, and even entire product


09:21:26
designs, accelerating product development cycles and reducing time to market. And finally, enhanced decision making. AI powered tools provide actionable insights and forecast guiding businesses in resource allocation, market strategy, and scaling efforts. Well, generative AI acts as a catalyst for growth by delivering scalable innovative solutions that adapt to the dynamic demands of modern business environments. Moving on to our next question. What are some common applications of generative AI in real


09:21:59
world? Generative AI has the wide range of application in real world like gaming. In the gaming industry, renald companies like Nvidia and Ubisoft are utilizing generative AI to enhance various aspects of game development and player experience. Generative AI is being used to boost creativity, improve screen time dynamics, refine shadow techniques, and elevate the overall player experience, making games more impressive and engaging. Next, in content creation. So in the field of content creation, companies like


09:22:31
Microsoft Azure and OpenAI, the creators of Chad GPT are developing advanced algorithms to generate unique and innovative content. This content not only captivates audience but is also highly appreciated by critics showcasing the transformative potentials of generative AI in creative industries. Next, in fashion and design. In the fashion and design industry, companies like H&M Group and Nike are leveraging generative AI to offer personalized clothing, shoes and accessory online. This innovative use of AI has brought


09:23:06
significant advancements enabling this brands to enhance customer experience and set new standards in personalized fashion. Next in healthcare industries. Well, in the healthcare industries, companies like Atom Wise and in silicone medicine are utilizing generative AI to enhance efficiency and accuracy in medical care. Given the critical importance of precision when it comes to saving lives, generative AI plays a pivotal role in advancing medical research, diagnostics, and treatment solutions. Next, in chat boards and


09:23:41
virtual assistants. So to enhance user experience major companies like Google, Microsoft and IBM are developing advanced chatbots and virtual assistants. These AIdriven tools aims to provide personalized support, improve efficiency and streamline communication making interactions more initiative and engaging. So our next question is what are some popular generative AI models you know? So first one we have is GPD. GPD stands for generative pre-trained transformer series. A series of language models designed to generate humanlike


09:24:18
text based on pre-trained data enabling task like text generation, translation and event summarization. Next we have bird birectional encoder representations from transformers. a language model that understands context in both directions like left to right and right to left to improve task like question answering and language understanding. The next generative AI model is DAL E and DAL E2 AI models capable of generating images from textual descriptions with DAL 2 offering enhanced image quality and more accurate


09:24:54
interpretations of complex prompts. Now moving on to our next question. What are some challenges associated with generative AI? Some key challenges associated with generative AI include data privacy and security. Large data set used for training may compromise user privacy or include sensitive information leading to regulatory and ethical concerns. Next, bias and fairness. Generative AI can amplify biases present in the training data resulting in outputs that are unfair or discriminatory. Next is the quality and accuracy.


09:25:31
Ensuring the quality and factual accuracy of AI generated content is difficult especially with diverse and large data sets. And the next challenge is interpretability and transparency. Generated content can lack traceability making it hard to explain or justify outputs. Potential for content to closely resemble on copyrighted material. And the next challenge is ethical concerns. misuse of generative AI for malicious purpose such as creating deep fakes, spreading misinformation or automating spam. Next,


09:26:04
resource intensity. Training generative models requires significant computational resources leading to high cost and environmental concerns. And finally, deployment challenges. So, ensuring robust, scalable and safe deployment of generative AI models in real world application is complex. And addressing these challenges requires a combination of technical solutions, ethical guidelines and legal frameworks. Moving on to next question. What is the large language model and how is it used in generative AI? Well, a large language


09:26:37
model is a type of artificial intelligence model that has been trained on vast amounts of text data to understand and generate humanlike language. And these models are typically built using deep learning architectures such as transformer based models like GPD and T5 and contains billions to trillions of parameters that help them capture the details of natural language and some of the key features of LLM include the massive scale. LLMs are trained on extensive data sets that includes a variety of domains such as


09:27:08
books, articles, websites and more. The next feature is contextual understanding. LLMs can understand the context of words, phrases, and sentences by processing long range dependencies, making them capable of generating coherent and contextually relevant text. Next, transfer learning. Pre-trained LLMs can be fine-tuned on specific task or data sets, allowing them to perform a wide range of language related task with minimal task specific data. But how LLMs are used in generative AI? Well, it is


09:27:41
used in text generation. LLM can generate coherent and human-like text based on the prompt. Like for example, GPT3 can write articles, create dialogues, generate poetry or even code. Example, given a prompt like write a story about a dragon and a wizard, the model generates a creative and coherent story. Next, it is used in text completion. LMS can predict and complete sentences or paragraphs based on the initial input making them useful in applications like autocomp completion chat boards and writing assistance.


09:28:16
Next, LLMs are used in translation and summarization. LLMs can translate text from one language to another or summarize long documents into concise summaries making them valuable for tasks requiring cross-lingual understanding or information distillation. Next, in conversational agents, LLMs power conversational AI systems such as chatbots, virtual assistants that can understand and respond to user queries in natural language, offering more dynamic and human-like interactions. It is also used in creative content


09:28:49
creation. LLMs are used to generate creative content such as articles, marketing copy, and even scripts for videos or films. Next in sentiment analysis and classification. LLMs can be used to fine-tune to classify text like example sentiment analysis, spam detection by predicting labels based on context and training data. Next, what are some common applications of large language models? So some of the common applications of large language models include chat bots and conversational AI powering virtual assistants and customer


09:29:24
support bots to engage in natural language conversations. Next language translation translating text from one language to another while preserving meaning and context. Also in summarization automatically compressing large documents or articles into concise summaries. And you can also give examples of sentiment analysis which determines the sentiment or emotional tone of text for applications in social media monitoring, customer feedback analysis and more. Also in code generation where it is assisting in


09:29:56
programming by generating code snippets based on natural language descriptions. And the next question is what is prompt engineering and why is it important in generative AI? Well, prompt engineering is the process of designing and refining prompts to effectively guide the output of a generative AI models, particularly language models like GPT. In the context of generative AI, a prompt is an input or instruction provided to the model which it uses to generate a response or output. The quality, clarity, and


09:30:28
structure of the prompt can significantly influence the model's output, making prompt engineering a critical skills in maximizing the performance and usefulness of these models. The image shows the concept of prompt engineering, emphasizing its role in optimizing interaction with AI systems. It highlights three key objectives. improving accuracy which ensures precise responses. Enhancing relevance which tailor outputs to user needs and increasing efficiency which streamlines process for better


09:30:59
performance. And together these elements demonstrate how effective prompts design can significantly improve the overall functionality and outcomes of AI models. Next, how does generative AI handle the creation of textbased content? So, the generative AI generates textbased content through a process that involves several key steps. Here's a simplified explanation. First, training on large data sets. Pre-trained on large data sets, AI models like GPT are trained on vast amounts of text data from diverse


09:31:31
sources such as books, articles, and websites. This extensive training helps the model to learn language patterns, context and general knowledge forming the foundation for its ability to perform various language task effectively. Next, the architecture of the model. Generative AI models are built on the transformer architecture, a highly effective framework for processing sequential data. It leverages mechanisms like self attentions to understand context and relationships within text enabling text such as


09:32:03
language generation and comprehension. Next, generating responses. The generation process involves multiple stages including structuring ideas, connecting relevant information, analyzing data, writing coherent content, and finally deploying or delivering the output in a meaningful and usable form. Next, iterative process. In simple terms, it means improving the model by training it, receiving feedback and making adjustments based on that feedback to enhance its performance. So, in summary, generative AI models like GPT generate


09:32:38
text by predicting word sequence based on learned patterns from extensive training data while understanding and maintaining context throughout the generation process. Examples of generative models include GPT4, birth transformers and T5 which stands for texttoext transfer transformer. Moving on to the next question. Can you explain how generative AI is used in chat bots or language translation? Generative AI enables chatbots to produce contextually relevant, coherent, and human-like responses improving the user experience.


09:33:12
Key areas where generative AI contributes include generating responses. By leveraging large language models, generative AI can produce more natural and engaging responses, making chatbot interactions feel more humanlike. Next, creativity. Generative AI can be used to generate creative content such as poems, stories, or scripts which can be incorporated into chatbot conversations to make them more entertaining and engaging. Next, personalization. Generative AI can analyze user data to personalize chatbot


09:33:45
responses, making the interaction feel more tailored to the individual user. And then we have context management. Generative AI can maintain context throughout a conversation, allowing chatbots to understand and response appropriately to the users involving needs and preferences. These capabilities of generative AI are transforming how chatbots and language translation tools are used making them more effective and userfriendly. Now moving on to the advanced level questions in generative AI and the


09:34:16
question is what is the role of the data in training generative AI models. Data plays a crucial role in training generative AI models as the quality, quantity and diversity of the data significantly impact the model's performance and output. Here are some of the examples of data usage in generative AI models. First, in image generation for models like GANs, large data sets of images are used to teach the models how to generate realistic images. Next, text generation. For models like GPT4, once


09:34:49
text from various sources like books, articles, website are used to train the model to understand and generate humanlike text. And next in music generation, models train on large collections of musical compositions, learn patterns in melody, harmony and rhythm to generate new music. Now moving on to next question. How do attention mechanism enhance the performance of generative AI models? Well, attention mechanisms significantly enhance the performance of generative AI models by allowing them to focus on


09:35:22
relevant parts of the input data when generating each part of the output. So here's how they work and contribute to improve performance. The text at the top states that the attention mechanism is a critical innovations that enhances the performance of generative AI models. The diagram shows how multiple attention heads process information in parallel. Each attention head focuses on different aspects of the input sequence, allowing the model to capture complex relationships and dependencies between


09:35:51
different parts of the data. The outputs of these attention heads are then concentrated and processed further to generate the final output. This multi-headed attention mechanism enables the model to learn more detailed representations of the input leading to improved performance in tasks such as language translation, text summarization and image generation. And the next question is what is GANs which stands for generative adversary networks and how do they function? GANs are the type of generative models that are


09:36:21
particularly powerful for generating realistic data such as images, text or audio. The diagram provides a clear visual representations of a GAN's architecture and its functioning. Well, GAN which stands for generative adversarial network. This is a central component. It highlights that a GAN is a system consisting of two main subn networks. Next, generator. Generator is responsible for creating new data samples. Well, it takes input and attempts to generate realistic data points, example, images, text that


09:36:55
mimics the training data. Next is the discriminator. The discriminator acts as an evaluator. It takes and input both real data samples from the training set and the generated samples from the generator. Its task is to differentiate between the real and fake data assigning a probability of being real to each sample. Now let us see the functioning of GANs. GANs comprise two key components, a generator and a discriminator. The generator aims to create new data instances that closely resembles the training data. Whereas the


09:37:28
discriminator acts as a binary classifier tasked with differentiating between real data samples from the training set and synthetic data generated by the generator. The adversary relationship between the generator and the discriminator drives the GANs to produce increasingly realistic synthetic data. So as you can see the screen, this image shows the functioning of a generative adversary network. It starts with random noise as input which is fed into the generator network. This network attempts to


09:37:58
generate a fake sample that resembles real data. The generated sample is then presented to the discriminator network alongside actual real samples from the training data set. The discriminator's task is to classify each sample as either real or fake. So both the generator and discriminator networks learn and improve through this adversarial process with a generator aiming to produce increasingly realistic samples that can fool the discriminator. while the discriminator strives to accurately distinguish between the real


09:38:30
and fake data. Next, how do generative models like GANs help in image generation? Generative models like generative adversary networks are powerful tools for image generation. GANs consist of two neural networks, the generator and the discriminator, which work together in a competitive process to create highly realistic images from random noises. Here's how GANs help in image generation. Generative models like GANs are powerful tools for creating realistic images. GANs use a dual network approach to achieve this. The


09:39:04
generator network creates new images. The discriminal network evaluates these images trying to distinguish between the real and generated images. This adversarial process forces the generator to create increasingly realistic images. Now let's see the training process of GATS. In this process, the generator network strives to improve its ability to create a realistic images that can deceive the discriminator. Conversely, the discriminator network focuses on enhancing its ability to detect fake images leading to an ongoing


09:39:37
improvement cycle for both networks through the continuous adversarial learning between the generator and discriminator. The generator eventually produces highquality images that are so realistic that they can successfully deceive the discriminator. Moving on to next question. What are variational autoenccoders and how do they differ from GANs? Well, variational autoenccoders are a type of generative model that is built on the principles of autoenccoders, a neural network architecture commonly used for


09:40:08
unsupervised learning task like dimensionality reduction and feature learning. Key components are encoder. The encoder maps input data, for example, an image to a latent space representing it as a distribution over latent variables. Next, the output. Instead of mapping to a single point in latent space, the encoder outputs the mean and variance of a gshian distribution. This distribution captures the uncertaintity in the encoding process. Next, we have decoder. The purpose is the decoder takes the sample


09:40:44
latent variables and maps it back to the data space reconstructing the original input data or generating new data. Output the decoder produces a reconstructions of a original input or a new data point that lies within the learned distribution. Next, what role can generative AI plays in education, especially in creating customized learning materials for students? Generative AI can play a transformative role in education by creating customized learning materials that cater to individual student needs. Here's how it


09:41:17
can be utilized effectively. First, in personalized learning parts, customized content. AI generates tailored content based on a student's learning pace, interest, and proficiency levels. For example, if a student struggles with a particular math concept, the AI can create additional exercise or explanations focused on that topic. Next, adaptive assessments. AI can design assessments that adapt in real time to a student's performance, offering more challenging problems as the student progresses or providing more


09:41:50
foundational questions if they encouer difficulties. Next, interactive learning materials. It provides dynamic textbooks. AI can create interactive textbooks that update based on the latest research and educational trends. These materials could include simulations, quizzes, and other interactive elements that respond on how a student interacts with the content. Next, simulations and virtual labs. AI can generate virtual experiments and simulations, allowing students to explore complex scientific


09:42:23
concepts in a safe, controlled environment. Next, language and writing support in grammar and style suggestions. AI can assist students in improving their writing by providing real-time feedback on grammar, style, and clarity. It can also suggest alternative phrases or vocabulary based on the students level. Next, essay generation assistance. For younger students or those learning new languages, AI can help generate essay outlines or suggest content structure, helping them organize their thoughts


09:42:54
more effectively. Next, automated content creation. Lesson plans. AI assist teachers by generating lesson plans that align with curriculum standards, saving time and allowing teachers to focus more on student interaction. Next, practice problems and quizzes. AI can create endless variations of practice problems and quizzes, ensuring that students get the repetitions they need without encountering the same problem twice. Next, support for diverse learning styles in visual and audio content. AI can generate visual ads like charts,


09:43:29
graphs, and diagrams to produce audio explanations for students. Next, AI can also create educational games that are tailored to the curriculums, making learning more engaging and fun for students with the preferences for interactive learning. Well, generative AI in education has the potential to create more personalized, engaging, and effective learning experience for students while also supporting teachers with the tools they need to enhance their teaching. Next, what is the Gossian mixture model


09:43:59
and how is it used in generative AI? A Gaussian mixture model is a probabilistic model used in generative AI. It assumes that data points arises from a combination of multiple Gshian distribution. The image visualize the concept and the first shows a single Gshian distribution. The second shows a mixture of two gshian distributions. And the third row shows data point generated from various gshian mixture model configuration. Well, it is valuable in genative AI for task like clustering, density estimation, and generating


09:44:32
synthetic data. The graph demonstrates data generation using a gshian mixture model. After training on a data set, it can create a new data points by sampling from a learned gshian distribution. The graph compares a theoretical normal distribution to data points generated through Monte Carlo sampling from this distribution showcasing the accuracy of the generated data and capturing the underlying distribution. This technique is crucial in generative AI for task like creating synthetic data that


09:45:02
resembles real world data. Next question is can you describe the concept of a transformer model? The transformer model is a type of deep learning architecture that has transformed the field of natural language processing and more recently has been adapted to other domains like computer vision introduced in the paper attention is all you need in 2017. The transformer model moved away from traditional recurrent neural networks and convolutionary neural networks by relying entirely on a mechanisms called self attention. And


09:45:35
this graph shows the increasing number of the model parameters in generative AI models over time. As the number of parameters grows, these models gain the capacity to process and learn from more complex data leading to enhanced performance in task like text generation and image synthesis. However, this increase in parameters also comes with challenges such as increased computational cost and the risk of overfiltering where the model performs well on training data but poorly on new unseen data. The next question is what


09:46:06
are model parameters and how do they affect the performance of generative AI models? Model parameters are crucial components of generative AI models that significantly influence their performance and output quality. These parameters are the internal settings that a model learns from the training data and uses to make predictions, generate content or perform specific task. Process of optimizing model parameters in generative AI starts with importing training data and initializing the model's parameters. An optimization


09:46:37
mechanism is then applied to the train and update the model's parameters iteratively. This process continues until the defined closing conditions are met. Finally, the optimized parameters are obtained which are crucial for the model's performance in generating realistic outputs. Model parameters influence the content generation process impacting the output quality. These parameters are crucial for feature learning enabling the model to identify and utilize patterns in the data to generate a realistic and meaningful


09:47:07
outputs. And the next question is how can generative AI be leveraged to create personalized experience for users in e-commerce? Generative AI can significantly enhance personalized experience in e-commerce by tailoring content, recommendations, and interactions to individual users. Here's how it can be leveraged. First, in personalized product recommendation, AI powered recommendations personalize product suggestions by analyzing individual user data by understanding browsing history, purchase patterns, and


09:47:39
preferences. AI algorithms recommend products aligned with user interest, enhancing the shopping experience and driving sales. Next, customized marketing campaigns. AI enables personalized email marketing by analyzing user data to generate targeted content by delivering messages with relevant products and promotions tailored to individual interest. AI enhances campaigns effectiveness, increasing engagement and driving conversions. Next, tailor product descriptions and reviews. AI personalizes product description by


09:48:11
analyzing user data to highlight features most relevant to them. This enhances the shopping experience and increases purchase likelihood by providing targeted information. Next, virtual assistants and chat bots. AI powered chat bots function as virtual shopping assistants. They provide personalized product recommendation, answer user queries, and guide them through their shopping journey based on individual preferences. Next, visual search and augmented reality. AIdriven visual search allows users to upload


09:48:42
images of derived products. The AI then generates suggestions for similar items available in the e-commerce store, offering a highly personalized search experience. And by leveraging generative AI, e-commerce platforms can create a deeply personalized shopping experience that not only enhances customer satisfaction, but also increases engagement, loyalty, and conversion rates. And with this we have come to an end to this full course on generative AI. And if you enjoyed listening to this full course please be kind enough to


09:49:14
like it and you can comment on any of your doubts and queries. We will reply to them at the earliest. And do look up for more videos and playlist and subscribe to Edurea's YouTube channel to learn more. Thank you for watching and happy learning.

