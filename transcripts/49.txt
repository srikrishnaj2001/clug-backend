
00:00:04
Welcome to our generative AI full course by simpler. Generative AI is a type of artificial intelligence that focuses on creating new content whether that's text, image or even music by learning from existing data. This exciting journey will start with the basics of what generative AI is and machine learning are. We'll then explore how AI has evolved over time and look into future to predict the trends for 2025. You'll also learn about advanced technologies like deep learning, reinforcement learning and recurrent


00:00:33
neural networks deep into generative tutorials and explain complex models like transformers and LSTMs. This course will also guide you through AI career skills needed and project ideas for 2024. Plus, we'll also cover how to use these tools from hugging face and analyze popular models like OpenAI strategy. By the end of this course, you'll have a solid understanding of AI's current state and its future possibilities. Let's get started. Before we commence, if you want to enhance your career in AI, here's


00:01:04
some quick info for you. Check out Simply Learn's postgraduate program in AI machine learning in partnership with P University and IBM. This course is perfect for aspiring AI enthusiasts and professionals looking to switch careers. Gain expertise in generative AI, prompt engineering, chart GPD, and many more. A year of experience is preferred. So, hurry up and enroll now and find the course link mentioned in the description box and in the pin comment. So, let's get started. Liam, a 19-year-old


00:01:30
freshman, recently joined an Ivy League college to study history and political science. While reading about thinkers and scholars of the early 20th century, he stumbled upon a name, Alan Turing. Liam was fascinated by Allen and realized that the computer that he knows of today, Allen is considered to be the father of modern computer science that eventually led to the invention of his computer. But there was something that was even more fascinating about Allan. Although Alan Turing was famous for his


00:02:05
work developing the first modern computers, decoding the encryption of German Enigma machines during the Second World War. He also built a detailed procedure known as the Turing test, forming the basis for artificial intelligence. Liam had his mind blown by this fact. He realized that AI is not a modern phenomenon, but rather more than a thought experiment existing since the early 90s. Liam used AI tools like chat, GPT, Perplexity, and Consensus on a daily basis for his research. He had a smartphone that he used for multiple


00:02:45
tasks like using Siri or Google Assistant to find local food places using autocorrect on multiple apps like Instagram and WhatsApp and even AI photo editing features. He realized that AI has seeped into almost every aspect of his life. From making trivial decisions like where to have his morning coffee to complex AI tools like chat GPT for his research to even his father's self-driving Tesla that he used whenever he got a chance to. Artificial intelligence or AI in the 21st century has become a very subtle


00:03:25
technology that exists in every human's life without them even realizing it. But what is this AI? Does this mean robots in a completely dystopian AI warlord future? Not really. Let us dive a little deeper into understanding everything about AI. Artificial intelligence or AI is like giving computers the ability to think and learn much like humans do. Imagine teaching a friend how to solve puzzles and then that friend can solve different types of puzzles on their own. AI works similarly. It helps computers


00:04:03
understand and carry out tasks that typically need human intelligence. These tasks include recognizing faces and photos, chatting with us through smart assistants like Siri or Google Assistant, and even driving cars. Think of AI as a smart helper that makes our daily lives easier. It can learn from data, make decisions, and improve itself over time. This means that AI isn't just about robots taking over the world. It's more about using smart technology to assist us in various ways. making complex tasks simpler and


00:04:40
everyday routines smoother. AI has found its way into many areas of our lives, often making things easier without us even realizing it. In healthcare, for example, AI helps doctors by quickly analyzing medical images like X-rays to detect issues faster than the human eye might. In finance, AI works to keep our money safe by spotting unusual activities in our bank accounts that could indicate fraud. When you stream shows on Netflix, AI suggests movies and series based on what you've watched and


00:05:16
liked before. In retail, AI manages stock and predicts what items will be popular, ensuring that store shelves are filled with what customers need. Even in our homes, AI is at work through smart devices like thermostats that learn your schedule and adjust the temperature automatically or lights that turn on when you enter a room. AI touches so many parts of our daily lives, making things more convenient and efficient. One of the best AI applications today which is very widely known and used is chat GPT. An advanced AI developed by


00:05:55
OpenAI that can chat with you just like a human would. Imagine having a friend who knows almost everything and can help you with any question or topic. That's what chat GPT does. But how does it work? Chad GPT is powered by something called a transformer model. This is a type of machine learning model that learns patterns in language by looking at a vast amount of text data from books, websites, and other sources. Think of it like reading millions of books and remembering important information from all of them. When you


00:06:32
ask Chat GPT a question, it doesn't just pull out a random answer. Instead, it looks at the words you used, understands the context, and predicts what a good response would be based on what it has learned. For example, if you ask about the weather, it understands you are looking for current weather conditions and gives you relevant information. If you ask it to help with homework, it draws on its knowledge to explain concepts clearly. Chat GPT uses a process called deep learning which is a bit like how our


00:07:07
brains work. It breaks down sentences into smaller parts and looks at how these parts fit together. This helps it understand not just the meaning of individual words but also how they combine to convey a complete idea. This is why chat GPT can handle complex questions and give answers that make sense. to make sure it provides useful and accurate information. Chad GPT was trained on a diverse range of topics. This training helps it recognize and generate text on anything from science and history to entertainment and


00:07:46
daily life. It's like having an encyclopedia and a friendly tutor rolled into one. Similar to chat GPT, there are a plethora of other tools and applications being developed every day that are trained for various purposes using varied kind of data sets. For example, Doll E which has been trained on a diverse data set of text and images from the internet. Stable diffusion which has been trained on a variety of images and corresponding text descriptions. Tesla Autopilot, which has been trained


00:08:21
on sensor data from Tesla vehicles and driving data, and so on and so forth. AI is a remarkable technology that holds great promise for the future, offering solutions to some of the world's most pressing challenges. Imagine a future where AI takes care of routine tasks, giving us more time to be creative and focus on what we love. AI can help in many ways from improving medical treatments to making our daily lives more efficient. However, it's essential to use AI responsibly. This means creating


00:09:01
guidelines and rules to ensure AI is developed and used in ways that benefit everyone. By embracing AI and understanding its potential, we can look forward to a future where technology and human creativity go hand in hand. AI is not just about smart gadgets. It's about opening new possibilities and making our world a better place. The future of AI is bright, filled with opportunities for innovation and progress, helping us achieve things we never thought possible. Have you ever wondered if AI


00:09:38
could do more than just solve problems and crunch numbers? What if it could actually create entirely new things? Well, buckle up because that's exactly what generative AI is all about. Think about it. Music that sounds like your favorite artist, but it's totally original. Paintings so beautiful they could hang in a museum, all created by a machine. It's pretty mind-blowing, right? Generative AI is not just about understanding and replicating existing data. It's about creating new content


00:10:09
that can amaze and inspire. And guess what? This field is exploding. By 2025, experts predict that a whooping 10% of the data we see will be AI generated. That's a 10-fold increase in just 2 years. So, with generative AI set to revolutionize so many areas, let's dive in and explore what makes this technology so exciting. Today, we will uncover its mysteries, learn how it works, and discovers the incredible possibilities it sold for the future. Whether you're a tech enthusiast, a creative professional, or just curious


00:10:44
about the next big thing in AI, this journey into the world of generative AI is sure to captivate your imagination. And before commencing, just a quick info for you. All right. Now let's understand what is generative AI. So generative AI is a subset of artificial intelligence that focuses on creating new data rather than just analyzing or interpreting existing data. Unlike traditional AI which might predict the next word in a sentence or classify an image as a cat or dog. Generative AI can create


00:11:16
entirely new content. Think of it as an artist who doesn't just critique paintings but creates original masterpieces. This technology uses models like GNS that is generative adversarial networks and VA that is variational autoenccoders to generate text images and even complex simulations. It's like giving AI a creative spark allowing it to produce new unique outputs based on the data it has learned from. All right. Now some of the well-known generative AI models include open AI's chat GBT Deli and


00:11:51
Midjourney. These models have gained popularity for their ability to generate humanlike text and realistic images. For instance, Chad GBT can engage in detailed and nuanced conversations mimicking human dialogue impressively well. Dali can create stunning images from textual descriptions, conjuring up visuals that look as if they were made by a skilled artist. Similarly, Mid Journey focuses on producing highquality artistic images based on the prompts. But it doesn't stop there. AI can compose original music, too. Open AI's


00:12:25
Museet, for example, can generate songs in various styles from classical to pop, blending different genres. Imagine a computer creating a symphony that sounds like it was composed by Bid Hovven or a pop song that could top the charts. This ability to generate novel, highquality content makes generative AI truly revolutionary. All right, so this was about generative AI. Now let's understand about the components and architecture of generative AI. So the inner workings of generative AI involve


00:12:56
a complex interplay between different components. At the heart lies a generator responsible for creating entirely new data samples. It's like a powerful machine that can take existing information and use it to build something entirely new. But how does that generator know what to create? That's where another critical crucial component comes in. That is the discriminator. This acts as a quality check comparing the generative data to the real data and judges its accuracy and realism. It's essentially tells the


00:13:27
generator, "Hey, that doesn't quite look right. Try again." Think of it like this. Imagine an artist trying to paint a portrait based on what they have learned about faces. The generator is the artist constantly creating new variations. The discriminator is the art critic providing the feedback and pushing the artist to refine their work. This back and forth process allows generative AI to learn and improve its creative abilities. There are other architectures besides the adversial


00:13:57
approach. Some models use an encoder and a decoder. The encoder acts like a data compressor taking complex information and condensing it into a smaller form. The decoder then takes this compressed representation and uses it to build original data but with a twist. It can introduce variations and creates something entirely new. No matter the architecture, all generative AI models rely on two massive things. That is massive amounts of data to learn from any significant computational power to process and generate new content. It's


00:14:28
like having a vast library of knowledge and a superpowered computer at your disposal to fuel your creative output. All right. Now let's understand about how generative AI is different from AI. So while both generative AI and traditional AI fall under the broad umbrella of artificial intelligence, their functions and capabilities differ significantly. Traditional AI is primarily designed for specific tasks like classification, prediction, and optimization. It's excellent at interpreting existing data to make


00:14:57
decisions or predictions. For instance, AI can analyze customer data to predict buying behavior or classify images based on their content. On the other hand, generative AI goes a step further by creating new data. It doesn't just interpret the world, it builds upon it. Imagine it this way. A traditional AI model might identify a face in a photo, recognizing features, and comparing them to a database to say this is a cat or this is a dog. It's great at sorting and making sense of what already exist. Now,


00:15:26
picture generative AI as an artist. Instead of just recognizing a phase, it can paint a completely new face that has never existed before. It creates rather than just understands. All right. So this was about how Gen AI is different from traditional AI. Now let's have a look at its applications. So Gen AI applications in the world of creativity, generative AI is inspiring artists, musicians, and even storytellers. It can help create unique artwork, compose new music, or even craft story lines, all


00:15:58
with its own unique twist. Imagine a tool that can spark creative ideas and help bring them to life. Healthcare is another area where generative AI is making waves. It can create realistic simulated data to train over AI models used in medicine, which is crucial for protecting patients privacy. This same ability to create simulation can also be used to model complex structure which can aid in drug discovery and medical research. Next, the entertainment industry is jumping to the generative AI bandwagon too. From creating immersive


00:16:29
visual experiences to developing a cuttingedge technology, generative AI is shaping the way we experience entertainment. Imagine exploring fantastical worlds or interacting with characters in a whole new way. Even businesses are getting in the action. Generative AI is being used to create personalized marketing campaigns, tailoring content to individual users for a more engaging experience. It's like having a tool that can understand your preferences and suggest things you would actually be interested in. The


00:16:56
possibilities with generative AI are truly endless by creating entirely new things instead of just analyzing existing data. So these were some of the Gen AI applications. Now let's understand the challenges and ethical concerns. Generative AI is exciting but like any powerful tool it comes with its own set of challenges. So talking about the challenges one big concern is how it might be misused. So imagine creating a fake videos that look like so real they can trick anyone. That's the dark side


00:17:26
of deep fakes. There's also the worry of AI generating false information that spreads like the wildfire online. We need to be careful about how this technology is used. Another question mark is the ownership. Who owns the creative output of a machine? If AI writes a song, who gets the credit? There's a legal gray areas that need to be ironed out. Generative AI also isn't cheap. It requires a ton of data and powerful computers to run, which can raise environmental concern about the energy it consumes. Here's another thing


00:17:58
to consider. The data used to train these models can be biased. If the AI learns from biased information, it outputs could be biased too. By addressing these challenges responsibly, we can assure generative AI is a force for good that benefits everyone. It's all about using this powerful tool wisely as we explore its incredible potential. So there you have it. We have taken a tour of generative AI, exploring its potential to create entirely new things and push the boundaries of creativity. From revolutionizing


00:18:28
industries to sparkling artistic innovation, generative AI is a powerful tool with a bright future. Imagine a world where creativity knows no bounds. Where machines can conjure a part, music, and literature with the flick of a digital switch. This isn't the stuff of science fiction. It's the reality of generative AI. A cutting edge technology that's reshaping our digital landscape. Picture this. According to a recent report by Salesforce, generative AI tools are already in the hands of 27% of


00:18:57
millennials, 28% of Gen X, and a staggering 29% of Gen Z. These aren't just numbers. They are a testament to the growing influence of generative AI in our daily lives. And as organizations raise to harness its power, the demand for skilled generative AI experts is skyrocketing. But what exactly is generative AI? It's more than just lines of code. It's a gateway to infinite possibilities. With generative AI, machines can create anything from images to text to music, all by learning from


00:19:28
existing data sets. It's the technology behind deep fakes, virtual influencers, and even the next big hit song. So, why should you care about generative AI in 2024? Because it's not just the future, it's the present. It's the key to unlocking new realms of creativity, innovation, and opportunity. And in this video, we are going to show you how to become a master of generative AI in 2024. So buckle up for the world of generative AI because the future is here and it's more exciting than ever before.


00:19:57
So welcome to the road map of becoming a generative AI expert in 2024. So the number one reason is technological advancement. Generative AI represents a significant leap in the evolution of technology particularly in its ability to generate complex outputs like video, audio, text and images. This innovation is set to expand exponentially marking a new age of technological innovation. And the next reason is wide-ranging applications. The surge in interest and development in generative AI is fueled


00:20:27
by advancements in machine learning models. Artificial intelligence and platforms like chat GPT and B. These tools have broad applications across various sectors making knowledge in this area highly valuable. Now moving to the next reason that is solving complex problems. Generative AI has the potential to simplify problem solving processes significantly. Its capabilities in creating realistic models can be applied to innovate and enhance solutions across industries. And the next reason is impact on major


00:20:56
fields. The integration of artificial intelligence into major fields is undeniable and generative AI plays a substantial role in this transformation. It not only presents a threat to certain jobs but also opens up a plethora of new opportunities in the tech industry and beyond. And the next is dynamic and unexplored field. The field of generative AI is filled with challenges and unexplored territories offering an exciting frontier for those interested in shaping the future of technology. It calls for creativity, problem solving


00:21:26
skills, and a willingness to delve into the unknown. So learning generative AI in 2024 positions individuals at the forefront of technological innovation, equipping them with the skills and knowledge to contribute to significant advancements and explore new possibilities in the digital world. And now we'll move to the major skills required to learn generative AI in 2024. So to effectively learn and excel in generative AI in 2024, individuals need to possess a specific set of skills that are foundational to understanding and


00:21:55
applying this technology. So let's see what those skills are. So let's start with the skills and the number one skill is deep learning and fundamentals. A solid understanding of deep learning concepts is crucial. This includes familiarity with neural networks, back propagation and the various types of deep learning models and architectures. And the next is machine learning concepts. Proficiency in machine learning is repeat. Proficiency in machine learning is necessary encompassing a broad range of


00:22:22
algorithms, their applications and an understanding of how they can be used within generative AI frameworks. And then comes the Python programming. Python programming remains the dominant programming language in AI and machine learning. Mastery of Python includes its syntax, data structure, libraries such as TensorFlow and PyTorch and frameworks is essential. And the next skill is generative models knowledge. Specific knowledge of genetic models such as genative adversial networks GN and variational autoenccoders repeat and


00:22:53
variational autoenccoders VAE is required. Understanding how these models function and are applied to key to innovating within the generative AI space. And the next skill is image and text processing. Skills in processing and manipulating image and text data are necessary as many generative AI applications involve creating or modifying such content. And the next on the list is data processing and data augmentation. The ability to pre-process and augment data efficiently can significantly improve the performance of


00:23:22
generative models. Skills in data cleaning, augmentation techniques and feature engineering are vital. And then comes ethical considerations. With the power of generative AI comes the responsibility to use it ethically. Understanding the ethical implications of generative AI including issues of bias, fairness, and piracy is crucial. The next is communication. Given the interdisciplinary nature of generative projects, effective communication skills are essential for collaborating with teams, explaining complex concepts in


00:23:51
simple terms and engaging with stakeholders. So developing these skills will prepare individuals for the dynamic and evolving field of generative AI enabling them to contribute meaningfully to advancements in technology and address the challenges that come with it. So let's move to the road map to learn generative AI in 2024. And the road map is as follows. So the first step is understanding the basics of machine learning. Then comes mastering programming language that is mainly the Python. Then is the learning data


00:24:19
science and related technologies. And then we have hands-on realtime projects. And then learning mathematical and statistics fundamentals. And then on the list is developer skills. And then we have the important thing that is keep learning and exploring. So starting with the number one point that is understanding the basics of machine learning. So let's start by wrapping your head around the core machine learning algorithms. It's like getting to know the tools in your toolkit. Each has its unique use. Make sure to


00:24:47
understand the differences between supervised, unsupervised, and reinforcement learning. Think of them as different paths to solving a puzzle. Some are straightforward. Repeat. Some are straightforward while others need you to figure out the rules as you go. Get comfortable with handling data. After all, data is the fuel for your machine learning engine. Learn how to clean, spit, and pre-process it to get your models running smoothly. Learn how to evaluate your models with metrics like accuracy and precision. It's like


00:25:14
checking the health of your model to ensure it's fit for the real world. And the next step for your road map would be master Python programming. So focus on getting a strong grip on Python syntax and structure. Python is the language of choice in AI. So this is the learning. Repeat. So this is like learning the alphabet before writing stories. Dive into libraries essential for AI such as pandas for data manipulation and scikitlearn for machine learning. Think of these libraries as your shortcuts to


00:25:41
build powerful models. Practice writing efficient code. It's not just about getting the right answer. It's about getting there faster and cleaner. Engage with the Python community. It's a treasure trove of knowledge and a great way to stay updated on the latest trends and packages. And then comes the next step that is explore data science and related technologies. Sharpen your skills in data visualization. Visuals can reveal patterns and insights in data that numbers alone might not show.


00:26:06
Master feature engineering to transform row data into a format that machines can better understand and predict for. Get a handle on building machine learning pipelines. These are like assembly lines that take your raw data and eye on emerging technologies and frameworks in data science that complement generative AI. Staying updated will give you an edge in your projects. So now moving to the next step that is engage in hands-on realtime projects. So choose projects that spark on your interest and


00:26:32
challenge you. This is where you get to apply what you have learned and see your knowledge comes to life. Work with different generative AI models. Each project is a chance to deepen your understanding and refine your skills. Don't just build, evaluate, and iterate on your projects. Every iteration is a step closer to mastery. Document and present your work clearly. Sharing your journey not only helps others learn but also solidifies your own understanding. Now moving to the next step for your


00:26:58
road map is solidify your math and statistics fundamentals. Dive deep into linear algebra and calculus. These are the building blocks for understanding how AI models learn and make predictions. Understand probability and statistics. This is crucial for modeling uncertainty and making informed predictions. Learn about optimization techniques. These are the strategies your models use to improve over time like a person learning from the mistakes to get better. Now moving to the next step that is develop essential developer


00:27:26
skills. Get comfortable with AI development tools. These tools can make your work faster, more efficient, and more collaborative. Focus on debugging and testing. A model that works flawlessly in theory might face unexpected challenges in the real world. Embrace ethical AI development. It's important to ensure your AI solutions are fair, accountable, and transparent. And then comes the keep learning. The field of AI is always evolving and staying curious is the key. So this was for the development of essential


00:27:54
developer skills. Now coming to the next step that is commit to continuous learning and exploration. So participate in AI communities. These are great spaces for learning from others experiences and sharing your own. Make reading research papers, blogs and books a habit. There are windows to the latest advancements and theories in AI. Attend workshops and conferences. These events can inspire you and expose you to the new ideas and technologies. Seek mentorship or collaborate on projects. Learning from others can accelerate your


00:28:22
growth and open new paths. So this was the road map for learning generative AI in 2024. Artificial intelligence is almost certain to remain the most discussed technology in 2025 influencing everything from boardrooms and classrooms to hospitals and homes. It will continue to become more deeply integrated into our daily lives. As AI presence grows, so does the need to address the ethical concern related to its impact on the human and society. Many critical questions remain unanswered. How will AI affect


00:28:53
employment? Will it take over human creativity? And what are the privacy implications of the increasing need to collect personal data to fuel AI algorithms in the digital age? In 2025, we might begin finding answer to some of these questions. We will also likely see amazing breakthroughs that would have seemed impossible just a few year ago. So, here is the list of what I think will be the most important topics and discussion about AI and automation in the next year. So hi, I am M. Welcome to this video on the top 10 AI trends for


00:29:24
2025. So in this video, we will talk about AI trends and automation for 2025. So without any further ado, let's get started. So number one is responsible AI. In 2025, there will be a growing emphasis on developing AI in the way that are ethical, secure, transparent, reliable, and respectful of intellectual property. Some of this will be driven by new laws. But there is also an increasing awareness of the risk of using AI irresponsibly. Issues like AI bias and false information are now widely recognized and reducing these


00:29:57
risk requires effort. Businesses that ignore these concern in 2025 could face negative press, regulatory scrutiny and customer dissatisfaction. Number second, nextgen voice assistant. AI voice assistant like Siri and Alexa have been part of our lives for a while but their conversational abilities have been eliminated. This year OpenAI introduced a more advanced voice mode for chaty capable of holding humanlike conversation. Google is also integrating its Gemini chatbot into mobile devices replacing the outdated hey Google


00:30:31
feature. By 2025, we can expect these improvements to be available in more devices, enabling more natural voice interaction. Number third, we have generative video. Imagine typing out a movie idea or at least a short video and watching it comes to life. Think of it as sharpity for videos. Open AAI has already shown a glimpse of this with its Sora model and by 2025, we could see the technology becoming more accessible. While we won't be making Hollywood level films just yet, this gives an exciting


00:31:05
preview of where generative AI could be headed. Number fourth, we have augmented working. This year, almost every major software tool quickly added generative AI features as no one wanted to fall behind. In 2025, I expect more thoughtful approaches to how humans and AI can work together to enhance our technical skills while giving us more time to focus on creative and social task that machines still can't handle. Instead of just adding chat bots everywhere, smart businesses will start using AI in ways that create real value.


00:31:40
The next we have realtime automated decision making. As businesses better understand the strategic use of AI, those with more developed AI strategies will move toward automating entire business processes. This will likely happen in areas like logistics, customer service, and marketing where algorithm will make decisions such as managing inventory or responding to customer inquiries with minimal human involvement. This will lead to faster reaction to changes in customer behavior or market condition and decreased


00:32:12
efficiency. And number six, we have predictive AI analytics. So predictive AI analytics will become even more powerful and accurate in 2025. It uses machine learning to examine large amounts of data and predict future trends. Businesses will use it to forecast customer behavior, market shift, and potential risk. In healthcare, it can help predict diseases, outbreaks, or patient outcomes. While retailers will optimize inventory by forecasting demand. By 2025, more industries will adopt predictive AI leading to quicker, more


00:32:43
precise and datadriven decisions. This technology will be key for staying competitive in an increasingly digital world. Number seven, we have autonomous AI agents. Today's AI tool generally perform simple tasks like generating text or analyzing data to make prediction. However, AI agents can carry out multiple tasks and adjust based on the results without needing precise instruction. This could be a step toward journal AI that can handle a wide range of task. However, this also brings up the question about the need for


00:33:13
oversight and accountability. Next, we have virtual agents. Virtual agents are AI system designed to mimic human conversation helping user with tasks like answering question or managing schedules. Example includes Siri and Alexa which are becoming more advanced able to understand context and emotion better. By 2025 virtual agents will be more personalized and easier to use changing industry like customer service and healthcare and even more. These agents will make everyday interaction quicker and more efficient. The next we


00:33:44
have AI in cyber security and defense. As cyber attacks become more frequent and sophisticated in 2025. AI will be essential for detecting breaches, spotting unusual activity and automating system to handle threats before they cause damage. AI will also help combat fishing and social engineering attacks by simulating these threats and teaching users how to avoid them. Last we have sustainable AI. So there are two aspects of sustainable AI. First, as cloud-based system consume more massive amounts of


00:34:15
power, there will be a push to shift data center to renewable energy sources. Second, AI can help other industries becoming more sustainable by for example reducing water and pesticide use in agriculture or managing traffic to cut down on vehicle emissions. So in 2025, AI will further establish itself as a key tool for environment production. In conclusion, AI technologies in 2025 are expected to bring continuous changes reshaping industries and daily life. In the coming years, AI will be smoothly


00:34:47
integrated into everyday task. From generative AI boosting creativity to autonomous vehicle transforming transportation, AI will become a natural part of daily routines. Hey there, welcome to the world of artificial intelligence. It's all around us. From Siri or Alexa that wakes us up to the self-driving cars that are no longer just a sci-fi dream. But have you ever stopped to wonder how we got here? How did AI evolve from a mere concept to something that's transforming our lives daily? And more importantly, where is it


00:35:21
headed? Is AI our future savior or a harbinger of doom? Let's start with a cool example of AI in action. Imagine a tired doctor named Sara working late at night. A sick child comes in with weird symptoms. Sara can't figure it out, but then she uses the hospital's new AI helper. She types in the symptoms and the AI quickly looks at thousands of similar cases. It suggests a rare genetic problem. Sara does the right test and soon the child is getting life-saving treatment. This isn't from a


00:35:55
movie. It's happening now. Artificial intelligence or AI is advancing rapidly and it's poised to change our lives in a way we can barely imagine. In this video, we will explore the different stages of AI from the simplest system to the most advanced and how these stages are shaping our present and how will it impact our future. Let's begin our journey through the stages of AI. So let's start with our first stage which is rulebased AI. Our journey begins with rule-based AI, the earliest form of AI.


00:36:25
This type of AI operates a predefined set of rules that doesn't have the ability to learn or adapt. Imagine an alarm clock that rings at 7:00 a.m. in the morning or a thermostat that turns on the air conditioner when the temperature hits 75Â° F. These devices work because they follow a simple set of rules by humans. In the 1950s, the idea of AI first started when people began asking, can we make machines that can think like humans? However, early AI was pretty basic. It could only follow these


00:36:58
set of rules. A major milestone was achieved in 1997 when a computer named Deep Blue defeated the world's best chess player, Gary Kasparov. While impressive, Deep Blue wasn't really thinking. It was just following a set of rules programmed by humans. As technology evolved, so did AI, moving from simple rules to more sophisticated systems. Let's talk about the second stage, which is contextbased AI. This stage represents a significant leap from rulebased systems as these AI systems can take into account the surrounding


00:37:33
environment and past interactions. Examples include Siri, Google Assistance, and Alexa. These AI systems analyze vast amounts of data, recognize patterns, and provide personalized experiences based on context. For example, if you talk about the weather and it's likely to rain, these AI assistants might suggest carrying an umbrella. or if you can ask for a recipe, they might suggest a nearby store where you can buy the ingredients taking into account your past purchases. With the ability to retain and use


00:38:05
context, AI became even more powerful as it moved into specialized area. Then we have narrow domain AI. The next stage is narrow domain AI where AI systems are designed to master specific task often surpassing human capabilities within their designated fields. For instance, IBM's Watson has been used in the medical field to analyze patient records and provide potential diagnosis. In finance, narrow AI can predict stock movements with high accuracy by analyzing market trends. In gaming, Alph


00:38:37
Go by Deep Mind is notable experience. It mastered the complex board game Go and defeated world champions, showing how narrow AI can excel in a specific domain. However, these systems are not generalists. They are designed to excel in particular task only. As AI systems become more specialized, the next stage took AI capabilities even further. Let's talk about the stage four which is reasoning AI. At this stage, AI begins to simulate complex thought processes similar to how humans think. Reasoning


00:39:09
AI doesn't even process data. It analyzes it, connects patterns, and make logical conclusions. A prime example is OpenAI Start GPT, a large language model trained on vast amounts of text. It can generate humanlike responses, answer questions, and even engage in meaningful conversations. Another example is autonomous vehicles, which uses reasoning AI to make split-second decisions, ensuring the safety of passengers and pedestrians. With reasoning capabilities, AI was getting closer to human-like intelligence


00:39:42
leading to the next stage of development, which is artificial general intelligence. We enter the realm of artificial general intelligence. At this stage, this stage represents AI that can perform any intellectual task and act like a human. AGI is not limited to specific task. It has versatility to learn and adapt to new challenges. Imagine waking up to a virtual assistant that can understand your mood, help you plan your day, give suggestions for your work, and even assist with cooking. AGI would be like teaching a human. But it


00:40:14
can learn thousands or millions of times faster. With AGI, the possibilities are endless. From advanced virtual assistant to AI systems that could potentially merge with human through brain computer interfaces. But the development of AI doesn't stop at AGI. There's potential for even greater advancement. We are talking about the super intelligent AI. Beyond AGI lies super intelligent AI. These AI systems could improve themselves without human input leading to exponential growth in intelligence.


00:40:45
Super intelligent AI could solve complex global issues like climate change, create innovations beyond our imagination, and introduce technology that might seem like science fiction today. Imagine an AI system that could be trillions of times more intelligent than all humans combined. The pace of innovation would be staggering, compressing thousands of years of technological advancements into just a single century. As AI continues to advance, it could reach a stage where it becomes self-aware. Let's talk about


00:41:15
stage seven, which is self-aware AI. Now, imagine an AI that is not just intelligent but also self-aware. Self-aware AI could understand its existence, have emotions, and perceive the world in ways that humans cannot. It could make decisions and evolve in directions that are beyond human control or comprehension. The idea of a conscious AI raises many ethical questions. How would such a being view humanity? What kind of relationship would we have with it? These are the questions we would need to consider as


00:41:47
we move closer to the stage of AI development. Beyond self-awareness, AI could evolve to transcend current human understanding. We talking about transcendent AI. This AI could create new forms of life, whether biological, digital, or something entirely new. It might use nanobots to repair ecosystem or even terraform the planet. By connecting the consciousness of multiple entities, transcendent AI could achieve a state of shared awareness and collective intelligence. As AI becomes more advanced, it could even venture


00:42:19
into the cosmos. We're talking about the cosmic AI. Cosmic AI could be the next step in interstellar exploration. This AI could handle challenges like cosmic radiation, extended isolation, and the vast time spans of space travels. Cosmic AI might merge with the universe itself, gaining a deep understanding of every aspect of reality from the quantum to the cosmic scale. It could explore higher dimensions, harness energy from stars and black holes, and create a network of intelligence across galaxies.


00:42:52
This AI could unify the cosmos, sharing knowledge and understanding on a scale beyond human imagination. Finally, we reach the most speculative and advanced stage of AI development, which is godlike AI. This godlike AI is all about knowing, powerful, and present everywhere. It could operate across multiple universes, assessing realities we can't even fathom. It might understand and influence the fabric of time, create new universe, or explore timeless beyond our comprehension. Godlike AI could continually craft new


00:43:26
experiences for itself or other conscious beings, potentially achieving a level of existence that transcends anything we can currently imagine. But with all these possibilities, we also need to think about the challenges that comes with AI. We need to make sure AI is developed and used in the right way. It's important that AI doesn't create unfairness or inv privacy. There have been times where AI made mistakes in things like hiring which shows why it's important to be careful and fair.


00:43:55
Another big challenge is jobs. AI can do repetitive task which might take away some of the jobs but it can also create new opportunities in areas we haven't even thought of yet. The key is to make sure that people have the skills they need for these new jobs. Despite these challenges, AI is also making incredible progress in solving tough problems. AI is also being used to solve some of the toughest problem in science. For example, OpenAI's chart GPT is now helping researchers and developers


00:44:23
create new tools applications from generating humanlike text to aiding into complex decision making. This shows how versatile and powerful AI can be in tackling real world challenges. And the best part is we have to say in how AI is used in the future. We get to decide how to use AI. It's like we are writing the future together. To make sure AI helps everyone, we need to make good rules, do more research to keep AI safe and teach people about AI so that we can make smart choices. As we journey through


00:44:54
these stages of AI, we see how technology could advance in ways that are both exciting and challenging. From simple rule-based systems to the potential of godlike AI, the possibilities are endless. But with these advancements come important questions about ethics control and the future of humanity. Open AI is one of the main leaders in the field of generative AI with its stat being one of the most popular and widely used examples. Chat GPT is powered by OpenAI's GP family of large language models LLMs. In August and September


00:45:27
2024, there were rumors about a new model from OpenAI, code name Strawberry. At first, it was unclear if it was the next version of GPT40 or something different. On September 12, OpenAI officially introduced the O1 model. Hi, I am M. In this video, we will discuss about OpenAI model 01 and its types. After this, we will perform some basic prompts using OpenAI preview and OpenAI mini. and at the end we will see comparison between the open models and GP4o. So without any further ado let's get started. What is open? The open


00:46:01
family is a group of LLMs that have been improved to handle more complex reasoning. These models are designed to offer a different experience from GP40 focusing on thinking through problems more thoroughly before responding. Unlike older models is built to solve challenging problems that require multiple steps and deep reasoning. OpenAI O1 models also use a technique called chain of thought prompting which allows the model to think through problem step by step. OpenAI1 consists of two models 01 preview and 01 mini.


00:46:33
The O1 preview model is meant for more complex task while the O1 mini is a smaller more affordable version. So what can OpenAI1 do? Open A1 can handle many tasks just like other GPT models from OpenAI such as answering questions, summarizing content and creating new material. However, O1 is especially good at more complex task including the first one is enance using. The O1 models are designed for advanced problem solving particularly in subjects like science, technology, engineering and math. The


00:47:04
second one is brainstorming and ideation with its improved reasoning. O1 is great at coming up with creative ideas and solution in various field. The number third is scientific research. O1 is perfect for task like anoting cell sequencing data or solving complex math needed in areas like quantum optics. The number fourth is coding. The oven models can write and fix code performing well on coding tests like human event and code forces and helping developers build multi-step workflows. The fifth one


00:47:34
mathematics. 01 is much better at math than previous model scoring 83% in the international mathematics olia test compared to GPT4's 13%. It also did well in other math competition like a IME making it useful for generating complex formulas for physics and the last one is self facteing can check the accuracy of its own responses helping to improve the reliability of its answer. You can use open models in several ways. Chat GBD plus and team users have access to O1 preview and O1 mini models and can


00:48:07
manually choose them in the model picker. Although free users don't have access to the O1 models yet, OpenAI planning to offer O1 mini to them in the future. Developers can also use these models open API and they are available on third party platform like Microsoft is your AI studio and GitHub models. So yes guys, I have opened this uh CHP 40 model here and CHGP1 preview as you can see. So I have this plus model. Okay, the paid version of CH GPD. So I can access this 01 preview and 01 VD mini


00:48:43
model. Okay, we will go with 01 preview model and we will put same prompts in both the model of the chat GPD 40 and the 01 preview and see what are the differences are coming. Okay. So we will do some math questions and we will do some coding. We will do some advanced reasoning and quantum physics as well. Okay. So let's start with so I have some prompt already written with me. So first one is number theory. Okay. So what I will do I will copy it from here and paste it in this and both. Okay. So let me run in for


00:49:31
and preview. So here you can see it's thinking. Okay. So this is what I was saying chain of thoughts. Okay. So these are the chain of thoughts. First is breaking down the primes. This is and then is identifying the GCD. And now see the difference between the output. See output is 561 is not a prime number and the GCD greatest comma DC or 48 and 180 is 12. Okay. Here see 01 preview is giving the output in step by step. First see determine if 561 is a prime number or not. The number 56 is not a prime number. Is composite number


00:50:17
because it has this this this. Okay. Then second step then the greatest common divisor. Then they found 12. And answer is no. 561 is not composite number because of this. And the greatest common divisor of 48 and 18 is 12. See just see the difference between the two models. This is why chupity 01 models are crazy for math coding and advanced reasoning quantum physics for these things. Okay. So let's go with our second step. So here if you will see you can see the attach file option in charge 40. Okay.


00:50:59
You can come upload from your computer. But here you we will see in 01 there is no attach file option. This is one drawback. Okay. So here upload from computer. So this is one small. Okay. And let me open this and this is the question I have. Okay. Yeah. So I will copy this. I will run this and this. Okay, see it's start giving the answer and O1 is still thinking solving the equation then solving analyzing the relationship. Okay, so charge 01 will take time but it will give you more accurate more step by step as you want.


00:51:49
Okay. So here you can see solve for X's question. This is this and here the steps you can see. Okay, this is more structured way you can see in a good structure way. Okay, chat GP1 preview give you in good structure way as O1 mini as well. Okay. So yeah. So here they wrote just one and two this is this and here if you'll see question one solve for x in this and step one is this step two is this and step three is this then the answer of x equals to three but here simply the wte we know this is this


00:52:31
and x= to 3. For the second question, see expanding the left hand side. This this is but here step one square both sides of the given equation. Start by squaring both side. Okay. It's written but not in good way. Okay. So this is why is better for math. Okay. So now let's check it for the coding part. Okay. So I have one question. Okay. Let me see what output it will give to first I will write I need. Okay, leave it. I will copy it and I will copy it as well here. Run it and run it. See, it's start giving answer.


00:53:17
Okay. And still this will adjust the parameters ensuring the code generation because JGPT01 will think first then it will analyze then after that it will give you answers. Okay here the code is done. See here the code is done and it's still thinking. Step one and first here you can't see anything. See step setup development environment pip install numpy plot li then this then this and here nothing and but I will ask it okay give me code in one tab okay here also give me code [Music] in in


00:54:18
single step. Okay. So I can just copy and paste. So what I will do I will open one online compiler and I will directly copy it and paste. Okay. So let's finish this. I hope it will work. So let me open W3 schools compiler. Okay. Yeah. Same I will open for this W3 school. Okay. So let me copy the code and my bad. and paste it here. Same for works for this. Okay. Okay. So, I will copy the code and I will paste it here. Okay. I hope. Okay. It gives something. Yeah. Cool. So, yes, now you can see the difference


00:55:35
between the output. So this is the output of 40 and this is the output of 01 preview. See 01 preview output is this and this is the output out output of 40. So this is the difference. This is why 01 takes time but it will give you more accurate result in a good way. Okay. So now let's check something else. So moving on, let's see some advanced reasoning question. Okay. So this is the logical puzzle one. The first one. Okay. So I will copy it and I will paste it here. Okay. This is for O1. This is for


00:56:22
preview. Why I'm not comparing 01 with mini? Because there both are same but slightly difference is there. Okay. So here we can see more difference between for old model versus new model you can say. Okay. So now see the answer is end in this much only but it will explain you in a better way. See thoughts for 7 seconds explanation that case one then case two. Okay, with conclusion in both scenario summary and this here this one small explanation and that's it right. So they created 01 preview for more you


00:57:11
know it will describe you more in a better way right now let's see some scientific reasoning as well okay so let me copy it here this is still thinking but start giving answer She thought for 16 seconds. So again I will say that you know chity 01 is much better than chutity 4. Chargity 4 is great for you know content writing and all but chargity 01 preview and mini are very good for reasoning math coding or quantum physics these type of things. Okay. Advanced reasoning. Okay. Charge 40 is great for you know generative


00:58:05
text. Okay. Like for marketing, writing copies, emails and all of those. So now let's see some comparison between open models and GPT4 model. When new models are released, their capabilities are revealed through benchmark data in the technical reports. The new OpenAI model excel in complex reasoning task. It surpasses human PhD level accuracy in physics, chemistry, biology on the GPQA benchmark. Coding becomes easier with O1 as it ranks in the 89th percentile of the competitive programming questions


00:58:35
code force. The model is also outstanding in math on a qualifying exam for international mathematics IMO GPD4 solved only 13% of problems while O1 achieved 83%. This is truly next level. on the standard ML benchmarks. It has huge improvements across the board. MMLU means multitask accuracy and GPQA is reasoning capabilities. Human evaluation. OpenAI asked people to compare O1 mini with GPT4 on difficult open-ended task across different topics using the same method as the O1 preview versus GPT 40 comparison like O1


00:59:12
preview. O1 mini was preferred over GPT4 for task that requires strong reasoning skills but GPT4 was still favored for language based task model speed. As a concrete example, we compared responses from GPT4 mini and preview on the word jing question while GPT4 did not answer correctly. Both mini and preview did and mini reach the answer around 3 to 5x faster. Limitation and what's next? Due to its specialization on STEM, science, technology, engineering, and math reasoning capabilities, open Mini's


00:59:47
factual knowledge on non- STEM topics such as dates, biographics, and trivia is comparable to small LMS such as GPT4 mini. Open AI will improve these limitation in future version as well as experiment the extending the model to other modalities and facilities outside of STEM. Do you know artificial intelligence is transforming industries across the globe, creating a wealth of career opportunities for those ready to embrace the future. Take Elon Musk for example. He is known for his work with Tesla and SpaceX and he co-founded


01:00:21
OpenAI, an organization dedicated to ensuring that AI benefits all the humanity. Musk transitions into AI underscores the massive potential of this field. Not just the tech enthusiast but for anyone willing to innovate and adapt. Imagine this. In the tech city of Hyderabad, India, Arjun sits at his desk, eyes focused on his computer screen. Just two years ago, he was a new computer science graduate working as a junior software developer at a small startup. His salary was modest and his career prospects seemed limited. But


01:00:58
everything changed when he discovered the booming field of artificial intelligence. Arjun spent his free time learning Python, exploring statistics, and experimenting with AI models. Fast forward 18 months, his hard work paid off. He landed a job as an AI engineer at a major tech company in Bengaluru, tripling his salary from 6 lakh to 18 lakhs per year. More importantly, Arjun found himself at the forefront of technology, working on projects that are shaping the future. Arjun's story is just one example of how AI transforms


01:01:32
careers in India. Across the country, professionals are seizing new opportunities in AI as companies invest heavily in this revolutionary field. But entering AI isn't easy. It requires dedication, continuous learning, and adaptability. In this guide, we will explore AI career paths, the skills you need, and what it is like to work in this dynamic field. So, let's talk about is AI is a good career or not. You have probably heard a lot about artificial intelligence or AI. It's everywhere and


01:02:03
it's shaking up industries all over the world. But here's the big question. Is AI a good career choice? Yes, absolutely it is. Take Elon Musk for example. We all know him as the guy behind Tesla and SpaceX. But did you know he also co-founded OpenAI? Even Alons diving into AI and that just shows how massive this field is becoming. And guess what? AI isn't just for tech geniuses. There's room for everyone. Let's talk about numbers. AI jobs are growing like crazy, up to 32% in recent years. And the pay


01:02:40
is pretty sweet with roles offering over $100,000 a year. So whether you're into engineering, research, or even the ethical side of the things, AI has something for you. Plus, the skills you pick up in AI can be used in all sorts of industries, making it a super flexible career choice. Now, AI is a big field and there are tons of different jobs you can go for. Let's break down some of the key roles. First up, we have machine learning engineers. These folks are like the backbone of AI. They build


01:03:12
models that can analyze huge amounts of data in real time. If you've got a background in data science or software engineering, this could be your thing. The average salary is around $131,000 in the US. Then there's data scientist. The detectives of the AI world. They dig into data to find patterns that help businesses make smart decisions. If you're good with programming and stats, this is a great option and you can make about $105,000 a year. Next, we've got business intelligence developers. They are the


01:03:46
ones to process and analyze data to spot trends that guide business strategies. If you enjoy working with data and have a background in computer science, this role might be for you. The average salary here is around $87,000 per year. Then we've got research scientist. These are the ones pushing AI to new heights by asking innovative questions and exploring new possibilities. It's a bit more academic, often needing advanced degrees, but it's super rewarding with salaries around $100,000. Next up, we have big data


01:04:22
engineers and architects. These are the folks who make sure all the different parts of business's technology talk to each other smoothly. They work with tools like Hadoop and Spark and they need strong programming and data visualization skills. And get this, the average salary is one of the highest in AI around $151,000 a year. Then we have AI software engineer. These engineers build a software that powers AI application. They need to be really good at coding and have a solid understanding of both


01:04:56
software engineering and AI. If you enjoy developing software and want to be a part of the EI revolution, this could be your role. The average salary is around $108,000. Now, if you're more into designing systems, you might want to look at becoming a software architect. These guys design and maintain entire AI system, making sure everything is scalable and efficient. With expertise in AI and cloud platforms, software architects can earn hefty salary about $150,000 a year. Let's not forget about


01:05:28
the data analyst. They have been around for a while, but their role has evolved big time with AI. Now they prepare data for machine learning models and create super insightful reports. If you're skilled in SQL, Python, and data visualization tools like Tableau, this could be a great fit for you. The average salary is around $65,000, but it can go much higher in tech companies. Another exciting roles is robotics engineer. These engineers design and maintain AI powered robots from factory robots to robots that help


01:06:02
in healthcare. They usually need advanced degrees in engineering and strong skills in AI, machine learning and IoT, Internet of Things. The average salary of robotics engineer is around $87,000. With experience, it can go up to even more. Last but not the least, we have got NLP engineers. NLP stands for natural language processing. And these engineers specialize in teaching machines to understand human language. Think voice assistants like Siri or Alexa. To get into this role, you'll need a background in computational


01:06:36
linguistics and programming skills. The average salary of an NLP engineer is around $78,000, and it can go even higher as you gain more experience. So, you can see the world of AI is full of exciting opportunities. Whether you're into coding, designing systems, working with data, or even building robots, there's a role for you in this fastest growing field. So, what skills do you actually need to learn to land an entry- level AI position? First off, you need to have a good understanding of AI and machine


01:07:08
learning concepts. You'll need programming skills like Python, Java, R. And knowing your way around tools like TensorFlow, and PyTorch will help you give an edge, too. And do not forget about SQL, pandas and big technologies like Hadoop and Spark which are super valuable. Plus, experience with AWS and Google Cloud is often required. So, which industries are hiring AI professionals? AI professionals are in high demand across a wide range of industries. Here are some of the top sectors that hire AI talent. Technology


01:07:42
companies like Microsoft, Apple, Google, and Facebook are leading the charge in AI innovation. Consulting firms like PWC, KPMG and Accenture are looking for AI experts to help businesses transform. Then we have healthcare. Organizations are using AI to revolutionize patient with treatment. Then we have got retail. Giants like Walmart and Amazon leverage AI to improve customer experiences. Then we have got media. Companies like Warner and Bloomberg are using AI to analyze and predict trends in this media


01:08:15
industry. AI is not just the future, it's the present. With right skills and determination, you can carve out a rewarding career in this exciting field. Whether you're drawn to a technical challenges or strategic possibilities, there's a role in AI that's perfect for you. So, start building your skills, stay curious, and get ready to be a part of the air revolution. The world is becoming increasingly competitive, requiring business owners or individual to find new ways to stay ahead. Modern


01:08:44
customers or individuals have higher expectations demanding personalized experience, meaningful relationships and faster responses. Artificial intelligence is a gamecher here. AI helps promote goods and services or make your life easy with minimal effort and maximum result allowing everyone to make faster, better informed decisions. However, with so many AI tools available, it can be challenging to identify the best ones for your needs and productivity boost. So here are top 10 AI tools in 2024 that can transform


01:09:17
your business or boost your productivity. On the number 10 we have tome is a tool that can help you share your thoughts and ideas quickly and effectively. Unlike other methods such as making a slide deck or building a web page, toms let you create engaging and detailed presentation in just a minute. You can enter any topic or idea and the AI will help you to put together a presentation that look great and gets your message across. It's like getting the ideas out of your head and into the world all without sacrificing quality.


01:09:48
With tome you can be sure that your presentation will be the both fast and effective. And ninth on the list is Zapier. Zapier is a popular web automation tool that connects different apps allowing user to automate repetitive task without coding knowledge. With Zapier you can combine the power of various AI tools to supercharge your productivity. Zapia supports more than 3,000 apps including popular platform like Gmail, Slack, and Google Sheets. This versatility makes it a valuable tool for individual teams and


01:10:19
businesses looking to streamline their operation and improve productivity. And also with 7,000 plus integration and services offering, Zapier empower businesses everywhere to create processes and systems that let computers do what they are best at doing and let humans do what they are best at doing. After covering Zapia, number eight on the list is Gravity Wright. Gravity Wright is an AI powered writing tool that transform content creation. It generates high quality SEO optimized content in over 30 languages catering to


01:10:51
diverse need like blog post, social media updates, ad copies and emails. These tools ensure 100% original plaggarism free content safeguarding your brand's integrity. Its AI capabilities also include text to email generation enhancing visual content for marketing purposes. The tool offers both free and paid plans making it versatile for freelancer, small business owner and marketing teams. On the seventh number we have audio box. Audio box is advanced AI tool developed by Meta designed to transform audio production. It allow


01:11:24
user to create custom voices, sound effect and audio stories with simple text prompts using natural language processing. Audio box generate highquality audio clips that can be used for various purposes such as text to speech, voice mimicking and sound effect creation. Additionally, audio box offer interactive storytelling demos enabling user to generate dynamic narratives between different AI bosses. This tool is particularly useful for content creator, marketers and anyone needing quick highquality audio production


01:11:54
without extensive manual effort. And next on number six, we have AOL. AOL is advanced AI power tool tailored for e-commerce and marketing professionals. It offers comprehensive suit of feature designed to streamline content creation and enhance personalization. With a cool user can generate customized text, images, voice and videos making it an invaluable asset for creating engaging product videos and marketing materials. Key feature of a cool include face swapping, realistic avatars, video transition and talking photos. These


01:12:26
tools allow businesses to create dynamic and personalized content that can captivate audience on social media and other platform. A cool's user-friendly interface and intelligent design make it easy for user to produce highquality content quickly and efficiently. On number five, we have 11 Labs. 11 Labs is a leading AI tools for X-to speech and voice cloning. Known for its highquality natural sounding speech generation, the platform includes features like voice lab for creating or cloning voices with


01:12:56
customizable options such as gender, age, and accent. Hey there. Did you know that AI voices can whisper or do pretty much anything? Ladies and gentlemen, hold on to your hats because this is one bizarre site. We have reports of an enormous fluffy pink monster strutting its stuff through downtown. Fluffy bird in downtown. Weird. Um, let's switch the setting to something more calming. Imagine diving into a fast-paced video game. Your heartbeat sinking with the story line. I got to go. The aliens are


01:13:31
closing in. That wasn't calming at all. Explore all those voices yourself on the 11 Labs platform. Professional voice cloning supports multiple language and needs around 30 minutes of voice samples for precise replication. The extensive voice library offers a variety of profiles suitable for podcast, video narration, and more. With various pricing plans ranging from free to enterprise level, 11 labs cers to individual creators and large businesses alike. Standing out for its userfriendly interface and superior voice output


01:14:03
quality. At number four, we have Go enhance. Go enhance AI is an advanced multimedia tool designed to revise video and image editing. It leverages powerful AI algorithm to enhance and upscale images transforming them into high resolution masterpiece with extreme detail. The platform standout feature video to video allow user to convert standard video into various animated style such as pixel art and anime giving a fresh and creative touch to otherwise ordinary footage. This AI tool is ideal for social media content creator,


01:14:38
marketer, educator, and anyone looking to bring their creative vision to life. Whether you need to create eye-catching marketing materials or professional grade videos, Go enhance AI provides the resources to do so efficiently. At number three, we have Ptory. Ptory AI powered tool designed to streamline video creation by transforming various content types into engaging visual media. It excels in converting textbased content like articles and script into compelling videos making it ideal for content marketers and educators. Users


01:15:11
can also upload their own images and videos to craft personalized content. The platform featured AI generated voiceovers which add a professional touch without the need for expensive voice talent. Victoria AI offers a range of customizable templates simplifying the video production process even for those with no design skills. Additionally, its unique textbased video editing capability allow user to repurpose existing content easily creating highlights or short clips from the longer videos. At number two, we


01:15:41
have Nvidia broadcast. It's a powerful tool that can enhance your video conferencing experience. Whether you are using Zoom or Teams, it can address common challenges like background noise, poor lightning, or lowquality audio video. With this software, you can improve audio quality by removing unwanted noise such as keyboard clicks or fan sound. It also offers virtual background option and blurring effect without needing a green screen. So you can seamlessly integrate it with other application like OBS, Zoom, Discord or


01:16:12
Microsoft Teams. Think of it as having a professional studio at home. Plus, it's a free for Nvidia RTX graphic card user. Visit the website to learn more and start using it today. After covering all the tools, at number one we have Taplio. Taplio is an AI powered tool designed to enhance your LinkedIn presence and personal branding. It leverages artificial intelligence to create engaging content, schedule post and provide insight into your LinkedIn performance. Taplio's main feature include AI powered content inspiration,


01:16:44
a library of viral post and a robust post composer foruling and managing LinkedIn content efficiently. Tableau also offers easy to understand LinkedIn analytics to help user make informed decision based on their performance data. A free Chrome extension provides a quick overview of performance metrics directly on linkedin.com making it a convenient tool for daily users. There you have it, top 10 AI tools that are set to transform your life in 2024. Whether you are developer, content creator, or someone looking to boost


01:17:16
their productivity, these tools are worth keeping an eye on. The future is here and it's powered by AI. So what is deep learning? Deep learning is a subset of machine learning which itself is a branch of artificial intelligence. Unlike traditional machine learning models which require manual feature extraction, deep learning models automatically discovers representation from raw data. So this is made possible through neural networks particularly deep neural networks which consist of multiple layers of interconnected nodes.


01:17:46
So these neural network are inspired by the structure and the function of human brain. Each layer in the network transform the input data into more abstract and composite representation. For instance, in image recognition, the initial layer might detect simple features like edges and textures while the deeper layer recognizes more complex structure like shapes and objects. So one of the key advantage of deep learning is its ability to handle large amount of unstructured data such as images, audios and text making it


01:18:16
extremely powerful for various application. So stay tuned as we delve deeper into how these neural networks are trained, the types of deep learning models and some exciting applications that are shaping our future. Types of deep learning. Deep learning AI can be applied supervised, unsupervised and reinforcemental machine learning using various methods for each. The first one supervised machine learning. In supervised learning, the neural network learns to make prediction or classify that data using label data sets. Both


01:18:46
input features and target variables are provided and the network learns by minimizing the error between its prediction and the actual targets. A process called back propagation. CNN and RNN are the common deep learning algorithms used for tasks like image classification, sentiment analysis and language translation. The second one, unsupervised machine learning. In unsupervised machine learning, the neural network discovers patterns or cluster in unlabelled data sets without target variables. It identifies hidden


01:19:16
pattern or relationship within the data. Algorithms like autoenccoders and generative models are used for tasks such as clustering, dimensionality reduction and anomaly detection. The third one, reinforcement machine learning. In this, an agent learns to make decision in an environment to maximize a reward signal. The agent takes action, observes the records and learns policies to maximize cumulative rewards over time. Deep reinformented learning algorithms like deep networks and deep deterministic polygradient are


01:19:48
used for tasks such as robotics and gameplay. Moving forward, let's see what are the artificial neural networks. Artificial neural networks inspired by the structure and the function of human neurons consist of interconnected layers of artificial neurals or units. The input layer receives data from the external resources and it passes to one or more hidden layers. Each neuron in these layers computes a weighted sum of inputs and transfers the result to the next layer. During training, the weight


01:20:19
of these connection are adjusted to optimize the network's performance. A fully connected artificial neural network includes an input layer or more hidden layers and an output layer. Each neuron in a hidden layer receives input from the previous layer and sends its output to the next layer. So this process continues until the final output layer produce the network response. So moving forward let's see types of neural networks. So deep learning models can automatically learn feature from data


01:20:49
making them ideal to task like image recognition, speech recognition and natural language processing. So the most common architecture in deep learnings are the first one feed forward neural network FN. So these are the simplest type of neural network where information flows linearly from the input to the output. They are widely used for tasks such as image classification, speech recognition and natural language processing NLP. The second one convolutional neural network designed specifically for image and video


01:21:19
recognition. CNN's automatically learn feature from images making them ideal for image classification, object detection and image segmentation. The third one, recurrent neural networks, RNNs are specialized for processing sequential data, time series and natural language. They maintain an internal state to capture information from previous input making them suitable for task such as a speech recognition, NLP and language transition. So now let's move forward and see some deep learning application. The first one is autonomous


01:21:51
vehicle. Deep learning is changing the development of self-driving car. Algorithms like CNN's process data from sensors and cameras to detect object, recognize traffic signs and make driving decision in real time, enhancing safety and efficiency on the road. The second one is healthcare diagnostic. Deep learning models are being used to analyze medical images such as X-rays, MRIs and CT scans with high accuracy. They help in early detection and diagnosis of diseases like cancer, improving treatment outcomes and saving


01:22:22
lives. The third one is NLP. Recent advancement in NLP powered by deep learning models like transformers, chat GPD have led to more sophisticated and humanlike text generation, translation and sentiment analysis. So application include virtual assistant, chat bots and automated customer service. The fourth one defake technology. So deep learning techniques are used to create highly realistic synthetic media known as defects. While this technology has entertainment and creative application, it also raises ethical concern regarding


01:22:56
misinformation and digital manipulation. The fifth one, predictive maintenance in industries like manufacturing and aviation. Deep learning models predict equipment failures before they occur by analyzing sensor data. The proactive approach reduces downtime, lowers maintenance cost, and improves operational efficiency. So now let's move forward and see some advantages and disadvantages of deep learning. So first one is high computational requirements. So deep learning requires significant data and computational resources for


01:23:27
training. Whereas advantage is high accuracy achieves a state-of-the-art performance in tasks like image recognition and natural language processing. Whereas deep learning needs large label data sets often require extensive label data set for training which can be costly and time consuming together. So second advantage of deep learning is automated feature engineering automatically discovers and learn relevant features from data without manual intervention. The third disadvantage is overfitting. So deep learning can


01:23:58
overfit to training data leading to poor performance on new unseen data. Whereas the third deep learning advantage is scalability. So deep learning can handle large complex data set and learn from massive amount of data. So in conclusion, deep learning is a transformative leap in AI mimicking human neural networks. It has changed healthcare, finance, autonomous vehicles and NLP. On July 25th, Open AI introduced search GBT, a new search tool changing how we find information online. Unlike traditional search engines which


01:24:33
require you to type in specific keywords, Sergeibility lets you ask question in natural everyday language just like having a conversation. So this is a big shift from how we were used to searching the web. Instead of thinking in keywords and hoping to find the right result, you can ask now search GBD exactly what you want to know and it will understand the context and give you direct answers. It designed to make searching easier and more intuitive without going through links and pages. But with this new way of searching, so


01:25:06
there are some important questions to consider. Can search GPT compete with Google, the search giant we all know? What makes sd different from AIO views? Another recent search tool. And how does it compare to chat GPT open AI popular conversational AI? So in this video we are going to explore these questions and more. We will look at what makes RGBT special, how it compares to other tools and why it might change the way we search for information. Whether you are new into tech or just curious, this


01:25:39
video will break it down in simple words. Stick around to learn more about search. So without any further ado, let's get started. So what is search GPT? Search is a new search engine prototype developed by OpenAI designed to enhance the way we search for information using AI. Unlike a typical chatbot like CHP, Search isn't just about having a conversation. It's focused on improving the search experience with some key features. The first one is direct answer. Instead of simply showing you a list of links,


01:26:10
Sergeyd delivers direct answer to your questions. For example, if you ask what is the best wireless noise cancellation headphone in 2024, SGBT will summarize the top choices highlighting their pros and cons based on expert reviews and user opinions. So, this approach is different from the traditional search engines that typically provide a list of links leading to various articles or videos. The second one is relevant sources. Serptt responses come with clear citations and links to the original sources ensuring transparency


01:26:41
and accuracy. So this way you can easily verify the information and del deeper into the topic if you want. The third one conversational search GPD allows you to have a back and forth dialogue with the search engine. You can ask follow-up questions or refine your original query based on the responsive you receive making your search experience more interactive and personalized. Now let's jump into the next topic which is GPT versus Google. So, Serge Gibb is being talked about a major competitor to


01:27:12
Google in the future. So, let's break down how they differ in their approach to search. The first one is conversational versus keyword based search. Search GPT uses a conversational interface allowing user to ask question in natural language and refine their queries through follow-up questions. So, this creates a more interactive search experience. On the other hand, Google relies on keyword based search where a user enter specific terms to find relevant web pages. The second thing is direct answer versus list of links. So,


01:27:44
one of the search GPT's standout feature is its ability to provide direct answers to the question. It summarizes information from the various sources and clearly sites them. So, you don't have to click through multiple links. Google typically present a list of links leaving user to sift through the results to find the information they need. The third one AI powered understanding versus keyword matching. Search GPs uses AI to understand the intent behind your question offering more relevant result


01:28:13
even if your query isn't perfectly worded. Google's primary method is keyword matching which can sometimes lead to less accurate result especially for complex queries. The fourth one, dynamic context versus isolated searches. So, search maintains content across multiple interaction allowing for more personalized responses. Whereas, Google treats each search as a separate query without remembering previous interaction. And the last one, realtime information versus index web pages. Search aim to provide the latest


01:28:45
information using real-time data from the web. Whereas Google V index is comprehensive but may include outdated or less relevant information. So now let's jump into the next topic which is serdity versus AI overviews. So SGD and AI overviews both use AI but they approach search and information delivery differently. It's also worth noting that both tools are still being developed. So their features and capabilities may evolve and even overlap as they grow. So here are the differences. The first one


01:29:15
is source attribution. Search GBT provides clear and direct citation linked to the original sources making it easy for user to verify the information whereas AI overviews include links. The citation may not always be clear or directly associated with specific claims. The second one is transparency control. Search promises greater transparency by offering publishers control over how their content is used including the option to opt out of AI training. AI overviews offers less transparency regarding the selection of


01:29:47
content and the summarization process used. The next one is scope and depth. SGBT strives to deliver detailed and comprehensive answers pulling from a broad range of sources including potential multimedia content and in AI overviews offers a concise summary of key points often with links for further exploration but with a more limited scope. Now let's jump into the next part. Serge GPD versus chatgity. Serge GPT and chat GBD both developed by OpenAI share some core features but serve different purposes. So here are


01:30:21
some differences. The first one is primary purpose. Serge GPT designed for search providing direct answer and sources from the web. Whereas SGPT focus on conversational AI generating text responses. The second one is information sources. Serge GPT relies on realtime information from the web whereas SGPD knowledge based on the training data which might not be correct. The third one is response format. Search GPT prioritize concise answers with citation and source links. So whereas SGT is more flexible generating longer text


01:30:53
summarizes creative content code and etc. The next feature is use cases. Search GPT idle for factf finding research and task requiring up-to-date information. Whereas GPD is suitable for creative writing, brainstorming, drafting, emails and other open-ended task. So now question arises when will ser be released? Sergey is currently in a limited prototype phase meaning it's not yet widely available. OpenAI is testing with a select group to gather feedback and improve the tool. So if you are interested in trying sir GPD so you


01:31:28
can join the wait list on its web page but you will need a chat GPD account. A full public release by the end of 2024 is unlikely as open hasn't set a timeline. It's more probable that surgdy features will gradually added to the chat GPD in 2024 or in 2025 with a potential standalone release later based on testing and the feedback. So with this we have come to end of this video. If you have any question or doubt, please feel free to ask in the comment section below. Our team of experts will


01:31:57
help you as soon as possible. Did you know that within just a few lines of code, you can transform an AI model into something far more powerful. Something that responds to questions, connects to live data, pull insights from databases, and even interacts with other app in real time. That's what Langchain allows you to do, and it's quickly becoming the go-to framework for AI developers. Think about this. You're about to create something amazing. An AI that can think, learn, and grow in ways we once only


01:32:27
dreamed of. And here's the best part. You don't need to be an AI expert to make that happen. Langchain is like a toolkit that connects the most advanced large language models like OpenAI's GPT to realtime data, allowing you to build AI applications that are smart, flexible, and highly interactive. Langchain is more than just a way to make AI development easier. It's a framework that allows different language models to work together seamlessly. So whether you want to understand user


01:32:58
questions with one LLM, create humanlike responses with another, or pull in data from an API or a database, Blankchain makes all possible. The framework takes care of heavy lifting, connecting models, managing data flows, and even customizing how your AI interacts with external sources. Now the question is why is lang chain so popular? It has become one of the most fastest growing opensource project because it's solving a huge problem for developers. The challenge of integrating generative AI


01:33:28
analyms with external data and complex workflows. As AI becomes more central to our lives in 2024, Langchain is helping developers create smarter, more powerful application. So whether it's just for chatbots, content creation or advanced data analysis. In this tutorial, I'll show you exactly how to get started with Langchain from setting up your environment to building the first AI powered app. I'll walk you through it. So, Langchain makes it possible to train models on our own custom data, opening


01:33:59
up more possibilities for building specialized intelligent application. By the end of this video, you will be ready to start building with Langchain. And trust me, once you see how easy it is, you'll wonder why you didn't start using it sooner. Let's start with a simple question. Why should we use lang? Imagine you're working with large language models like GPT4 or hugging face models and you want to take their capabilities further like integrating them with your own data sources or


01:34:27
allowing them to take action based on information they retrieve. This is where Langchain comes in. Langchain is like an open source framework that allows you to build intelligent applications by connecting large language models with external data sources. It can turn static AI interactions into dynamic data aware workflows. One of the best parts is you don't have to manually code everything from scratch. Lang chain abstracts away much more complexity of working with LLMs allowing developers to


01:34:56
focus on building functional applications instead of wrangling API calls and managing data pipelines. So, Langchain is set to play even bigger role in AI development because it enables you to harness true power of generative AI by connecting it with realtime data and external tools. So, now we have understood what lang chain is. Let us now understand how to install lang and we'll just simply go to the website docs part and we'll just read through this documentation. So here it has explained


01:35:26
what lang chain is and what are the framework consisting of. So we also have this tutorials on how do we install lang chain. Okay. So for installing you can just simply click on this quick start and uh see here it has uh written how do we set up on Jupyter notebook. So this is the command if you want to uh install lanch we will use the pip pip command. So just simply you can copy this command pip install lchin and you have to open your command prompt or the terminal in your computer and here you have to


01:36:03
simply copy paste the command. So as you can see it has it will uh load all the packages which is required for installing lang chain. So you can see here requirement already satisfied. This is because I had already installed my uh lchain before. So uh we have understood how to install this lang chain by using this command and you can also install the lm chain. We'll understand it later. Now let me just show you what else you need to install. First we have understood this lang chain. Then we have


01:36:33
the pine cone client. So we'll just simply search here pine cone client and uh it will redirect us to this page. So pine cone client is actually a vector store for storing and retrieving embeddings which we will use in the later steps. So pine cone is also used to actually uh you know create secret API keys. So here you can also create the API keys. You can also read the documentation part. So uh so we'll understand how to create API keys using open AI. But first uh let us install pine cone client in our system. So we'll


01:37:14
again go to command from this terminal and we'll just copy paste pip install pine cone client. So you can see here it will download and install all the packages required. So it has been installed. Now the third thing we're talking about is open AI client. So we'll use open AI models for a language large language task. So uh so we'll just simply search here open AI. Okay. And so it has redirected us to this page open AAI platform. And uh okay before starting this uh so this is the platform here


01:37:55
where you can create export an API key uh in open AI. Okay. So you can see here overview quick start concept everything is there. And uh to create an API key, we'll just simply click here. And uh here you have to select this option create a new secret key. Suppose I give my secret key name. Anything you can give. Suppose uh I give test test 1 2 3. Okay. And permissions is all. And we create the secret key. Now uh you need to uh actually save your key. We will just copy this key because it will be


01:38:33
required later while uh debugging the code. So we will just copy paste this secret key. We will require it later. And then done. So these are the keys I have created. So actually charg LLM models like open AI and hugging face uses lang chain to integrate with other APIs to create your own custom LLM models or chat bots. So suppose here we have logged in our charge GPT here and uh if I search here who won the uh WDC World Cup in 2023. So here it has shown the answer. For example, uh if I search who won the


01:39:22
cricket match world [Music] cup. So here as you can see the charge has given answer as as my last knowledge update the unit for men's welcome has not taken place yet. This is happening because the uh this charg older version has not been trained on the latest uh upcoming news or whatever the new technology is. So by using lang you can integrate with other APIs and you can create your own customized LM models or chat bots which help you to train your own custom data using various tool and APIs. So uh before we move on, I have


01:40:05
already showed you how to create the secret uh API key and how you have to store that the API key address. So first we have already understood how do we install lang chain here. So by using the pip command we do it and uh so you also need to install python uh 3.8 or later installed in your system. So I already have Python installed and to check that you can just simply I already have it installed in my system. So to check that I'll simply just type here Python minus minus version and click enter. So as you can


01:40:41
see it has shown me the Python version which is installed in my computer. This so the second step is already uh we have discussed which is open AI API key here. So second step we have already discussed how do we create our open AI API key. So we have to sign up in our open AI then go to the API key section and then create your new secret key and these are the keys I have created and you can just uh keep it later later you'll use that. So now we'll come to the third step which is create a project directory and


01:41:13
setup. So what we do is uh we have Jupiter installed in our system. So we'll just go to command prompt here and type uh J U py Jupiter notebook. So it will redirect us to the Jupyter notebook installed in our system. So if not uh we can just simply go from here. It is loading right now. We have to wait. Now you just need to click on this uh new and Python 3 IP by kernel because Python has been installed in my system. So we will use Python as our component here and here you can just give the prompts the command. Before


01:41:54
that you have to create a Python file which we can also create this Python file in Visual Studio Code. Just simply go to Visual Studio Code here and just simply click on file new file and I'll just type here Python. P1 and you can uh you have to first create store the API key. For this we use the command open AI API key equal to and just give your uh secret key. Okay, you can just simply uh copy paste here your secret key and just store this. So this ensures that your API key is stored securely and it can be used


01:42:48
whenever needed. Now the step four is to initialize a project and install the required libraries. So you need to add some additional libraries like streamlet to make a user interface. So let's uh add that too in our project folder. You can either create u a text file name requirements.ext and then uh install all these. We have already installed the OpenAI lang chain. We just need to install streamllet. So here you can just give the command pip install streamllet. So as you can see this I have already uh installed


01:43:24
streamllet before. Same wise you can also install openAI if it is not installed in your system using the command terminal the Windows PowerShell. So uh we have understood this how what all packages and what all we need to install. Now the next step is to build your first lang chain app to create a simple app which uses a input query and the app will generate a response using open GPT model. So you have to create a Python file name uh main.pi here. So so as you can see I've already uh imported


01:43:57
this main.pi and this is my code here. import streamllet as ST from lang and the constants I have created and then I have initialized the open AI with our API key. So you have you have to just type this prompt here. I'm using VS code here. You can also do it in your Jupyter notebook and then to create the streamllet app you have to give a title lang demo with openAI. So this is the title I have provided and then the text input pro for prompt the prompt is uh st.ext text input and enter a prompt.


01:44:29
You can just type enter a prompt or whatever you wish to and then display the response. So if prompt response is lm.tpredict prompt. So you can use the predict method for llm. So here what the app will so after creating and debugging this in the terminal. So your app will initialize openai using your API key and the user inputs a prompt through the streamllet interface. lang chain processes the input and sends it to the open GBT model and the AI generates a response which is then displayed in the


01:45:00
app. So now you can use all these prompts to run on your app. So to do this you can just uh to see your app in action you can just go to the terminal and run the following command which is stream run main.pi. So you can just simply go to uh the terminal here and just simply type the command uh simply type the command which is stream streamllet run main py. So by giving this prompt a new tab in your browser will open displaying the app and you can also type any question into the input box. So last


01:45:34
now we have understood all these steps. So this was a quite basic tutorial on how to install lang chain and then you know integrate it with the app. You can also customize and expand. So langchain's flexibility allows you to integrate other APIs also external data sources or even add memory to your AI application. So whether you're building a simple chatboard or more complex AI system, the possibilities are endless. So by following all these steps you will have a fully functioning app running in


01:46:03
your system in no time. Do you know friends that according to the lending statistics the demand for AI and ML specialist is projected to surge by 40% between 2023 to 2027 and on an average an ML engineer is expected to earn around 133 and $336 per year. So if you are an aspiring ML engineer and thinking about what innovative projects you can show in your portfolio then your wait is over cuz in this video I'll be covering eight amazing ML projects that you can showcase in your resume. So guys let's


01:46:40
start first with a beginner level project and the first project that we are going to encounter that is home value prediction. So guys this project aims to develop a predictive model to estimate the value of residential properties. The model will analyze various features such as location, square, footage, number of bedrooms and bathrooms, age of the property and other relevant factors. By leveraging historical property data, the model will be able to provide accurate home value predictions which can be useful for real


01:47:10
estate agents, buyers and sellers. So guys the programming language that we are going to use all over here will be Python and machine learning libraries that we will be using will be scikitlearn, tensorflow, kas and for data handling libraries we have pandas, numpy and for visualization we have to use mattplot lil and sebon. Now what will be the approach for this one guys? So guys the first one that we have a data collection. So here what is going to happen guys? So first you have to collect the historical property data


01:47:42
from the sources like Zillow Retailer.com. You can also get database from the public real estate databases like Kaggle data sets where you have Zillow home value prediction. Ensure that the data set include features like location where you have latitude, longitude, square footage, number of rooms, year build, property type and previous sales. The next step that comes is data cleaning. You have to handle the missing values by using imputation techniques or removing incomplete records. Removing outliers that may skew


01:48:12
the model's prediction, normalize or standardize the data to ensure consistency. The third one that we have is feature engineering. You have to create new features such as proximity to schools, crime rates, and access to the public transportation. Encode categorial variables, example property type, location using techniques like one hot encoding. generate interaction features that capture relationship between existing features. The fourth one that we have is model selection. Use regression models like linear


01:48:40
regression, random forest, gradient boosting, neural networks. Experiment with different models to identify the best performing one. Now in the next phase, all you have to do guys is model training and evaluation. Split the data set into training and test sets. Train the model on a training set and evaluate their performance on the testing set using metrics like RSME which means root mean squared error. You can use cross validation to ensure the model's robustness and avoid overfitting. The


01:49:10
sixth one that we have all over here is hyperparameter tuning. You can optimize the model's hyperparameter using techniques such as grid search or random search to improve accuracy. And if you're looking forward to deploy your model, then you can develop a web interface using flask or Django to allow users to input property features and get predictions. You can deploy the model on the cloud platform like AWS for scalability. Now if you talk about the complexity level of this, we all know


01:49:37
that it is a beginner level project. Now let us move on to the one more set that is music genre classification and generation. So guys, this is also one of the most beginner level project. This project aims to develop a system that can classify music tracks into different genres and generate new music composition within specified genre. The goal is to build a model that analyzes audio features to categorize music and uses deep learning techniques to create new music. This project introduces advanced concept of audio processing,


01:50:07
deep learning and generative models. So guys, what will be used in this? So we'll have programming language that will be Python. For audio processing, we'll be using librosa. For machine learning libraries, we'll be using TensorFlow, Kiraas, PyTorch. For data handling libraries, we'll be using pandas, numpy. For visualization, we'll be using mattplot list, seabon. For the data set, guys, you can use gtzan music genre data set or you can get it from free music archive. So guys, in the


01:50:34
first phase, we are going to have data collection. You can obtain data sets containing music tracks and their corresponding genre from the sources from GTN music genre data set and the free music archive. Ensure that a data set includes diverse genre and substantial number of tracks per genre. Next, we'll go for data prep-processing. Use library to load and pre-process audio files including feature extraction such as mil frequency, septal coefficients, chroma features, and spectral contrast. You can normalize the


01:51:04
extracted features to ensure consistent input for the given models. Now if we talk about feature engineering guys, you can extract additional features from the audio files such as tempo, beat, zero crossing rate etc. Create a feature matrix that represents the extracted audio features. Then go for the model selection. Use conventional neural network or recurrent neural networks for the music genre classification. Split the data set into training and testing data set. Now if we talk about model training and evaluation guys, then you


01:51:33
can train the selected classification model on the training set. Evaluate the model's performance on the testing set using metrics like accuracy, precision, recall, and F1 score. Use confusion matrices to understand the classification performance across different genres. Now, if you talk about model selection and training for the music generation, what you will do guys, you can use the generative adversial networks or recurrent neural networks such as LSTM, long short-term memory for music generation. Train the generative


01:52:01
model on the data set to create new music sequences. Next we have model training and evaluation. You can train the generative model on sequences of audio features. You can evaluate the generated music by listening tests and by objective metrics like inception score or fche audio distance. If I talk about hyperparameter tuning guys, you can optimize the model. You can use the hyperparameters using techniques like grid search or random search to improve performance. If I talk about deployment guys, you can deploy these models on


01:52:30
cloud platforms like AWS. Now let us move on to our next project. So guys the complexity of this project is at the beginner level. Now let us move to the intermediate level projects. Next project that we have all over here is sentiment analysis of Twitter data. This project aims to develop a sentiment analysis model that can classify tweets at positive, negative or neutral. The goal is to analyze public sentiment on various topics or events using natural language techniques. So guys what will be used all over here? So in this we


01:53:01
will have programming language like Python okay NLP libraries NLTK spacy for machine learning libraries we can use scikitle learn tensorflow kas for data handling libraries we have pandas numpy for visualization we have mattplot lilip sebon and we can use the API Twitter for data collection now how you going to work on it guys so guys if I talk about the data collection use a twitter API to collect tweets based on specific hashtags like keywords or topics Extract relevant fields like tweet text,


01:53:33
user information, time stamp, etc. Then if I talk about data prep-processing guys, clean the tweet text by removing special characters, links, mentions, hashtags and stop words. Tokenize the text and perform limization or stemming to reduce the words to their base form. Next, if you talk about feature engineering guys, you convert the clean text data into numerical representation using TF, back of words or word embedding. Now if I talk about model selection guys, you can choose a classification algorithms such as


01:54:02
logistic regression, n bias or LSTM. Split the data set into training and testing data set. Now if I talk about model training and evaluation, then you can train the selected model on the training set. Evaluate the model's performance on the testing set using metrics like accuracy, precision, recall, and FN score. Use cross validation to ensure the model's robustness. If I talk about hyperparameter tuning guys, you can optimize the model's hyperparameters using grid search or random search to


01:54:31
improve the performance for deployment which can be optional. You can deploy your model on AWS for real-time sentiment analysis. So if I talk about the complexity level guys, its complexity is intermediate. So guys, our next project is customer segmentation using K means clustering. This project aims to segment customers into distinct groups based on their purchasing behavior and demographic information. The objective is to understand customer segments and tailor marketing strategies accordingly. So guys, what programming


01:55:01
languages we'll be using? So basically we'll be using Python. For machine learning libraries, we will have scikitlearn. For data handling libraries, we'll have pandas, numpy. For visualization libraries, we'll have mattplot, seaborn. And data set source will be e-commerce transaction data. how we are going to work on this one. For data collection, we can obtain a data set of e-commerce transactions that include customer demographics, purchase history, and product information. Next,


01:55:25
we'll have data prep-processing. For data prep-processing, we are going to do the cleaning of the data by handling the missing values and outliers. Then, for feature engineering, we are going to create features like total purchase amount, purchase frequency, and recency of the purchases. Then, we are going to proceed for the model selection. You can use K means clustering to segment the customers into distinct groups. You can determine the optimal number of clusters using methods like elbow methods or


01:55:52
silhou. Now if I talk about model training and evaluation, you can train the K means model on the process data set. You can evaluate the quality of clusters by analyzing intracluster and intercluster distances. Next we have the evaluation. You can visualize the clusters using techniques like PCA, principal component analysis, TSN etc. Next we have the hyperparameter tuning. Now now you can tune this model and interpret the characteristic of each segment. You can develop a target marketing strategies for each segment


01:56:24
based on unique behavior and preferences. Now deployment is optional. You can develop a dashboard using flask or Django to visualize customer segments and track marketing campaigns. So guys if I talk about the complexity of this project. So this is an intermediate level project. So guys for data set you can use the Kaggle's customer segmentation data set which is available at the Kaggle's platform. Now the third intermediate level project that we have all over here is building a chatbot with


01:56:53
Rasa. This project aims to build an intelligent chatbot using Rasa framework. The chatbot will be capable of understanding user queries and providing appropriate responses making it useful for customer support, personal assistance or information retrieval. What languages we are going to use? So it will be Python based. We'll have the NLP libraries like Rasa, NLTK, Spacey. For machine learning libraries, we are going to have scikitlearn, tensorflow, kas, etc. For data handling libraries, we are going to use pandas, numpy. So


01:57:24
guys, this was what we are going to do it. And how you can work on this one by collecting the data, collect the conversation data and FAQs from the target domain, annotate the data to create training examples for the chatbot. Next comes is data prep-processing. Clean the text data by removing special characters and normalizing the text. You can tokenize and limitize the text to prepare for a training. The third one we have the models training. You can use Rasa's NLU's component to train a model for


01:57:52
intent recognition and entity extraction. You can define the dialog management policies to handle different conversation flows. Next guys, you can perform the feature engineering and integration. You can integrate the Ras NLU and core components to build complete chatbot. You can connect the chatbot to messaging platform like Facebook, Messenger, etc. For model selection and testing, what you can do guys, you can test this chatbot with various inputs to ensure that it handles the scenarios appropriately. And you can


01:58:20
also select the right model using this. Now, if I talk about model training guys, what do you have to do? You have to collect the user feedback and conversational logs to continuously work on training the model. Next, similarly you have to retrain the model periodically with the new data to see how it is working. So that will be your evaluation. Now for the hyperparameter tuning, what you going to do guys? You have to check in those scenarios where it is able to tune up with those scenarios where it can handle the input


01:58:50
appropriately. And next is deployment. So guys, for deploying it, you can use AWS. So guys, for a data set, you can use Ras open source. So that's a very good data set for you to proceed. So guys, if you talk about difficulty of this project, this is an intermediate level project. Now let us move on to the advanced level projects. For advanced level projects, the first one that comes up to my mind is movie similarity from plot summaries. Now this project aims to develop a system that can find out


01:59:19
recommended movies similar to a given movie based on their plot summaries. By analyzing the textual content of the movie plot summaries, the model will identify similarities and suggests movies with similar themes, story lines or genres. This project introduces beginners to natural language processing text similarly measures and recommendation systems. What languages we are going to use guys? we'll be using Python NLP libraries like NLTK spacy machine learning libraries like scikitlearn data handling libraries like


01:59:50
pandas numpy visualization we can use numpy and data set source will be IMDb or kegel so guys this process is also involving the data collection then you have to go for data cleaning then feature engineering next model selection so similar process as I have discussed in other projects so you have to also go through the same one next what you have to do device. Similarly, what you have to do, you have to train the model, then evaluate the model, then hypertune it and finally proceed for the deployment.


02:00:21
So, this is overall process of this project. Try to research on the website a lot like how you can extract it. So, guys, you can use kegel or towards data science to research more about this project. Now guys, if I talk about the difficulty of this project and this is an advanced level project. Now let us move on to the next one that we have all over here that is image segmentation project for brain tumor prognosis. This is a very very amazing project and definitely you can put up on your portfolio. Basically guys this project


02:00:53
aims to develop an image segmentation model to identify and delinate brain tumors from MRI scans. The goal is to accurately segment the tumor regions which can aid in prognosis treatment planning surgical interventions. This project introduces intermediate level concepts of computer visions, deep learning and medical image analysis. So guys, what we'll be using all over here for programming languages, we can use Python. For deep learning libraries, we can use TensorFlow, KAS, PyTorch. For image processing libraries, we can use


02:01:22
OpenCV, scikit, image. For data handling libraries, we can use pandas, numpy. For visualization, we can use mattplot lab, cbond. Now if I talk about what is the process of developing this project the first step will be same data collection. So next step you have to go for data prep-processing. Third step you have to do the model selection where you can use CNN models for image segmentation task. Then you go for model selection. Moving ahead you're going to have the model training and evaluation.


02:01:52
You have to split the data set into training and validation and testing data sets. Next proceed for the evaluation phase. Okay, evaluate the model with certain metrics. So here I can give you certain idea like you can use dice coefficient, intersection over union or accuracy. Then go for hyperparameter tuning where you have to optimize the model's hyperparameters. You can use grid search or random search as we have discussed. And finally you can deploy this model on AWS. Now guys we have come to the final


02:02:21
project. This is also a very amazing project guys. So guys the complexity level of this project is advanced level. Now let us move on to our final project that is the impact of climate change on birds. This is a very very amazing project and definitely you can add it on your resume. This project aims to analyze the impact of climate change on the bird population and migration patterns by examining various climatic factors and their correlation with bird species data. The project seeks to predict how climate change might affect


02:02:52
bird behavior and distribution. This project will introduce you some advanced level concepts like time series analysis, environmental data modeling, etc. So guys, what programming languages we'll be using? For data analysis, you can see we'll have pandas, numpy. For machine learning libraries, we are going to have scikitlearn, tensorflow. For visualization, we're going to have mattplot lab, plotly. For geospatial, we are going to have geopandas, folium. For data source, we're going to have public


02:03:18
data sets on bird observation and climate data sources from eird. Now what will the process flow for this one guys? First you have to proceed for data collection. Gather bird observation from the data like EIRD which provides extensive record of bird sightings. Okay. And for climate data you can collect it from NOA including temperature, precipitation and other relevant climatic factors over the time. And similar next process will be the data prep-processing. Then you have to proceed for feature engineering. Then


02:03:46
you have to go for model selection. Okay. Moving ahead you have to go for model training. then evaluation, then hyperparameter tuning and finally you have to deploy the model. So research about this project, see what models you're going to use. Suppose I can give you a hint about this. You can use time series analysis models like ARMA or ML models. You can also use random forest or gradient boosting for predicting impact on the bird population. So guys, use Google exhaustively to research about this project. This is also a very


02:04:18
amazing project and it's going to give you a lot of idea. Now if I talk about the complexity level of this, it is an advanced level project. Today we will take you through a hands-off lab demo of how we can use GN generative adversarial network for the image classification. And for amazing videos like this subscribe to our YouTube channel and press that bell icon to stay updated. So in today's session we will discuss what GN is and moving ahead we will cover types of models in G and in the end we


02:04:45
will do a hands-off lab demo of celebrated face image using GN. So now let's see what is GN. So generative adversarial networks were introduced in 2014 by J goodfellow and co-authors. Gn perform unsupervised learning task in machine learning. Gn can be used to generate new example that possibly could have been drawn from the original data set. So this is an image of GM. There is a database that has a real 100 rupee node. The generator neural network generates fake 100 rupee node. So the discriminator network will help to


02:05:20
identify the real and the fake node or the real and the fake images you can say. So moving ahead let's see what is generator. So a generator is a GN neural network that creates fake data to be trained on the discriminator. It learns to generate plausible data. So the generator examples or instances become negative training examples of the discriminator. So as you can see here the random input generate a new fake image. The main aim of the generator is to make the discriminator classify its output as real. So the part of GN that


02:05:51
trains the generator includes the noisy input actor or generator network which transform the random input into an instance or the discriminator network which classify the generator data. So after seeing what is generator let's see what is a discriminator. So the discriminator is a neural network that identifies the real data from the fake data created by the generator. So the discriminator training data comes from two sources. The real data instance such as real pictures of birds, human currency, nodes and


02:06:20
anything are used by the discriminator as a positive sample during training. The second one is the fake data instance created by the generator are used as a negative examples during the training process. So discriminator decide from the real images and the fake images generated by generator and discriminator decide which is fake and which are real. So now let's move on to the programming part and see how we can use GN using celebrity face image data set. So here we will start with GN generative


02:06:47
adversarial networks. Okay. So first I will rename it GN. Okay. So here we will import uh some libraries like import OS. So we will do from pytorch machine learning deep learning library which work for like neural networks. So here I will write from torch dot utlis dot data import data loader. Okay. So what is this torch dot list data? So this is an abstract class representing a data set and you here you can custom data set that inherit data set and override the data set. Okay. And this import data loader. So data loader


02:07:41
is a client application for the bulk import or export of the data. And we can use it for to insert update delete or export like records. And when importing data, data loader reads, extract and loads data from the CSV files like comma separated values or from a database connection you can say. And when exporting data, it output a CSV file. Okay. Then moving forward to vision dot transform as t. Okay. So transform are like very common image transformation available in the to vision. So transformation module they


02:08:33
can be changed together using compost. So most transform classes have function equivalent functional transform give fine grain control over the transformations. And one more like from dot vision vision dot transforms sorry data sets import image folder. Okay. Invalid syntax it is invalid. Let I will tell you it's not. It's import. Okay. Yes. So now what I will do? Okay. Touch util. It is. Yeah, now it's working fine. So now we will import uh the data set. So we are here we are using celebrity face


02:09:53
image. Okay. So I will provide you the data set in the description box below. Don't worry. Okay. So you can download from data set directly from there. So this is my path to data set 375 desktop face image data it. Okay, now let's run it. Okay, now let's run it. Okay, now I guess it's fine. Yeah. So here what I will do, I will set the image size and all. So image size was to 64. Then patch size equals to 256. Then batch size equals to 256. Then stats equals to 0.5 comma 0.5 and again


02:11:36
0.5 okay 0.5 comma 0.5 comma 0.5 okay so here we have set the image size and the patch size and the stat values So now what we will do we will train the data set. So here I will write train train ds equals to image folder of data sorry data directory comma transform t dot t dot compose Here I will add D dot uh the G size then image size. Okay. Then again t dot center crop. Center crop here I will write image size I will be small. Then here I will add t dot to tensor comma t dot normalize stats.


02:13:35
Okay. Let me do like this. Here I can write train DL equals to data loader then train DS test size then shuffle equals to true comma num workers equals to two number of workers then here I will write pin memory okay true let me run it okay the system cannot find the path specified C user okay so there is an path error Okay. So, let me copy my path. Let's see. Now, let me run. Yeah. So, it's working fine. So let me import torch from torch vision dot utils import migr. Okay. Then import mattplot


02:15:27
lib plot lib dot pip plot s plt [Music] then plot l. So this torch vision dot utils import my grid is used to uh make a grid. Okay, grid you know small small boxes and this mat plot li you already know is used for the making charts and different types of chart line chart, bar chart, pie chart. Okay. So let me run this some spaces. So here I will write now make a function non img tensors then return img tensors stats 1 0 plus that's zero and again zero. Okay. So let me run this. Now what we will do? We will make


02:16:59
again a new function for show images and show badges. Okay. For that I will write df show image. Okay. Then images, n max equals to 64. 64 will be there. Then figure comma axis equals to plt dot subplots figure size 10. Okay. Then axis dot set x ax dot set vertx. Okay. Then ex dot im show. This is image show. Then make grid the non with the n function images dot detect and max comma and row number of rows will be 8 then dot permute 1 comma 2 comma 0. Okay. Then df show badge DL comma n max= to 64.


02:19:09
Then for images in DL show images then images, max and max then trick. Okay. So now let's see some batches. So I will write show batch train. It's loading. It's loading. Okay. Some error. Okay. Image. Okay, I'm sure not. Okay, the spelling mistake. So, as you can see here, this maybe Robert Downey Junior. This is Robert Downey Jr. uh this is also Robert Junior and different celebrities here. So we have to do GN in this we will generate the fake images and we'll generate the new images then


02:20:50
discriminator will set the images which are real or fake. Okay. So now let's use GPU like let's see uh GPU is available or not. Okay. So here I will write dev get default device then if dot is available then return to dot device down. Okay. as return dot dot device to CPU then DF to device data device like for from this we will move tensor to chosen device like okay If is instance and see instance data, comma list, comma, double return to device X, comma device for X in data. Return data dot


02:22:58
to device one non clocking equals to true. Okay. T will be capital here. Then I will write class device data loader. So here what we will do we will wrap a data loader to move data to a device. So for that df init function to self comma dl comma device then here I will write self dot dl = to dl then self dot device Okay. So here I will write for the iteration. So here I have to give two underscore. Here I will write again self. So it yield a batch to data after moving it to a device. So for for


02:24:44
bin self dot dl then yield to device then b comma self dot device. Okay. And the last one is dfl length. We'll write self then it will return the number of batches. So return length of self dot dl. Okay. Invalid syntax. Okay. Not do not do. Okay. So here I will write device. And I will write device get fault device. Okay. Then train DL equals to device device data loader and chain DL common device. Okay. So uh as we already know what is GN and what is discriminator and you know generator. So let's uh take again


02:26:54
GN overview. So a generative address network GN has two bars. So the generator learns to generate possible data. The generator instant become negative training examples for the for producing impossible results. So So you have data. So what discriminator will do? Discriminator will you know decide from the generated data and the real data which are fake and which are real. Okay. This will generator will do. Discriminator sorry. Okay. So discriminator like takes an image as an input and tries to classify it as real


02:27:38
or generated. In this sense it's like any other neural network. So I will use here CNN which outputs as a single neural network for every image. So okay so I hope you know again like what discriminator is what generator is and what is like real data it is this okay and we will generate the data okay fake data and what discriminator will do discriminator will check whether the data is fake or real okay so here I will write import torch dot n as n. Here I will write discriminator equals to n dot sequential


02:28:49
Okay. So, these are some So these are some layer okay flattened layer, convoluted layer. Okay, leaky uh radio layer. So here I am setting you know discriminator like 3 into 64 64 okay so here 64x 128 128 by 256. So these are the sizes sizes of the images. Okay. So here discriminator equals to two device discriminator device. Okay. this. Okay. What's wrong? The spelling is wrong maybe. Okay. So, it's saying discriminator is not defined. Okay. Let me debug. Okay. nothing else the spelling was wrong so


02:30:32
sorry for that so let me do for the better visuals okay so I know I hope you know the generator what generator network is so here what I will do I will set the size latent size equals to 128 okay so here we have set the discriminator. Now what we will do? We will set the generator. Okay, the sizes like 3 into 64, 64 or 32, 128 and so on for all the layers. So here I'm setting for the generator the same I will write here generator to device generator comma device again the generator okay I'm copying this one


02:32:02
[Music] Okay. The data is defined here. Okay. That's working fine. So here so now what I will do I will do the discriminator training. Okay. So for that I have to write DF train discriminator real images. comma opt. Okay. Now we will clear the discriminator gradients. So opt d dot zero grid. Okay. Here we will pass real images through discriminator. Okay. So these are the for the real images because by we have to show the all the real and the fake images then we'll shuffle then and we'll find out which is


02:33:21
real and which is not. Okay. So and now we will generate the fake images using latent. Okay. So for this later equals to torch.random random input and the best size we are giving the latent size we are giving. Okay, fake images equal to generator. So now what we will do? We will pass the fake images through discriminator as we did for the real images. Okay. So now we will update discriminator weights. For that I have to write loss equals to real loss. then plus fake loss. Okay. Then loss dot backward


02:34:27
opt dot step return loss dot item comma score of fake score. Okay, I done. Okay, bracket is missing. gloss backward in 36. Okay. So here what we did we did the we pass the real images to discriminator then generate fake images and the same time we pass the fake images through discriminator and the at the end the loss equals to real loss and the fake loss we update the discriminator weights. Okay. So now, so this was the discriminator training. Now what we will do? We will do the generator training.


02:35:44
Okay. So for that I have to write DF. For that I have to write df train generator then opt g do dot zero grat. So what we are doing here we are clearing the generator gradients before that we did for the same the discriminator one. Okay. So now we will generate the fake images. Okay. What generator do? Generator only uh generate the fake images. Okay. So from this prediction from this prediction what we we are doing we are just make trying to fool the discriminator. Okay. So so here we will update the generator


02:37:03
weight. So I will write loss dot backward. Then I will write opt_g dot step. Then I will write return loss dot item. Okay let's run it. So here I will write from torch vision dot ut image and here I will write sample directory equals to generated generated Okay. And OS dot main directory sample directory comma exist. Okay. Equals to true. Okay. So now what we will do? We will uh save the sample data. Okay. So we I have to create uh to save samples uh one function. Okay. So here what I'm doing


02:38:37
we are I'm making the fake images generating the fake images and saving it. Okay. So moving forward but I will fix the I will fix uh the latent latent equals to dodge dot random input then 64 latent size comma 1 comma 1 comma then device question device. Then again save samples to zero comma fixed latent. Okay. Save samples is not defined. It's defined here. Yeah. So see this is the generated images. This is the fake image. Okay. Now what I will do? I will do the full training loop. For that I have to


02:40:05
write from TKDM dot notebook import DM then import torch.n NN dot functional SF. Let me give the spaces. So now what we will do? We will train this uh we will do the full training loop till the 400 epox. So it will take a very long time. So first I will write the definition. Okay, I will define one uh function. Okay, and then I will get back to you. So yes, what I did uh so this I have set the losses and the scores. Okay. And uh these are the optimizer some optimizers opt you can say optimizer we


02:41:10
are creating and here I'm training the uh discriminator and here I'm training the generator okay for the loss and here the record of the loss in the you know scores will the save and this is for the log of losses and the scores last batch and for This this is for the generated image. Okay, it will save the generated image. Okay, we have already created here you can see for the sample image for the saving. Okay, now what I will do? I will write percent time then LR = to 0.005 005 then epox= to 400 epox means it will take a


02:42:08
huge step. So history equals to fit box. Okay fit is not defined. Okay, I have to run it again. Okay, why it's coming like this? Okay, something item object has zero grid. Okay, have to check. So, as you can see, it started running. So this box will run till 400. So it will take a long time, very long time. So I will get back to you after that. So as you can see this is of 1 by 400. So it will run three till 339. Okay. So it will take a very very long time. So it will define the loss of generated, the loss of discriminator,


02:43:32
the real score and the fake score and at the same time it's saving the generated images. Okay. So it will take a long time and then I will get back to you. So as you can see here the GN are done till like 400. Okay, till all the 400. Okay. So now let's do some losses. Comma losses of discriminator and the real score and the fake scores equals to history. So here I will touch dot save the generator dot state let's go directory path comma g dot P. Okay. Then I will write torch dot save with


02:45:06
discriminator dot state directory path comma d.t. Okay. Some spelling mistake is there. Yeah. So I write from ipython dot display import image. Okay. So here I will write image like what the generator generated did the image dot slash generated slash images then 0001.b PNG. Okay, let's see. So this is this is the first image which generated by the generator. Okay, so same we we have 400 epox. So let's see. So here I will check the 100 image. So as you can see 100 image is more bit clear.


02:46:56
So what if I will check for the like 300 300 image one it's more bit clear okay now let's check the 400 image I hope see it is clear so it is these are the fake images which are generated by the generator to fool the discriminator to confuse the discriminator okay so now we will plot a graph we will prograph for the epoch and the loss in the for the discriminator and the generator. So for that right so as you can see this is a discriminator okay blue one and there is a generator generator so loss for the


02:47:53
generator is the more and the loss for discriminator is less which is very good and now let's see the real and fake images okay so these are the real images score and these are the fake images welcome to This video tutorial by simply learn. In this video, we will learn about an important popular deep learning neural network called generative adversarial networks. Yan lun one of the pioneers in the field of machine learning and deep learning described it as the most interesting idea in the last 10 years in


02:48:27
machine learning. In this video, you will learn about what are generative adversary networks and look in brief at generator and discriminator. Then we'll understand how GANs work and the different types of GANs. Finally, we'll look at some of the applications of GANs. So let's begin. So what are generative adversarial networks? Generative adversarial networks or GANs introduced in 2014 by Ian J. Goodfellow and co-authors became very popular in the field of machine learning. GAN is an


02:48:58
unsupervised learning task in machine learning. It consists of two models that automatically discover and learn the patterns in input data. The two models called generator and discriminator compete with each other to analyze, capture, and copy the variations within a data set. GANs can be used to generate new examples that possibly could have been drawn from the original data set. In the image below, you can see that there is a database that has real 100 rupee nodes. The generator which is basically a neural network generates


02:49:30
fake 100 rupees nodes. The discriminator network will identify if the nodes are real or fake. Let us now understand in brief about what is a generator. A generator in GANs is a neural network that creates fake data to be trained on the discriminator. It learns to generate plausible data. The generated instances become negative training examples for the discriminator. It takes a fixed length random vector carrying noise as input and generates a sample. Now the main aim of the generator is to make the


02:50:00
discriminator classify its output as real. The portion of the GAN that trains a generator includes a noisy input vector. The generator network which transforms the random input into a data instance. A discriminator network which classifies the generator data. And a generator loss which penalizes the generator for failing to do the discriminator. The back propagation method is used to adjust each weight in the right direction by calculating the weights impact on the output. The back propagation method is used to obtain


02:50:32
gradients and these gradients can help change the generator weights. Now let us understand in brief what a discriminator is. A discriminator is a neural network model that identifies real data from the fake data generated by the generator. The discriminator's training data comes from two sources. The real data instances such as real pictures of birds, humans, currency notes, etc. are used by the discriminator as positive samples during the training. The fake data instances created by the generator are used as


02:51:02
negative examples during the training process. While training the discriminator, it connects with two loss functions. During discriminator training, the discriminator ignores the generator loss and just uses the discriminator loss. In the process of training the discriminator, the discriminator classifies both real data and fake data from the generator. The discriminator laws penalizes the discriminator for mclassifying a real data instance as fake or a fake data instance as real. Now moving ahead,


02:51:32
let's understand how GANs work. Now GANs consists of two networks. A generator which is represented as G of X and a discriminator which is represented as D of X. They both play an adversarial game where the generator tries to fool the discriminator by generating data similar to those in the training set. The discriminator tries not to be fooled by identifying fake data from the real data. They both work simultaneously to learn and train complex data like audio, video or image files. Now you are aware


02:52:03
that GANs consists of two networks a generator G of X and discriminator D of X. Now the generator network takes a sample and generates a fake sample of data. The generator is trained to increase the probability of the discriminator network to make mistakes. On the other hand, the discriminator network decides whether the data is generated or taken from the real sample using a binary classification problem with the help of a sigmoid function that gives the output in the range zero and one. Here is an example of a generative


02:52:33
adversarial network trying to identify if the 100 rupee nodes are real or fake. So first a noise vector or the input vector is fed to the generator network. The generator creates fake 100 rupee nodes. The real images of 100 rupee nodes stored in a database are passed to the discriminator along with the fake nodes. The discriminator then identifies the nodes and classifies them as real or fake. We train the model, calculate the loss function at the end of the discriminator network and back propagate


02:53:01
the loss into both discriminator and generator. Now the mathematical equation of training again can be represented as you can see here. Now this is the equation and these are the parameters here. G represents generator. D represents the discriminator. Now P data of X is the probability distribution of real data. P of zed is the distribution of generator. X is the sample of probability data of X. Zed is the sample size from P of zed. D of X is the discriminator network and G of zed is the generator network. Now the


02:53:37
discriminator focuses to maximize the objective function such that D of X is close to 1 and Z of zed is close to zero. It simply means that the discriminator should identify all the images from the training set as real that is one and all the generated images as fake that is zero. The generator wants to minimize the objective function such that D of Z of zed is one. This means that the generator tries to generate images that are classified as real that is one by the discriminator network. Next, let's see the steps for


02:54:10
training a neural network. So, we have to first define the problem and collect the data. Then, we'll choose the architecture of GAN. Now, depending on your problem, choose how your GAN should look like. Then we need to train the discriminator on real data. Now that will help us predict them as real for n number of times. Next you need to generate fake inputs for the generator. After that you need to train the discriminator on fake data. To predict the generator data is fake. Finally train the generator on the output of


02:54:43
discriminator. With the discriminator predictions available. Train the generator to fool the discriminator. Let us now look at the different types of GANs. So first we have vanilla GANs. Now vanilla GANs have minmax optimization formula that we saw earlier where the discriminator is a binary classifier and is using sigmoid cross entropy loss during optimization. In vanilla GANs the generator and the discriminator are simple multi-layer perceptrons. The algorithm tries to optimize the mathematical equation using stochastic


02:55:15
gradient descent. Up next we have deep convolutional GANs or DC GANs. Now DC GANs support convolutional neural networks instead of vanilla neural networks at both discriminator and generator. They are more stable and generate higher quality images. The generator is a set of convolutional layers with fractional strided convolutions or transpose convolutions. So it unsamples the input image at every convolutional layer. The discriminator is a set of convolutional layers with strided convolutions. So it downsamples


02:55:46
the input image at every convolutional layer. Moving ahead, the third type we have is conditional GANs or C GANs. Vanilla GANs can be extended into conditional models by using an extra label information to generate better results. In C GAN, an additional parameter called Y is added to the generator for generating the corresponding data. Labels are fed as input to the discriminator to help distinguish the real data from fake data generated. Finally, we have super resolution GANs. Now, SR GANs use deep


02:56:18
neural networks along with adversarial neural network to produce higher resolution images. Super resolution GANs generate a photorealistic highresolution image when given a low resolution image. Let's look at some of the important applications of GANs. So, with the help of DC GANs, you can train images of cartoon characters for generating faces of anime characters and Pokemon characters as well. Next, GANs can be used on the images of humans to generate realistic faces. The faces that you see on your screens have been


02:56:49
generated using GANs and do not exist in reality. Third application we have is GANs can be used to build realistic images from textual descriptions of objects like birds, humans, and other animals. We input a sentence and generate multiple images fitting the description. Here is an example of a text to image translation using GANs for a bird with a black head, yellow body, and a short beak. The final application we have is creating 3D objects. So, GANs can generate 3D models using 2D pictures of objects from multiple perspectives.


02:57:24
GANs are very popular in the gaming industry. GANs can help automate the task of creating 3D characters and backgrounds to give them a realistic feel. Welcome to our video about Transformers in AI. And no, we don't mean the robot toys from the movies. We are diving into something even cooler in the world of computers and AI. Have you ever wondered how your phone knows what word you might type next? Or how Google Translate works so well? That's where transformers come in. They are like super smart computer brains that can


02:57:55
understand and create humanlike text. Here's a fun example. I asked a transformer to tell me a joke and it said, "Why did the computer go to the art school?" Because it wanted to improve its draw speed. Okay, that's a bit cheesy, but it shows how these computer programs can come up with new ideas on their own. Transformers are changing how we use technology every day. They help us with things like translating languages, summarizing long articles, writing emails and stories, and even playing games like chess. In


02:58:27
this video, we will explore how transformers work, why they are so special, and what cool things they might do in the future. So, let's talk about what exactly are transformers. Transformers are an artificial intelligence model used to process and generate natural languages. They can read and understand huge amount of text and then use that knowledge to answer questions, translate languages, summarize information, and even create stories or write code. The magic behind transformers is their ability to focus


02:58:57
on different text parts with attention mechanisms. This means that they can understand context better than older models, making their outputs more accurate and natural sounding. The basic structure of a transformer includes two main parts, the encoder and the decoder. Think of the encoder as a translator that understands and processes the input and the decoder as the one that takes the processed information and turns it into the output. For example, if we are translating a sentence from English to


02:59:26
French, the encoder reads the English sentence and converts it into a form that AI can understand. The decoder then takes this form and generates the French sentence. A great example of a transformer in action is CH GBT. ChatG uses transformers to understand and generate humanlike text. When you ask a question, it processes your input with its encoder and generates a response with its decoder. This lets it have conversations, write essays, and even tell jokes. For instance, if you ask Chad GPT, what's the weather like today?


02:59:57
It uses its transformer model to understand your question and responded with it's sunny with a chance of rain in the afternoon. This ability to understand and generate text makes transformers incredibly powerful. So, let's talk about how transformers work. Transformers are especially good at sequencetosequence learning task like translating a sentence from one language to another. Here's how they work. First, there's the attention mechanism. This allows the transformer to focus on


03:00:26
different parts of input data. For example, if it's translating the sentence, the cat sat on the mat, it can pay attention to each word's context to understand the meaning better. So it knows CAT is related to SAT and MAT helping it produce an accurate translation in another language. Transformers also use something called positional encoding. Since they process all words at once, they need a way to understand the order of the words. Positional encoding adds information about the position of each word to the


03:00:56
input helping the transformers understand the sequence. Another key feature is like parallel processing. Unlike older models like recurrent neural network which is RNNS, the process takes words by word, transformers can process the entire sentence at once. This makes them much faster and more efficient. Let's compare transformers with recurrent neural networks. But first, let's understand what are RNNs. So RNS is a type of neural network designed to handle sequential data. They process data one


03:01:29
step at a time maintaining a memory of previous steps. This makes them good for task where order matters like speech recognition or time series prediction. However, RNS have a problem called the vanishing gradient which means that they can forget information from earlier in the sequence. Imagine trying to understand the sentence Alice went to the park and then to the store and RNN might struggle to remember Alice by the time it gets to the store. But a transformer can easily keep track of Alice throughout the sentence. So why


03:02:00
are transformers better? Unlike RNS, transformers process the entire sentence at once, keeping the context intact. This solves the vanishing gradient problem and makes transformers faster and more accurate to task like language translation and text generation. So let's talk about the applications of transformers. At first we have language translation. They are used by services like Google translate to convert text from one language to another. For example, translating hello, how are you to Spanish as hola. Then we have


03:02:32
document summarization. They can take long articles and summarize them into shorter, more concise versions. For instance, summarizing a 10page report into a few key points, making it easier to understand the main ideas without reading the whole document. Then we have content generation. Transformers can write articles, stories, and even code. They can create new content based on what they have learned. For example, you could ask a transformer to write a short story about a space adventure, and then


03:03:01
it would come up with a unique narrative. Then we have game playing. Transformers can learn and play complex games like chess, making strategic decisions just like a human player. They analyze the entire board and make moves considering all possible outcomes. Let's talk about image processing. They are used in task like image classification and object detection helping computers understand visual data. For example, identifying objects in a photo like recognizing a cat, tree or a car. Now let's understand the training process.


03:03:33
The training transformers involves two main steps. Semi-supervised learning. They can learn from both label data where the answer is known and unlabelled data where the answer is not provided. This makes them very versatile. For example, a transformer could be trained on a mix of articles with and without summaries to learn how to summarize text effectively. Pre-training and fine-tuning. Transformers are pre-trained on a large data set to learn general patterns. Then they are fine-tuned with specific task making


03:04:04
them highly versatile. For instance, a transformer might be pre-trained on a large collection of books to understand language and then fine-tuned to generate marketing copy for a specific brand. The future potential of transformers is huge. Researchers are continuously improving them, making them even more powerful. We can expect more advanced applications in areas like healthcare, finance, and more sophisticated AI systems that interact with humans in more natural ways. Imagine having an AI that can provide personalized medical


03:04:34
advice or one that can help you write a novel. In conclusion, we can say that transformers are a revolutionary architecture in AI. They offer speed, efficiency, and versatility, changing how we interact with the technology. The future looks bright for transformers, and we can't wait to see what they'll do next. So now, what is RNN? RNN are a type of neural network that are designed to process sequential data. They can analyze data with temporal dimensions such as time series, speech, and text.


03:05:06
RN can do this by using a hidden state passed from one time stamp to the next. The next hidden state is updated at each other time step based on the input and the previous hidden state. RNN are able to capture short-term dependencies in sequential data but they struggle with capturing long-term dependencies. Why the LSTMs are made? So moving forward let's discuss types of LSTM gates. So LSTM models have three types of gates. The input gate, the forget gate and the output gate. So let's first discuss the input gate.


03:05:43
The input gate controls the flow of information into the memory cell deciding what to store. The input gate determines which values from the input should be updated in the memory cell. It uses a sigmoid activation function to scale the values between 0 and one and then applies pointwise multiplication to decide what information to store. Next is forget gate controls the flow of information out of the memory cell. Deciding what to discard. The forget gate decides what information should be discarded from the memory cell. It also


03:06:15
uses a zigmoid activation function to scale the values between zero and one followed by pointwise multiplication to determine what information to forget. The last one is output gate controls the flow of information out of the LSTM deciding what to use for the output. The output gate determines the output of the LSTM unit. It uses a sigmoid activation function to scale the values from 0 to one. Then applies f multiplication to produce the output of the LSTM unit. So these gates implemented using sigmoid


03:06:46
function are trained using back propagation. They open and close based on the input and the previous hidden state allowing the LSTM to selectively retain or discard information effectively capturing long-term dependencies. So now let's discuss application of LSTM. LSTM models are highly effective and used in various application including video analysis, analyzing video frames to identify action, object and scenes. The second is language simulation task like language modeling, machine translation and text


03:07:18
summarization. The third one is time series prediction. So predicting future values in a time series. The fourth is voice recognition tasks such as speech to text transcription and command recognition. The last one is sentiment analysis classifying test sentiment as positive negative on. So there are many more examples of LSTM. So now let's move forward and understand LSTM model and how it works with example. Let's consider the task of predicting the next word in a sentence. This is a common


03:07:48
application of LSTM networks in natural language processing. So I will break it down a step by step using the analogy of remembering a story and deciding what comes next based on the context. So imagine you are reading a story. As you read, you need to remember what has happened so far to predict what might happen next. So let's illustrate with a simple example sentence. The cat sat on the dash. So you want to predict the next word which could be matt or roof or something else. An LSTM network helps


03:08:18
this make prediction by remembering important parts of the story and forgetting irrelevant details. So now let's dive into step-by-step process. So step-by-step exclamation using LSTM. The first one is reading a story input sequence. As you read each word in the word sentence, you process it and store relevant information. For example, the you understand its determiner. Cat, you know it's a noun and the subject of the sentence. Said indicates the action performed by the subject. On preposition


03:08:49
indicating the relationship between the cat and the next noun. So this sequence diagram showing the words being read and processed. So second comes forget gate. As you move through the sentence, you might decide that some details are no longer important. For instance, you might decide that knowing the is less important now that you have kept and said. So the word forget gates help discard this less important information. So this sequence diagram you can see on the screen showing how relevant information is discarded. So the third


03:09:21
one input gate when you read on you need to decide how relevant this information is. So this sequence diagram in the screen is showing how new information is integrated with the last one. Okay. The fourth one is the cell state memory part. So this is like your memory of the story so far. It carries information about the subject cat and the action set on. So it updates the new information as you read each word. Okay, the cat set on the retaining the important context. So this sequence diagram showing how the


03:09:57
memory is updated with the new information. So the last one is output gate. When you need to predict the next word, the output gate helps you decide based on the current memory cell state. So it uses the context the cat set on the. So the predict the next word might be matt because cat and matt are often associated with the context. So it can predict anything. The cat sat on the table or on the sofa anything. But Matt, why I'm saying Matt? Because cat and matt are often associated in the same


03:10:31
context. So this diagram is showing the prediction of the next word based on the current memory. So there are many applications where you can use LSTM in predicting time series or next word in a sentence. So by the using of LSTM gates input gate, forget gate and output gate and updating the cell state the network can predict the next word in a sequence by maintaining relevant context and discarding unnecessary information. This step-by-step process allow LSTM network to effectively handle sequence and make


03:11:02
accurate prediction based on the context. LLMs. If you ever wondered how machine learning can now understand and generate humanlike text, you are in the right place. From chatboards like Chat GP to AI assistant that powers search engines, LLMs are transforming how we interact with technology. One of the most exciting advancement in this space is Google's Gemini or OpenAI Charging large language model designed to push the boundaries of what AI can achieve. In this video, we will explore what LLMs are, how they work, and why


03:11:34
models like Geminy are critical for the future of AI. Google Gemini is part of a new wave of AI models that are smarter, faster, and more efficient. It is designed to understand context better, offer more accurate responses and integrate deeply into service like Google search and Google Assistant, providing more humanlike interactions. So we will break down the science behind LLMs including their massive training data sets, transformer architecture and how models like Gemini use deep learning innovation to change industries. Plus we


03:12:05
will compare Google Gemini to other popular LMS such as OpenAI Chity models showing how each of these technologies is used to power chat bots, virtual assistants and other AIdriven application. By end of this video, you will have a clear understanding of how large language models like Gemini work, their key features, and what they mean for their future AI. Don't forget to like, subscribe, and hit the bell icon to never miss any update from Simply Learn. So, what are the large language models? Large language models like


03:12:33
Chargen pre-trained transformer 4 o and Google Gemini are sophisticated AI system designed to comprehend and generate humanlike text. These models are built using deep learning techniques and are trained on vast data set collected from the internet. They leverage self attention mechanism to analyze relationship between words or tokens allowing them to capture context and produce coherent relevant responses. LLMs have significant application including powering virtual assistant, chatboards, content creation, language


03:13:03
translation and supporting research and decision making. Their ability to generate fluent and contextually appropriate text has advanced natural language processing and improved human computer interaction. So now let's see what are large language model used for. Large language models are utilized in scenarios with limited or no domain specific data available for training. These scenarios include both few short and zero short training approaches which rely on the model's strong inductive


03:13:28
bias and its capability to derive meaningful representation from a small amount of data or even no data at all. So now let's see how are large language model trained. Large language models typically undergo pre-training on a board. All encompassing data set that shares statical similarities with the data set specific to the target task. The objective of pre-training is to enable the model to require highlevel feature that can later be applied during the finetuning phase for specific task. So there are some training processes of


03:13:58
LLM which involves several steps. The first one is text prep-processing. The textual data is transformed into a numerical representation that the LLM model can effectively process. This conversion may be involve techniques like tokenization and coding and creating input sequences. The second one is random parameter initialization. The model's parameter are initialized randomly before the training process begins. The third one is input numerical data. The numerical representation of the text data is fed into the model of


03:14:24
processing. The model's architecture typically based on transformers allows it to capture the conceptual relationship between the words or tokens in the next. The fourth one is loss function calculation. A loss function calculation measure the discrepancy between the model's prediction and the actual next word or token in a syntax. The LLM model aims to minimize this loss during training. The fifth one is parameter optimization. The model parameter are registered through optimization technique. This involves


03:14:50
calculating gradient and updating the parameters accordingly gradually improving the model's performance. The last one is iterative training. The training process is repeated over multiple iteration or epochs until the model's output achieve a satisfactory level of accuracy on that given task or data set. By following this training process, large language model learn to capture linguistic patterns, understand context and generate coherent responses enabling them to excel at various language related tasks. The next topic


03:15:17
is how do large language models work. So large language models leverage deep neural network to generate output based on patterns learned from the training data. Typically a large language model adopts a transformer architecture which enables the model to identify relationship between words in a sentence irrespective of their position in the sequence. In contrast to RNNs that rely on recurrence to capture token relationship transformer neural network employs self attention as their primary mechanism. Self attention calculates


03:15:46
attention scores that determine the importance of each token with respect to the other token in the text sequence facilitating the modeling of intricate relationship within the data. Next, let's see application of large language models. Large language models have a wide range of application across various domains. So here are some notable application. The first one is natural language processing NLP. Large language models are used to improve natural language understanding tasks such as sentiment analysis, named entity


03:16:13
recognition, text classification, and language modeling. The second one is chatbot and virtual assistant. Large language models power conversational agents, chatbots, and virtual assistant providing more interactive and humanlike user interaction. The third one is machine translation. LA language models have been used for automatic language translation enabling text translation between different languages with improved accuracy. The fourth one is sentiment analysis. LLMs can analyze and classify the sentiment or emotion


03:16:41
expressed in a piece of text which is valuable for market research, brand monitoring and social media analysis. The fifth one is content recommendation. These models can be employed to provide personalized content recommendations enhancing user experience and engagement on platforms such as news website or streaming services. So these application highlight the potential impact of large language models in various domains for improving language understanding automation. Looked at a lot of examples


03:17:07
of machine learning. So let's see if we can give a little bit more of a concrete definition. What is machine learning? Machine learning is the science of making computers learn and act like humans by feeding data and information without being explicitly programmed. We see here we have a nice little diagram where we have our ordinary system your computer nowadays you can even run a lot of the stuff on a cell phone because cell phones have advanced so much. And then with artificial intelligence and


03:17:34
machine learning it now takes the data and it learns from what happened before and then it predicts what's going to come next. And then really the biggest part right now in machine learning that's going on is it improves on that. How do we find a new solution? So we go from descriptive where it's learning about stuff and understanding how it fits together to predicting what it's going to do to postcripting coming up with a new solution. And when we're working on machine learning there's a


03:18:02
number of different diagrams that people have posted for what steps to go through. A lot of it might be very domain specific. So if you're working on photo identification versus language versus medical or physics, some of these are switched around a little bit or new things are put in. They're very specific to the domain. This is kind of a very general diagram. First, you want to define your objective. Very important to know what it is you're wanting to predict. Then you're going to be


03:18:28
collecting the data. So once you've defined an objective, you need to collect the data that matches. You spend a lot of time in data science collecting data and the next step preparing the data. You got to make sure that your data is clean going in. There's the old saying, bad data in, bad answer out or bad data out. And then once you've gone through and we've cleaned all this stuff coming in, then you're going to select the algorithm. Which algorithm are you going to use? You're going to train that


03:18:56
algorithm. In this case, I think we're going to be working with SVM, the support vector machine. Then you have to test the model. Does this model work? Is this a valid model for what we're doing? And then once you've tested it, you want to run your prediction. You want to run your prediction or your choice or whatever output it's going to come up with. And then once everything is set and you've done lots of testing, then you want to go ahead and deploy the model. And remember I said domain


03:19:21
specific. This is very general as far as the scope of doing something. A lot of models you get halfway through and you realize that your data is missing something and you have to go collect new data because you've run a test in here someplace along the line. You're saying, "Hey, I'm not really getting the answers I need." So there's a lot of things that are domain specific that become part of this model. This is a very general model, but it's a very good model to start with. And we do have some basic


03:19:44
divisions of what machine learning does. That's important to know. For instance, do you want to predict a category? Well, if you're categorizing thing, that's classification. For instance, whether the stock price will increase or decrease. So in other words, I'm looking for a yes no answer. Is it going up or is it going down? And in that case, we'd actually say, is it going up? True. If it's not going up, it's false, meaning it's going down. This way, it's a yes,


03:20:08
no. 01. Do you want to predict a quantity? That's regression. So remember, we just did classification. Now we're looking at regression. These are the two major divisions in what data is doing. For instance, predicting the age of a person based on the height, weight, health, and other factors. So based on these different factors, you might guess how old a person is. And then there are a lot of domainspecific things like do you want to detect an anomaly? That's anomaly detection. This is actually very popular


03:20:37
right now. For instance, you want to detect money withdrawal anomalies. You want to know when someone's making a withdrawal that might not be their own account. We've actually brought this up because this is really big right now. If you're predicting the stock, whether to buy stock or not, you want to be able to know if what's going on in the stock market is an anomaly. Use a different prediction model because something else is going on. and you got to pull out new information in there or is this just the


03:21:00
norm? I'm going to get my normal return on my money invested. So being able to detect anomalies is very big in data science these days. Another question that comes up which is on what we call untrained data is do you want to discover structure in unexplored data and that's called clustering. For instance, finding groups of customers with similar behavior given a large database of customer data containing their demographics and past buying records. And in this case, we might notice that anybody who's wearing


03:21:31
certain set of shoes goes shopping at certain stores or whatever it is, they're going to make certain purchases. By having that information, it helps us to market or group people together. So then we can now explore that group and find out what it is we want to market to them if you're in the marketing world. And that might also work in just about any arena. You might want to group people together, whether they're uh based on their different areas and investments and financial background,


03:21:58
whether you're going to give them a loan or not before you even start looking at whether they're valid customer for the bank. You might want to look at all these different areas and group them together based on unknown data. So you're not you don't know what the data is going to tell you, but you want to cluster people together that come together. Let's take a quick detour for quiz time. Oh, my favorite. So, we're going to have a couple questions here under our quiz time and um we'll be


03:22:22
posting the answers in these part two of this tutorial. So, let's go ahead and take a look at these quiz times questions and hopefully you'll get them all right and it'll get you thinking about how to process data and what's going on. Can you tell what's happening in the following cases? Of course, you're sitting there with your cup of coffee and you have your checkbox and your pen trying to figure out what's your next step in your data science analysis. So the first one is grouping


03:22:47
documents into different categories based on the topic and content of each document. Very big these days. You know, you have legal documents, you have uh maybe it's a sports group documents, maybe you're analyzing newspaper postings, but certainly having that automated is a huge thing in today's world. B identifying handwritten digits in images correctly. So we want to know whether uh they're writing an A or capital A B C what are they writing out in their hand digit their handwriting. C


03:23:19
behavior of a website indicating that the site is not working as designed. D predicting salary of an individual based on his or her years of experience the way HR hiring uh setup there. So stay tuned for part two. We'll go ahead and answer these questions when we get to the part two of this tutorial. or you can just simply write at the bottom and send a note to simply learn and they'll follow up with you on it. Back to our regular content. Now these last few bring us into the next topic which is


03:23:50
another way of dividing our types of machine learning and that is with supervised unsupervised and reinforcement learning. Supervised learning is a method used to enable machines to classify, predict objects, problems or situations based on labeled data fed to the machine. And in here you see we have a jumble of data with circles, triangles and squares. And we label them. We have what's a circle, what's a triangle, what's a square. And we have our model training and it trains it. So we know the answer. Very


03:24:21
important when you're doing supervised learning, you already know the answer to a lot of your information coming in. So you have a huge group of data coming in and then you have a new data coming in. So we've trained our model. The model now knows the difference between a circle, a square, a triangle. And now that we've trained it, we can send in in this case a square and a circle goes in and it predicts that the top one's a square and the next one's a circle. And you can see that this is uh being able


03:24:46
to predict whether someone's going to default on a loan because I was talking about banks earlier. Supervised learning on stock market, whether you're going to make money or not. That's always important. And if you are looking to make a fortune in the stock market, keep in mind it is very difficult to get all the data correct on the stock market. It is very uh it fluctuates in ways you really hard to predict. So it's quite a roller coaster ride. If you're running machine learning on the stock market,


03:25:12
you start realizing you really have to dig for new data. So we have supervised learning. And if you have supervised, we need unsupervised learning. In unsupervised learning, machine learning model finds the hidden pattern in an unlabeled data. So in this case, instead of telling it what the circle is or what a triangle is and what a square is, it goes in there, looks at them, and says for whatever reason, it groups them together. Maybe it'll group it by the number of corners, and it notices that a


03:25:39
number of them all have three corners, a number of them all have four corners, and a number of them all have no corners. And it's able to filter those through and group them together. We talked about that earlier with looking at a group of people who are out shopping. We want to group them together to find out what they have in common. And of course, once you understand what people have in common, maybe you have one of them who's a customer at your store, or you have five of them are customer at your store, and they have a


03:26:03
lot in common with five others who are not customers at your store. How do you market to those five who aren't customers at your store yet? They fit the demographics of who's going to shop there, and you'd like them to shop at your store, not the one next door. Of course, this is a simplified version. And you can see very easily the difference between a triangle and a circle which is might not be so easy in marketing. Reinforcement learning. Reinforcement learning is an important type of machine learning where an agent


03:26:26
learns how to behave in an environment by performing actions and seeing the result. And we have here where the in this case a baby. It's actually great that they used an infant for this slide because the reinforcement learning is very much in its infant stages. But it's also probably the biggest machine learning demand out there right now or in the future. It's going to be coming up over the next few years is reinforcement learning and how to make that work for us. And you can see here


03:26:53
where we have our action. In the action in this one, it goes into the fire. Hopefully the baby didn't just little candle, not a giant fire pit like it looks like here. When the baby comes out and the new state is the baby is sad and crying because they got burned on the fire. And then maybe they take another action. The baby's called the agent because it's the one taking the actions. And in this case, they didn't go into the fire. They went a different direction and now the baby's happy and


03:27:16
laughing and playing. Reinforcement learning is very easy to understand because that's how as humans, that's one of the ways we learn. We learn whether it is, you know, you burn yourself on the stove, don't do that anymore. Don't touch the stove. In the big picture, being able to have machine learning program or an AI be able to do this is huge because now we're starting to learn how to learn. That's a big jump in the world of computer and machine learning. And we're going to go back and just kind


03:27:43
of go back over supervised versus unsupervised learning. Understanding this is huge because this is going to come up in any project you're working on. We have in supervised learning, we have labeled data. We have direct feedback. So someone's already gone in there and said, "Yes, that's a triangle. No, that's not a triangle." And then you predict an outcome. So you have a nice prediction. this is this this new set of data is coming in and we know what it's going to be. And then with unsupervised


03:28:08
training, it's not labeled. So, we really don't know what it is. There's no feedback. So, we're not telling it whether it's right or wrong. We're not telling it whether it's a triangle or a square. We're not telling it to go left or right. All we do is we're finding hidden structure in the data, grouping the data together to find out what connects to each other. And then you can use these together. So imagine you have an image and you're not sure what you're


03:28:34
looking for. So you go in and you have the unstructured data, find all these things that are connected together and then somebody looks at those and labels them. Now you can take that label data and program something to predict what's in the picture. So you can see how they go back and forth and you can start connecting all these different tools together to make a bigger picture. There are many interesting machine learning algorithms. Let's have a look at a few of them. Hopefully this gave you a


03:29:00
little flavor of what's out there and these are some of the most important ones that are currently being used. We'll take a look at linear regression, decision tree, and the support vector machine. Let's start with a closer look at linear regression. Linear regression is perhaps one of the most well-known and well understood algorithms in statistics and machine learning. Linear regression is a linear model. For example, a model that assumes a linear relationship between the input variables


03:29:26
x and the single output variable y. And you'll see this if you remember from your algebra classes, y = mx + c. Imagine we are predicting distance traveled y from speed x. Our linear regression model representation for this problem would be y = m * x + c or distance= m * speed + c where m is the coefficient and c is the y intercept. And we're going to look at two different variations of this. First, we're going to start with time is constant. And you can see we have a bicyclist. He's got a


03:30:00
safety gear on. Thank goodness. Speed equals 10 meters/s. And so over a certain amount of time, his distance equals 36 kilometers. We have a second bicyclist who's going twice the speed or 20 m/s. And you can guess if he's going twice the speed and time is a constant, then he's going to go twice the distance. And that's easy to compute. 36 * 2, you get 72 km. And so if you had the question of how fast would somebody going three times that speed or 30 m/s is, you can easily compute the distance


03:30:32
in our head. We can do that without needing a computer. But we want to do this for more complicated data. So it's kind of nice to compare the two, but let's just take a look at that and what that looks like in a graph. So in a linear regression model, we have our distance to the speed and we have our m equals the ve slope of the line. And we'll notice that the line has a plus slope. And as the speed increases, distance also increases. Hence the variables have a positive relationship.


03:30:59
And so your speed of the person, which equals y= mx plus c, distance traveled in a fixed interval of time. And we could very easily compute either following the line or just knowing it's three times 10 m/s that this is roughly 102 km distance that this third bicycle has traveled. One of the key definitions on here is positive relationship. So the slope of the line is positive. As distance increase, so does speed increase. Let's take a look at our second example where we put distance is a constant. So we have speed equals 10


03:31:31
m/s. they have a certain distance to go and it takes him 100 seconds to travel that distance. And we have our second bicyclist who's still doing 20 meters/s. Since he's going twice the speed, we can guess he'll cover the distance in about half the time, 50 seconds. And of course, you could probably guess on the third one 100 / 30 since he's going three times the speed. You can easily guess that this is 33.33 seconds time. We put that into a linear regression model or a graph. If


03:31:59
the distance is assumed to be constant, let's see the relationship between speed and time a significant amount of data. Let's uh see what the mathematical implementation of linear regression and we'll take this data. So suppose we have this data set where we have xyx= 1 2 3 4 5 standard series and the y value is 3 22 43. When we take that and we go ahead and plot these points on a graph, you can see there's kind of a nice scattering and you could probably eyeball a line through the middle of it.


03:32:29
But we're going to calculate that exact line for linear regression. And the first thing we do is we come up here and we have the mean of Xi. And remember mean is basically the average. So we added five plus 4 plus 3 plus 2 + 1 and divide by five. And that simply comes out as three. And then we'll do the same for y. We'll go ahead and add up all those numbers and divide by five. And we end up with a mean value of y of i equals 2.8 where the x i references it's an average or means value. And the yi


03:33:00
also equals a means value of y. And when we plot that, you'll see that we can put in the y= 2.8 and the x= 3 in there on our graph. We kind of gave it a little different color so you could sort it out with the dash lines on it. And it's important to note that when we do the linear regression, the linear regression model should go through that dot. Now, let's find our regression equation to find the best fit line. Remember, we go ahead and take our y= mx plus c. So, we're looking for m and c. So, to find


03:33:28
this equation for our data, we need to find our slope of m and our coefficient of c. And we have y = mx + c where m equals the sum of x - x average * y - y average or y means and x means over the sum of x - x means squared. That's how we get the slope of the value of the line. And we can easily do that by creating some columns here. We have xy. Computers are really good about iterating through data. And so we can easily compute this and fill in a graph of data. And in our graph you can easily


03:34:04
see that if we have our x value of 1 and if you remember the x i or the means value is 3. 1 - 3 equals a -2 and 2 - 3 = a -1 so on and so forth. And we can easily fill in the column of x - x i y - yi. And then from those we can compute x - x i^ 2 and x - x i * y - yi. And you can guess it that the next step is to go ahead and sum the different columns for the answers we need. So we get a total of 10 for our x - x i^2 and a total of two for x - x i * y - yi. And we plug those in, we get 2/10, which equals2. So


03:34:47
now we know the slope of our line equals2. So we can calculate the value of c. That'd be the next step is we need to know where it crosses the y ais. And if you remember, I mentioned earlier that the linear regression line has to pass through the means value, the one that we showed earlier. We can just flip back up there to that graph. And you can see right here, there's our means value, which is 3, x= 3, and y= 2.8. And since we know that value, we can simply plug that into our formula. Y =2x + c. So we


03:35:20
plug that in, we get 2.8 8 =2 * 3 + c. And you can just solve for c. So now we know that our coefficient equals 2.2. And once we have all that, we can go ahead and plot our regression line. y =2 * x + 2.2. And then from this equation, we can compute new values. So let's predict the values of y using x= 1 2 3 4 5 and plot the points. Remember the 1 2 3 4 5 was our original x values. So now we're going to see what y thinks they are, not what they actually are. And we plug those in, we get y of designated


03:35:58
with y of p. You can see that x= 1= 2.4, x= 2= 2.6, and so on and so on. So we have our y predicted values of what we think it's going to be when we plug those numbers in. And when we plot the predicted values along with the actual values, we can see the difference. And this is one of the things that's very important with linear regression in any of these models is to understand the error. And so we can calculate the error on all of our different values. And you can see over here we plotted um x and y


03:36:27
and y predict. And we draw a little line so you can sort of see what the error looks like there between the different points. So our goal is to reduce this error. We want to minimize that error value on our linear regression model. Minimizing the distance. There are lots of ways to minimize the distance between the line and the data points like sum of squared errors, sum of absolute errors, root mean square error, etc. We keep moving this line through the data points to make sure the best fit line has the


03:36:55
least squared distance between the data points and the regression line. So to recap with a very simple linear regression model, we first figure out the formula of our line through the middle and then we slowly adjust the line to minimize the error. Keep in mind this is a very simple formula. The math gets even though the math is very much the same, it gets much more complex as we add in different dimensions. So this is only two dimensions. Y equals MX + C. But you can take that out to X, Z, Y, J,


03:37:24
Q, all the different features in there and they can plot a linear regression model on all of those using the different formulas to minimize the error. Let's go ahead and take a look at decision trees. A very different way to solve problems in the linear regression model. Decision tree is a treeshaped algorithm used to determine a course of action. Each branch of a tree represents a possible decision, occurrence, or reaction. We have data which tells us if it is a good day to play golf. And if we


03:37:52
were to open this data up in a general spreadsheet, you can see we have the outlook whether it's rainy, overcast, sunny, temperature, hot, mild, cool, humidity, windy, and did I like to play golf that day? Yes or no. So, we're taking a census. And certainly, I wouldn't want a computer telling me when I should go play golf or not. But you could imagine if you got up in the night before, you're trying to plan your day and it comes up and says, "Tomorrow would be a good day for golf for you in


03:38:20
the morning and not a good day in the afternoon or something like that." This becomes very beneficial. And we see this in a lot of applications coming out now where it gives you suggestions and lets you know what what would uh fit the match for you for the next day or the next purchase or the next uh whatever you know next mail out in this case is tomorrow a good day for playing golf based on the weather coming in. And so we come up and let's uh determine if you should play golf when the day is sunny


03:38:46
and windy. So we found out the forecast tomorrow is going to be sunny and windy. And suppose we draw our tree like this. We're going to have our humidity and then we have our normal, which is if it's if you have a normal humidity, you're going to go play golf. And if the humidity is really high, then we look at the outlook. And if the outlook is sunny, overcast, or rainy, it's going to change what you choose to do. So, if you know that it's a very high humidity and it's sunny, you're probably not going to


03:39:12
play golf cuz you're going to be out there miserable fighting off the mosquitoes that are out joining you to play golf with you. Maybe if it's rainy, you probably don't want to play in the rain. But if it's slightly overcast and you get just the right shadow, that's a good day to play golf and be outside out on the green. Now, in this example, you can probably make your own tree pretty easily. So, it's a very simple set of data going in. But the question is, how do you know what to split? Where do you


03:39:37
split your data? What if this is much more complicated data where it's not something that you would particularly understand like studying cancer? They take about 36 measurements of the cancerous cells and then each one of those measurements represents how bulbous it is, how extended it is, how sharp the edges are, something that as a human we would have no understanding of. So how do we decide how to split that data up and is that the right decision tree? But so that's a question that's


03:40:04
going to come up. Is this the right decision tree? For that we should calculate entropy and information gain. Two important vocabulary words there are the entropy and the information gain. Entropy. Entropy is a measure of randomness or impurity in the data set. Entropy should be low. So we want the chaos to be as low as possible. We don't want to look at it and be confused by the images or what's going on there with mixed data. And the information gain, it is a measure of decrease in entropy


03:40:34
after the data set is split. Also known as entropy reduction. information gain should be high. So we want our information that we get out of the split to be as high as possible. Let's take a look at entropy from the mathematical side. In this case, we're going to denote entropy as I of P of and N where P is the probability that you're going to play a game of golf and N is the probability where you're not going to play the game of golf. Now, you don't really have to memorize these formulas.


03:41:05
There's a few of them out there depending on what you're working with. But it's important to note that this is where this formula is coming from. So when you see it, you're not lost when you're running your programming, unless you're building your own decision tree code in the back. And we simply have a log squar of p + n minus n / p + n * the log squar of n of p + n. But let's break that down and see what actually looks like when we're computing that from the computer script side. Entropy of a


03:41:31
target class of the data set is the whole entropy. So we have entropy play golf and we look at this. If we go back to the data you can simply count how many yeses and no in our complete data set for playing golf days. In our complete set we find we have five days we did play golf and 9 days we did not play golf. And so our I equals if you add those together 9 + 5 is 14. And so our I equals 5 over 14 and 9 over 14. That's our P andn values that we plug into that formula. And you can go 5 over


03:42:05
14als.36. 9 over 14= 64. And when you do the whole equation, you get the minus.36 log<unk>^2 of.36 -.64 log<unk> of 64. And we get a set value. We get 94. So we now have a full entropy value for the whole set of data that we're working with. And we want to make that entropy go down. And just like we calculated the entropy out for the whole set, we can also calculate entropy for playing golf and the outlook. Is it going to be overcast or rainy or sunny? And so we look at the entropy. We have P


03:42:42
of sunny time E of three of two. And that just comes out how many sunny days yes and how many sunny days no over the total which is five. Don't forget to put the we'll divide that five out later on. equals P overcast = 4 comma 0 plus rainy = 2a 3 and then when you do the whole setup we have 5 over4 remember I said there was a total of five 5 over 14 * the i of 3 of 2 + 4 over 14 * the 4 0 and 514 over i of 23 and so we can now compute the entropy of just the part that has to do with the forecast and we


03:43:22
get 693 similar We can calculate the entropy of other predictors like temperature, humidity, and wind. And so we look at the gain outlook. How much are we going to gain from this entropy play golf minus entropy play golf outlook? And we can take the original 0.94 for the whole set minus the entropy of just the rainy day and temperature and we end up with a gain of.247. So this is our information gain. Remember we define entropy and we define information gain. The higher the information gain, the lower the entropy,


03:43:55
the better. The information gain of the other three attributes can be calculated in the same way. So we have our gain for temperature equals 0.029. We have our gain for humidity equals.152. And our gain for a windy day equals 0048. And if you do a quick comparison, you'll see the.247 is the greatest gain of information. So that's the split we want. Now let's build the decision tree. So, we have the outlook. Is it going to be sunny, overcast, or rainy? That's our first split because that gives us the


03:44:26
most information gain. And we can continue to go down the tree using the different information gains with the largest information. We can continue down the nodes of the tree where we choose the attribute with the largest information gain as the root node and then continue to split each subnode with the largest information gain that we can compute. And although it's a little bit of a tongue twister to say all that, you can see that it's a very easy to view visual model. We have our outlook. We


03:44:52
split it three different directions. If the outlook is overcast, we're going to play. And then we can split those further down if we want. So if the over outlook is sunny, but then it's also windy. If it's uh windy, we're not going to play. If it's uh not windy, we'll play. So, we can easily build a nice decision tree to guess what we would like to do tomorrow and give us a nice recommendation for the day. So, we want to know if it's a good day to play golf when it's sunny and windy. Remember the


03:45:18
original question that came out, tomorrow's weather report is sunny and windy. You can see by going down the tree, we go outlook sunny, outlook windy. We're not going to play golf tomorrow. So, our little smartwatch pops up and says, I'm sorry, tomorrow's not a good day for golf. It's going to be sunny and windy. And if you're a huge golf fan, you might go, "Uhoh, it's not a good day to play golf." We can go in and watch a golf game at home. So, we'll sit in front of the TV instead of being


03:45:43
out playing golf in the wind. Now that we looked at our decision tree, let's look at the third one of our algorithms we're investigating. Support vector machine. Support vector machine is a widely used classification algorithm. The idea of support vector machine is simple. The algorithm creates a separation line which divides the classes in the best possible manner. For example, dog or cat, disease or no disease. Suppose we have a labeled sample data which tells height and weight of males and females. A new data


03:46:12
point arrives and we want to know whether it's going to be a male or a female. So we start by drawing a line. We draw decision lines. But if we consider decision line one, then we will classify the individual as a male. And if we consider decision line two, then it'll be a female. So you can see this person kind of lies in the middle of the two groups. So it's a little confusing trying to figure out which line they should be under. We need to know which line divides the classes correctly. But


03:46:37
how the goal is to choose a hyper plane and that is one of the key words they use when we talk about support vector machines. Choose a hyper plane with the greatest possible margin between the decision line and the nearest point within the training set. So you can see here we have our support vector. we have the two nearest points to it and we draw a line between those two points and the distance margin is the distance between the hyper plane and the nearest data point from either set. So we actually


03:47:06
have a value and it should be equal distance between the two points that we're comparing it to. When we draw the hyperplanes we observe that line one has a maximum distance. So we observe that line one has a maximum distance margin. So we'll classify the new data point correctly. And our result on this one is going to be that the new data point is MEL. One of the reasons we call it a hyper plane versus a line is that a lot of times we're not looking at just weight and height. We might be looking


03:47:36
at 36 different features or dimensions. And so when we cut it with a hyper plane, it's more of a three-dimensional cut in the data. Multi-dimensional that cuts the data a certain way. and each plane continues to cut it down until we get the best fit or match. Let's understand this with the help of an example problem statement. You always start with a problem statement when you're going to put some code together. We're going to do some coding now. Classifying muffin and cupcake recipes


03:48:01
using support vector machines. So, the cupcake versus the muffin. Let's have a look at our data set. And we have the different recipes here. We have a muffin recipe that has so much flour. I'm not sure what measurement 55 is in, but it has 55, maybe it's ounces, but it has a certain amount of flour, certain amount of milk, sugar, butter, egg, baking powder, vanilla, and salt. And so based on these measurements, we want to guess whether we're making a muffin or a cupcake. And you can see in this one, we


03:48:31
don't have just two features. We don't just have height and weight as we did before between the male and female. In here, we have a number of features. In fact, in this we're looking at eight different features to guess whether it's a muffin or a cupcake. What's the difference between a muffin and a cupcake? Turns out muffins have more flour while cupcakes have more butter and sugar. So basically the cupcakes a little bit more of a dessert where the muffin's a little bit more of a fancy


03:48:58
bread. But how do we do that in Python? How do we code that to go through recipes and figure out what the recipe is? And I really just want to say cupcakes versus muffins like some big professional wrestling thing. Before we start in our cupcakes versus muffins, we are going to be working in Python. There's many versions of Python, many different editors. That is one of the strengths and weaknesses of Python is it just has so much stuff attached to it. It's one of the more popular data science programming packages you can


03:49:29
use. In this case, we're going to go ahead and use Anaconda and Jupyter Notebook. The Anaconda Navigator has all kinds of fun tools. Once you're into the Anaconda Navigator, you can change environments. I actually have a number of environments on here. We'll be using Python 36 environment. So, this is in Python version 36. Although, it doesn't matter too much which version you use. I usually try to stay with the 3x because they're current unless you have a project that's very specifically in


03:49:59
version 2x. 2.7 I think is usually what most people use in the version two. And then once we're in our um Jupiter notebook editor, I can go up and create a new file and we'll just jump in here. In this case, we're doing SVM muffin versus cupcake. And then let's start with our packages for data analysis. And we almost always use a couple there's a few very standard packages we use. We use import oops import numpy that's for number python. They usually denote it as np that's very


03:50:38
comma that's very common. And then we're going to import pandas as pd. And numpy deals with number arrays. There's a lot of cool things you can do with the numpy uh setup as far as multiplying all the values in an array in a numpy array. Data array pandas I can't remember if we're using it actually in this data set. I think we do as an import. It makes a nice data frame. And the difference between a data frame and a numpy array is that a data frame is more like your Excel spreadsheet. You have


03:51:09
columns, you have indexes. So you have different ways of referencing it easily viewing it. And there's additional features you can run on a data frame. And pandas kind of sits on numpy. So they you need them both in there. And then finally, we're working with the support vector machine. So from sklearn, we're going to use the sklearn model. Import svm support vector machine. And then as a data scientist, you should always try to visualize your data. Some data obviously is too complicated or


03:51:44
doesn't make any sense to the human. But if it's possible, it's good to take a second look at it so that you can actually see what you're doing. Now, for that, we're going to use two packages. We're going to import mapplot library.pipplot as plt. Again, very common. And we're going to import seabor as sns. And we'll go ahead and set the font scale in the SNS right in our import line. That's what this U semicolon followed by a line of data. We're going to set the SNS. And these


03:52:13
are great because the the seabour sits on top of map plot library just like pandas sits on numpy. So it adds a lot more features and uses and control. We're obviously not going to get into mattplot library and seabour. It' be its own tutorial. We're really just focusing on the SVM, the support vector machine from sklearn. And since we're in Jupiter notebook, uh we have to add a special line in here for our mattplot library. And that's your percentage sign or amber sign mattplot library in line. Now, if


03:52:46
you're doing this in just a straight code project, a lot of times I use like Notepad++ and I'll run it from there. You don't have to have that line in there because it'll just pop up as its own window on your computer depending on how your computer's set up because we're running this in the Jupyter notebook as a browser setup. This tells it to display all of our graphics right below on the page. So that's what that line is for. I remember the first time I ran this I didn't know that and I had to go


03:53:14
look that up years ago. It's quite a headache. So mapplot library inline is just because we're running this on the web setup and we can go ahead and run this. make sure all our modules are in. They're all imported, which is great. If you don't have them import, you'll need to go ahead and pip. Use the pip or however you do it. There's a lot of other install packages out there, although pip is the most common. And you have to make sure these are all installed on your Python setup. The next


03:53:39
step, of course, is we got to look at the data. You can't run a model for predicting data if you don't have actual data. So, to do that, let me go ahead and open this up and take a look. And we have our uh cupcakes versus muffins. and it's a CSV file or CSV meaning that it's commaepparated variable and it's going to open it up in a nice uh spreadsheet for me and you can see up here we have the type we have muffin muffin muffin cupcake cupcake cupcake and then it's broken up into flour milk sugar egg


03:54:09
baking powder vanilla and salt so we can do is we can go ahead and look at this data also in our Python let us create a variable recipes equals We're going to use our pandas module read CSV. Remember it was a commaepparated variable and the file name happened to be cupcakes versus muffins. Oops, I got double brackets there. Do it this way. There we go. Cupcakes versus muffins. because the program I loaded or the the place I saved this particular Python program is in the same folder, we can get by with just the file name. But


03:54:56
remember, if you're storing it in a different location, you have to also put down the full path on there. And then because we're in pandas, we're going to go ahead and you can actually in line you can do this, but let me do the full print. You can just type in recipes.head head in the Jupyter notebook. But if you're running in code in a different script, you'd need to go ahead and type out the whole print recipes. And Pandanda's nose is that's going to do the first five lines of


03:55:26
data. And if we flip back on over to the spreadsheet where we opened up our CSV file, uh you can see where it starts on line two. This one calls it zero. And then 2 3 4 5 6 is going to match. Go and close that out because we don't need that anymore. And it always starts at zero. And these are it automatically indexes it since we didn't tell it to use an index in here. So that's the index number for the left hand side. And it automatically took the top row as labels. So pandas using it to read a CSV


03:56:00
is just really slick and fast. One of the reasons we love our pandas, not just because they're cute and cuddly teddy bears. And let's go ahead and plot our data. And I'm not going to plot all of it. I'm just going to plot the uh sugar and flour. Now, obviously, you can see where they get really complicated if we have tons of different features. And so, you'll break them up and maybe look at just two of them at a time to see how they connect. And to plot them, we're going


03:56:31
to go ahead and use Seabor. So, that's our SNS. And the command for that is snplot. And then the two different variables I'm going to plot is flour and sugar. Data equals recipes. The hue equals type. And this is a lot of fun because it knows that this is pandas coming in. So this is one of the powerful things about pandas mixed with seabour and doing graphing. And then we're going to use a pallet set one. There's a lot of different sets in there. You can go look them up for seabour. We'll do a regular


03:57:08
fit regular equals false. So, we're not really trying to fit anything. And it's a scatter KWS. A lot of these settings you can look up in Seabor. Half of these you could probably leave off when you run them. Somebody played with this and found out that these were the best settings for doing a Seabor plot. And let's go ahead and run that. And because it does it in line, it just puts it right on the page. And you can see right here that just based on sugar and flour alone, there's


03:57:37
a definite split. And we use these models because you can actually look at it and say, "Hey, if I drew a line right between the middle of the blue dots and the red dots, we'd be able to do an SVM and and a hyper plane right there in the middle." Then the next step is to format or pre-process our data. And we're going to break that up into two parts. We need a type label. And remember, we're going to decide whether it's a muffin or a cupcake. Well, a computer doesn't know muffin or cupcake.


03:58:16
It knows zero and one. So, what we're going to do is we're going to create a type label. And from this we'll create a numpy array np where and this is where we can do some logic. We take our recipes from our panda and wherever type equals muffin it's going to be zero. And then if it doesn't equal muffin which is cupcakes it's going to be one. So we create our type label. This is the answer. So when we're doing our training model remember we have to have a a training data. This is what we're going


03:58:49
to train it with is that it's zero or one. it's a muffin or it's not. And then we're going to create our recipe features. And if you remember correctly from right up here, the first column is type. So we really don't need the type column because that's our muffin or cupcake. And in pandas, we can easily sort that out. We take our value recipes. That's a pandas function built into pandas. values converting them to values. So it's just the column titles going across


03:59:26
the top and we don't want the first one. So what we do is since it always starts at zero, we want one colon till the end. And then we want to go ahead and make this a list. And this converts it to a list of strings. And then we can go ahead and just take a look and see what we're looking at for the features. Make sure it looks right. Me go ahead and run that. And I forgot the S on recipes. So, we'll go ahead and add the S in there and then run that. And we can see we have flour, milk, sugar, butter, egg,


04:00:03
baking powder, vanilla, and salt. And that matches what we have up here, right? We printed out everything but the type. So, we have our features and we have our label. Now, the recipe features is just the titles of the columns. We actually need the ingredients. And at this point, we have a couple options. One, we could run it over all the ingredients. And when you're doing this, usually you do, but for our example, we want to limit it so you can easily see what's going on because if we did all


04:00:37
the ingredients, we have, you know, that's what, um, seven, eight different hyperplanes that would be built into it. We only want to look at one so you can see what the SVM is doing. And so we'll take our recipes and we'll do just flour and sugar. Again, you can replace that with your recipe features and do all of them, but we're going to do just flour and sugar. And we're going to convert that to values. We don't need to make a list out of it because it's not string values. These


04:01:04
are actual values on there. And we can go ahead and just print ingredients. And you can see what that looks like. Uh, and so we have just the nan of flour and sugar, just the two sets of plots. And just for fun, let's go ahead and take this over here and take our recipe features. And so if we decided to use all the recipe features, you'll see that it makes a nice column of different data. So it just strips out all the labels and everything. We just have just the values. But because we want to be


04:01:40
able to view this easily in a plot later on, we'll go ahead and take that and just do flour and sugar. And we'll run that. And you'll see it's just the two columns. So the next step is to go ahead and fit our model. We'll go ahead and just call it model. And it's a SVM. We're using a package called SVC. In this case, we're going to go ahead and set the kernel equals linear. So, it's using a specific setup on there. And if we go to the reference on their website for the


04:02:16
SVM, you'll see that there's about there's eight of them here. Three of them are for regression. Three are for classification. The SVC, support vector classification, is probably one of the most commonly used. And then there's also one for detecting outliers and another one that has to do with something a little bit more specific on the model. But SBC and SVR are the two most commonly used standing for support vector classifier and support vector regression. Remember regression is an


04:02:45
actual value, a float value or whatever you're trying to work on. And SBC is a classifier. So it's a yes, no, true false. But for this we want to know 01 muffin cupcake. We go ahead and create our model. And once we have our model created, we're going to do model.fit. And this is very common, especially in the sklearn. All their models are followed with the fit command. And what we put into the fit, what we're training with it is we're putting in the ingredients, which in this case we limited to just flour and


04:03:17
sugar, and the type label. Is it a muffin or a cupcake? Now, in more complicated data science series, you'd want to split into, we won't get into that today, where you split it into training data and test data. And they even do something where they split it into thirds, where a third is used for where you switch between which one's training and test. There's all kinds of things go into that. It gets very complicated when you get to the higher end. Not overly complicated, just an extra step, which we're not


04:03:47
going to do today because this is a very simple set of data. And let's go ahead and run this. And now we have our model fit. And I got an error here. So let me fix that real quick. It's capital SPC. It turns out I did it lowercase. Support vector classifier. There we go. Let's go ahead and run that. And you'll see it comes up with all this information that it prints out automatically. These are the defaults of the model. You notice that we changed the kernel to linear. And there's our kernel linear on the


04:04:20
printout. And there's other different settings you can mess with. We're going to just leave that alone for right now. For this, we don't really need to mess with any of those. So, next we're going to dig a little bit into our newly trained model. And we're going to do this so we can show you on a graph. And let's go ahead and get the separating. and we're going to say uh we're going to use a W for our variable on here and we're going to do model.coreefficient_0. So what the heck


04:04:57
is that? Again, we're digging into the model. So we've already got a prediction and a train. This is a math behind it that we're looking at right now. And so the w is going to represent two different coefficients. And if you remember, we had y = mx + c. So these coefficients are connected to that but in two-dimensional it's a plane. We don't want to spend too much time on this because you can get lost in the confusion of the math. So if you're a math wiz this is great. You can go


04:05:30
through here and you'll see that we have a= minus w of 0 over w of 1. Remember there's two different values there. And that's basically the slope that we're generating. And then we're going to build an xx. What is xx? We're going to set it up to a numpy array. There's our np line space. So we're creating a line of values between 30 and 60. So it just creates a set of numbers for x. And then if you remember correctly, we have our formula y equ= the slope * x plus the intercept. Well, to make this


04:06:12
work, we can do this as y equals the slope times each value in that array. That's the neat thing about numpy. So, when I do a * xx, which is a whole numpy array of values, it multiplies a across all of them. And then it takes those same values and we subtract the model intercept. That's your uh we had mx plus c. So, that'd be the c from the formula yals mx plus c. And that's where all these numbers come from. A little bit confusing because it's digging out of these different


04:06:45
arrays. And then we want to do is we're going to take this and we're going to go ahead and plot it. So plot the parallels to separating hyper plane that pass through the support vectors. And so we're going to create B equals a model support vectors. Pulling our support vectors out there. Here's our y, which we now know is a set of data. We have uh we're going to create y down = a * xx + b1 - a * b 0 and then model support vector b is going to be set that to a new value the minus1 setup and y y up =


04:07:22
a * xx + b1 - a * b 0. And we can go ahead and just run this to load these variables up. If you wanted to know understand a little bit more what's going on, you can see if we print y, we just run that. You can see it's an array. This is a line. It's going to have in this case between 30 and 60. So there's going to be 30 variables in here. And the same thing with y y up y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y y down and we'll we'll plot


04:07:53
those in just a minute on a graph so you can see what those look like. Just go ahead and delete that out of here and run that. So, it loads up the variables. Nice clean slate. I'm just going to copy this from before. Remember this? Our SNS, our Seabor plot, LM plot, flower, sugar. And I'll just go and run that real quick so you can see what remember what that looks like. It's just a straight graph on there. And then one of the neat things is because Seabor sits on top of pipplot, we can do the piplot for the


04:08:24
line going through. And that is simply plt.plot And that's our xx and y are two corresponding values xy. And then somebody played with this to figure out that the line width equals 2 and the color black would look nice. So let's go ahead and run this whole thing with the pie plot on there. And you can see when we do this, it's just doing flour and sugar on here. Corresponding line between the sugar and the flour and the muffin versus cupcake. Um, and then we generated the U support vectors, the yy down and y up. So let's


04:09:04
take a look and see what that looks like. So we'll do our plot. And again, this is all against xx, our x value, but this time we have y down. And let's do something a little fun with this. We can put in a k dash dash. That just tells it to make it a dotted line. And if we're going to do the down one, we also want to do the up one. So here's our y up. And when we run that, it adds both sets of line. And so here's our support. And this is what you expect. You expect these two lines to go through the


04:09:47
nearest data point. So the dash lines go through the nearest muffin and the nearest cupcake when it's plotting it. And then your SVM goes right down the middle. So it gives it a nice split in our data. And you can see how easy it is to see based just on sugar and flour which one's a muffin or a cupcake. Let's go ahead and create a function to predict muffin or cupcake. I've got my um recipes I pulled off the um internet and I want to see the difference between a muffin or a cupcake. And so we need a


04:10:27
function to push that through. And uh we create a function with def. And let's call it muffin or cupcake. And remember we're just doing flour and sugar today. We're not doing all the ingredients. And that actually is a pretty good split. You really don't need all the ingredients to know it's flour and sugar. And let's go ahead and do an if else statement. So if model predict is of flour and sugar equals zero. So we take our model and we do run a predict. It's very common in sklearn


04:10:57
where you have a predict. You put the data in and it's going to return a value. In this case if it equals zero then print you're looking at a muffin recipe. Else if it's not zero that means it's one then you're looking at a cupcake recipe. That's pretty straightforward for function or def for definition. Def is how you do that in Python. And of course, if you're going to create a function, you should run something in it. And so, let's run a cupcake. And we're going to send it values 50 and 20.


04:11:26
A muffin or a cupcake. I don't know what it is. And let's run this and just see what it gives us. It says, "Oh, it's a muffin. You're looking at a muffin recipe." So, it very easily predicts whether we're looking at a muffin or a cupcake recipe. Let's plot this. There we go. Plot this on the graph so we can see what that actually looks like. And I'm just going to copy and paste it from below where we're plotting all the points in there. So, this is nothing


04:11:53
different than we did before. If I run it, you'll see it has all the points and the lines on there. And what we want to do is we want to add another point. And we'll do pltot. And if you remember correctly, we did for our test we did 50 and 20. And then somebody went in here and decided we'll do YO for yellow or it's kind of a orangeish yellow color is going to come up. Marker size nine. Those are settings you can play with. Somebody else played with them to come up with the right setup so it looks


04:12:24
good. And you can see there it is graphed. Clearly a muffin. In this case in cupcakes versus muffins, the muffin has won. And if you'd like to do your own muffin cupcake contender series, you certainly can send a note down below and the team at SimplyLearn will send you over the data they use for the muffin and cupcake. And that's true of any of the data. We didn't actually run a plot on it earlier. We had men versus women. You can also request that information to run it on your data setup. So you can test


04:12:58
that out. So to go back over our setup, we went ahead for our support vector machine code. We did a predict 40 parts flour, 20 parts sugar. I think it was different than the one we did whether it's a muffin or a cupcake. Hence, we have built a classifier using SVM which is able to classify if a recipe is of a cupcake or a muffin. Which wraps up our cupcake versus muffin. What's in it for you? We're going to cover clustering. What is clustering? K means clustering which is one of the most common used


04:13:31
clustering tools out there including a flowchart to understand K means clustering and how it functions and then we'll do an actual Python live demo on clustering of cars based on brands. Then we're going to cover logistic regression. What is logistic regression? Logistic regression curve and sigmoid function. And then we'll do another Python code demo to classify a tumor as malignant or benign based on features. And let's start with clustering. Suppose we have a pile of books of different


04:14:01
genres. Now we divide them into different groups like fiction, horror, education, and as we can see from this young lady, she definitely is into heavy horror. You can just tell by those eyes and the maple Canadian leaf on her shirt. But we have fiction, horror, and education. And we want to go ahead and divide our books up. Well, organizing objects into groups based on similarity is clustering. And in this case, as we're looking at the books, we're talking about clustering things with known categories. But you can also use


04:14:30
it to explore data. So you might not know the categories. You just know that you need to divide it up in some way to conquer the data and to organize it better. But in this case, we're going to be looking at clustering in specific categories. And let's just take a deeper look at that. We're going to use K means clustering. K means clustering is probably the most commonly used clustering tool in the machine learning library. K means clustering is an example of unsupervised learning. If you


04:14:56
remember from our previous thing, it is used when you have unlabeled data. So we don't know the answer yet. We have a bunch of data that we want to cluster to different groups. Define clusters in the data based on feature similarity. So we've introduced a couple terms here. We've already talked about unsupervised learning and unlabeled data. So we don't know the answer yet. We're just going to group stuff together and see if we can find an unanswer connect. We've also introduced


04:15:24
feature similarity. Features being different features of the data. Now, with books, we can easily see fiction and horror and history books. But a lot of times with data, some of that information isn't so easy to see right when we first look at it. And so, K means is one of those tools where we can start finding things that connect that match with each other. Suppose we have these data points and want to assign them into a cluster. Now when I look at these data points, I would probably group them into two clusters just by


04:15:52
looking at them. I'd say two of these group of data kind of come together. But in K means we pick K clusters and assign random centrids to clusters where the K clusters represents two different clusters. We pick K clusters and say random centroids to the clusters. Then we compute distance from objects to the centrids. Now we form new clusters based on minimum distances and calculate the centrids. So we figure out what the best distance is for the centrid. Then we move the centrid and recalculate those


04:16:23
distances. Repeat previous two steps iteratively till the cluster centroid stop changing their positions and become static. Repeat previous two steps iteratively till the cluster centroid stop changing and the positions become static. Once the clusters become static, then K means clustering algorithm is said to be converged. And there's another term we see throughout machine learning is converged. That means whatever math we're using to figure out the answer has come to a solution or it's converged on an answer. Shall we


04:16:52
see the flowchart to understand make a little bit more sense by putting it into a nice easy step by step? So we start, we choose K. We'll look at the elbow method in just a moment. We assign random centrids to clusters and sometimes you pick the centrids because you might look at the data in a in a graph and say ah these are probably the central points. Then we compute the distance from the objects to the centrids. We take that and we form new clusters based on minimum distance and calculate their centrids. Then we


04:17:22
compute the distance from objects to the new centrids. And then we go back and repeat those last two steps. We calculate the distances. So as we're doing it, it brings into the new centrid and then we move the centrid around and we figure out what the best which objects are closest to each centrid. So the objects can switch from one centroid to the other as the centroidids are moved around and we continue that until it is converged. Let's see an example of this. Suppose we have this data set of


04:17:49
seven individuals and their score on two topics A and B. Uh so here's our subject in this case referring to the person taking the uh test and then we have subject A where we see what they've scored on their first subject and we have subject B and we can see what they score on the second subject. Now let's take two farthest apart points as initial cluster centroidids. Now remember we talked about selecting them randomly or we can also just put them in different points and pick the furthest


04:18:17
one apart so they move together. Either one works okay depending on what kind of data you're working on and what you know about it. So we took the two furthest points one and one and five and seven. And now let's take the two farthest apart points as initial cluster centrids. Each point is then assigned to the closest cluster with respect to the distance from the centrids. So we take each one of these points in there. We measure that distance. And you can see that if we measured each of those


04:18:43
distances and you use the the Pythagorean theorem for a triangle in this case because you know the x and the y and you can figure out the diagonal line from that or you just take a ruler and put it on your monitor. That'd be kind of silly but it would work if you're just eyeballing it. You can see how they naturally come together in certain areas. Now we again calculate the centroidids of each cluster. So cluster one and then cluster two and we look at each individual dot. There's one, two, three. We're in one cluster.


04:19:13
Uh the centrid then moves over. It becomes 1.8 comma 2.3. So remember it was at 1 and one. Well, the very center of the data we're looking at would put it at the one point roughly 22, but 1.8 and 2.3. And the second one, if we wanted to make the overall mean vector, the average vector of all the different distances to that centrid, we come up with 4, 1, and 54. So we've now moved the centrids. We compare each individual's distance to its own cluster mean and to that of the opposite cluster


04:19:43
and we find build a nice chart on here that the as we move that centrid around we now have a new different kind of clustering of groups and using uklidian distance between the points and the mean we get the same formula you see new formulas coming up. So we have our individual dots distance to the mean centrid of the cluster and distance to the mean centrid of the cluster. Only individual three is nearer to the mean of the opposite cluster cluster two than its own cluster one. And you can see here in the diagram where we've kind of


04:20:11
circled that one in the middle. So when we've moved the clust the centroidids of the clusters over one of the points shifted to the other cluster because it's closer to that group of individuals. Thus, individual 3 is relocated to cluster 2, resulting in a new partition. And we regenerate all those numbers of how close they are to the different clusters. For the new clusters, we will find the actual cluster centroidids. So now we move the centrids over. And you can see that we've now formed two very distinct


04:20:39
clusters on here. On comparing the distance of each individual's distance to its own cluster mean and to that of the opposite cluster, we find that the data points are stable. Hence, we have our final clusters. Now if you remember I brought up a concept earlier K mean on the K means algorithm choosing the right value of K will help in less number of iterations and to find the appropriate number of clusters in a data set we use the elbow method and within sum of squares WSS is defined as the sum of the


04:21:09
squared distance between each member of the cluster and its centrid and so you see we've done here is we have the number of clusters and as you do the same K means algorithm over the different clusters and you calculate what that centrid looks like and you find the optimal you can actually find the optimal number of clusters using the elbow the graph is called as the elbow method and on this we guessed at two just by looking at the data but as you can see the slope you actually just look for right there where the elbow is in


04:21:38
the slope and you have a clear answer that we want two different to start with k means equals two a lot of times people end up computing k means equals 2 3 four five until they find the value which fits on the elbow joint. Sometimes you can just look at the data and if you're really good with that specific domain remember domain I mentioned that last time you'll know that that where to pick those numbers and where to start guessing at what that K value is. So let's take this and we're going to use a


04:22:06
use case using K means clustering to cluster cars into brands using parameters such as horsepower, cubic inches, make, year, etc. So, we're going to use the data set cars data having information about three brands of cars, Toyota, Honda, and Nissan. We'll go back to my favorite tool, the Anaconda Navigator with the Jupiter notebook. And let's go ahead and flip over to our Jupyter notebook. And in our Jupyter notebook, I'm going to go ahead and just paste the uh basic code that we usually


04:22:38
start a lot of these off with. We're not going to go too much into this code because we've already discussed numpy, we've already discussed mapplot library and pandas. Numpy being the number array, pandas being the pandas data frame and mattplot for the graphing. And don't forget uh since if you're using the Jupyter notebook, you do need the mattplot library in line so that it plots everything on the screen. If you're using a different Python editor, then you probably don't need that


04:23:03
because it'll have a popup window on your computer. And we'll go ahead and run this just to load our libraries and our setup into here. The next step is of course to look at our data which I've already opened up in a spreadsheet. And you can see here we have the miles per gallon, cylinders, cubic inches, horsepower, weight pounds, how you know how heavy it is, time it takes to get to 60. My card is probably on this one at about 80 or 90. What year it is? So this is you can actually see this is kind of


04:23:32
older cars and then the brand Toyota, Honda, Nissan. So the different cars are coming from all the way from 1971 if we scroll down to uh the 80s. We have between the 70s and 80s a number of cars that they've put out. And let's uh we come back here. We're going to do importing the data. So we'll go ahead and do data set equals and we'll use pandas to read this in. And it's uh from a CSV file. Remember, you can always post this in the comments and request the data files for these either in the


04:24:02
comments here on the YouTube video or go to simplylearn.com and request that. The cars CSV, I put it in the same folder as the code that I've stored. So, my Python code is stored in the same folder. So, I don't have to put the full path. If you store them in different folders, you do have to change this. And double check your name variables. And we'll go ahead and run this. And uh we've chosen data set arbitrarily because, you know, it's a data set we're importing. And we've


04:24:25
now imported our car CSV into the data set. As you know, you have to prep the data. So, we're going to create the X data. This is the one that we're going to try to figure out what's going on with. And then there is a number of ways to do this, but we'll do it in a simple loop so you can actually see what's going on. So, we'll do for i and x.c columns. So, we're going to go through each of the columns. And a lot of times it's important I I'll make lists of the


04:24:52
columns and do this because I might remove certain columns or there might be columns that I want to be processed differently. But for this we can go ahead and take x of i and we want to go fill na and that's a pandas command. But the question is what are we going to fill the missing data with? We definitely don't want to just put in a number that doesn't actually mean something. And so one of the tricks you can do with this is we can take x of i. And in addition to that, we want to go


04:25:21
ahead and turn this into an integer because a lot of these are integers. So we'll go ahead and keep it integers. And me add the bracket here. And a lot of editors will do this. They'll think that you're closing one bracket. Make sure you get that second bracket in there if it's a double bracket. That's always something that happens regularly. So once we have our integer of x of yi, this is going to fill in any missing data with the average. And I was so busy closing one set of brackets, I forgot


04:25:46
that the mean is also has brackets in there for the pandas. So we can see here we're going to fill in all the data with the average value for that column. So if there's missing data is in the average of the data it does have. Then once we've done that, we'll go ahead and loop through it again and just check and see to make sure everything is filled in correctly. And we'll print and then we take x is null. And this returns a set of the null value or the how many lines are null.


04:26:13
And we'll just sum that up to see what that looks like. And so when I run this and so with the X, what we want to do is we want to remove the last column because that had the models. That's what we're trying to see if we can cluster these things and figure out the models. There is so many different ways to sort the X out. For one, we could take the X and we could go data set, our variable we're using, and use the eyelocation, one of the features that's in pandas, and we could take that and then take all


04:26:43
the rows and all but the last column of the data set. And at this time, we could do values. We just convert it to values. So, that's one way to do this. And if I let me just put this down here and print X, it's a capital X we chose. and I run this, you can see it's just the values. We could also take out the values and it's not going to return anything because there's no values connected to it. What I like to do with this is instead of doing the location which does integers more common is to come in here


04:27:15
and we have our data set and we're going to do data set dot or data set columns. And remember that lists all the columns. So if I come in here, let me just mark that as red and I print data set. columns. You can see that I have my index here. I have my MPG cylinders everything including the brand which we don't want. So the way to get rid of the brand would be to do data columns of everything but the last one minus one. So now if I print this, you'll see the brand disappears. And so I can actually


04:27:54
just take data set columns minus one and I'll put it right in here for the columns we're going to look at. And let's unmark this. And unmark this. And now if I do an X.Ahead, I now have a new data frame. And you can see right here we have all the different columns except for the brand at the end of the year. And it turns out when you start playing with the data set, you're going to get an error later on and it'll say cannot convert string to float value. And that's because for some reason these


04:28:31
things the way they recorded them must have been recorded as strings. So we have a neat feature in here on pandas to convert. And it is simply convert objects. And for this we're going to do convert oops convert underscore numeric numeric equals true. And yes, I did have to go look that up. I don't have it memorized the convert numeric in there. If I'm working with a lot of these things, I remember them. But um depending on where I'm at, what I'm doing, I usually have to look it up.


04:29:04
And we run that. Oops, I must have missed something in here. Let me double check my spelling. And when I double check my spilling, you'll see I missed the first underscore in the convert objects. And when I run this, it now has everything converted into a numeric value because that's what we're going to be working with is numeric values down here. And the next part is that we need to go through the data and eliminate null values. Most people when they're doing small amounts, you working with


04:29:32
small data pools discover afterwards that they have a null value and they have to go back and do this. So, you know, be aware whenever we're formatting this data, things are going to pop up and sometimes you go backwards to fix it. And that's fine. That's just part of exploring the data and understanding what you have. And I should have done this earlier, but let me go ahead and increase the size of my window one notch. There we go. Easier to see. So, we'll do 4 I in working with X


04:30:03
dot columns. will page through all the columns. And we want to take x of i and we're going to change that. We're going to alter it. And so with this, we want to go ahead and fill in x of i. Pandas has the fill in a. And that just fills in any non-existent missing data. And we'll put my brackets up. And there's a lot of different ways to fill this data. If you have a really large data set, some people just void out that data because they and then look at it later in a separate exploration of data. One


04:30:38
of the tricks we can do is we can take our column and we can find the means and the means is in there or quotation marks. So we take the columns, we're going to fill in the non-existing one with the means. The problem is that returns a decimal float. So some of these aren't decimals certainly. Let me be a little careful of doing this, but for this example, we're just going to fill it in with the integer version of this. Keeps it on par with the other data that isn't a decimal point. And then what we also want to do


04:31:13
is we want to double check. A lot of times you do this first part first to double check, then you do the fill, and then you do it again just to make sure you did it right. So, we're going to go through and test for missing data. And one of the re ways you can do that is simply go in here and take our X of I column. So it's going to go through the X of I column. It says is null. So it's going to return any any place there's a null value. It actually goes through all the rows of each column is null. And


04:31:43
then we want to go ahead and sum that. So we take that, we add the sum value. And these are all pandas. So is null is a panda command and so is sum. And if we go through that and we go ahead and run it and we go ahead and take and run that, you'll see that all the columns have zero null values. So we've now tested and double checked and our data is nice and clean. We have no null values. Everything is now a number value. We turned it into numeric and we've removed the last column in our


04:32:13
data. And at this point, we're actually going to start using the elbow method to find the optimal number of clusters. So, we're now actually getting into the sklearn part. Uh, the K means clustering on here. I guess we'll go ahead and zoom it up one more notch so you can see what I'm typing in here. And then from sklearn going to or sklearn cluster, we're going to import K means. I always forget to capitalize the K and the M when I do this. And those capital K capital M K means and we'll go and create a um array


04:32:57
WCSS equals we'll make it an empty array. If you remember from the elbow method from our slide within the sums of squares WSS is defined as the sum of squared distance between each member of the cluster and it centrid. So we're looking at that change in differences as far as a squared distance. And we're going to run this over a number of K mean values. In fact, let's go for I in range. We'll do 11 of them. Rain zero of 11. And the first thing we're going to do is we're going to create the actual


04:33:33
we'll do it all lowercase. And so we're going to create this object from the K means that we just imported. And the variable that we want to put into this is in clusters. And we're going to set that equals to I. That's the most important one because we're looking at how increasing the number of clusters changes our answer. There are a lot of settings to the K means. Our guys in the back did a great job just kind of playing with some of them. The most common ones that you see in a lot of


04:34:12
stuff is how you admit your K means. So we have K means plus plus. This is just a tool to let the model itself be smart how it picks it centrids to start with its initial centroidids. We only want to iterate no more than 300 times. We have a max iteration we put in there. We have the infinite the random state equals zero. You really don't need to worry too much about these when you're first learning this. As you start digging in deeper, you start finding that these are shortcuts that will speed up the process


04:34:43
as far as a setup. But the big one that we're working with is the inclusters equals I. So, we're going to literally train our K means 11 times. We're going to do this process 11 times. And if you're working with big data, you know, the first thing you do is you run a small sample of the data so you can test all your stuff on it. And you can already see the problem that if I'm going to iterate through a terabyte of data 11 times and then the k means itself is iterating through the data


04:35:12
multiple times. That's a heck of a process. So you got to be a little careful with this. A lot of times though you can find your elbow using the elbow method. Find your optimal number on a sample of data especially if you're working with larger data sources. So we want to go ahead and take our k means and we're just going to fit it. If you're looking at any of the sklearn, very common you fit your model. And if you remember correctly, our variable we're using is the capital X. And once


04:35:38
we fit this value, we go back to the array we made. And we want to go and just append that value on the end. And it's not the actual fit we're pinning in there. It's when it generates it, it generates the value you're looking for is inertia. So K means.inertia will pull that specific value out that we need. And let's get a visual on this. We'll do our PLT plot. And what we're plotting here is first the x axis, which is range 0 11. So that will generate a nice little plot there. And the wcss for our


04:36:15
y axis. It's always nice to give our uh plot a title. And let's see, we'll just give it the elbow method for the title. And let's get some labels. So let's go ahead and do PLT X label. And what we'll do, we'll do number of clusters for that. And PLT Y label. And for that, we can do oops, there we go. WCSS since that's what we're doing on the plot on there. And finally, we want to go ahead and display our graph, which is simply plt. Oops. Show. There we go. And because we


04:36:52
have it set to inline, it'll appear in line. Hopefully I didn't make a type error on there. And you can see we get a very nice graph. You can see a very nice elbow joint there at uh two and again right around three and four. And then after that, there's not very much. Now, as a data scientist, if I was looking at this, I would do either three or four. And I'd actually try both of them to see what the um output look like. And they've already tried this in the back. So, we're just going to use three as a


04:37:25
setup on here. And let's go ahead and see what that looks like when we actually use this to show the different kinds of cars. And so, let's go ahead and apply the K means to the cars data set. And basically, we're going to copy the code that we loop through up above where K means equals K means number of clusters. And we're just going to set the number of clusters to three since that's what we're going to look for. And you can do three and four on this and graph them just to see how they come up


04:37:53
differently. It'd be kind of curious to look at that. But for this, we're just going to set it to three. Go ahead and create our own variable Y k means for our answers. And we're going to set that equal to Whoops, that double equal there to K means. But we're not going to do a fit. We're going to do a fit predict is the setup you want to use. And when you're using untrained models, you'll see um a slightly different because usually you see fit and then you see just the predict. But we want to both


04:38:23
fit and predict the k means on this. And that's fit underscore predict. And then our capital x is the data we're working with. And before we plot this data, we're going to do a little pandas trick. We're going to take our x value and we're going to set x as matrix. So we're converting this into a nice rows and columns kind of setup. But we want the we're going to have columns equals none. So it's just going to be a matrix of data in here. And let's go ahead and run


04:38:51
that. A little warning. You'll see this warnings pop up because things are always being updated. So there's like minor changes in the versions and future versions. Instead of matrix now that it's more common to set it values instead of doing as matrix, but mass matrix works just fine for right now and you'll want to update that later on. But let's go ahead and dive in and plot this and see what that looks like. And before we dive into plotting this data, I always like to take a look and see what


04:39:19
I am plotting. So let's take a look at why K means. I'm just going to print that out down here. And we see we have an array of answers. We have 2 1 0 2 1 2. So it's clustering these different rows of data based on the three different spaces it thinks it's going to be. And then let's go ahead and print X and see what we have for X. And we'll see that X is an array. It's a matrix. So we have our different values in the array. And what we're going to do, it's very


04:39:51
hard to plot all the different values in the array. So we're only going to be looking at the first two or positions zero and one. And if you were doing a full presentation in front of the board meeting, you might actually do a little different than and dig a little deeper into the different aspects because this is all the different columns we looked at. But we'll only look at columns one and two for this to make it easy. So let's go ahead and clear this data out of here and let's bring up our plot. And


04:40:21
we're going to do a scatter plot here. So plt scatter. And this looks a little complicated. So let's explain what's going on with this. We're going to take the x values and we're only interested in y of k means equals 0, the first cluster. Okay? And then we're going to take value zero for the x-axis. And then we're going to do the same thing here. We're only interested in k means equals 0, but we're going to take the second column. So we're only looking at the first two


04:40:52
columns in our answer or in the data. And then the guys in the back played with this a little bit to make it pretty. And they discovered that it looks good with a size equals 100. That's the size of the dots. We're going to use red for this one. And when they were looking at the data and what came out, it was definitely the Toyota on this. We're just going to go ahead and label it Toyota. Again, that's something you really have to explore in here as far as playing with those numbers and


04:41:22
see what looks good. We'll go ahead and hit enter in there. And I'm just going to paste in the next two lines, which is the next two cars. And this is our Nissa and Honda. And you'll see with our scatter plot, we're now looking at where Y_K means equals 1. And we want the zero column and YK means equals 2. Again, we're looking at just the first two columns, 0 and one. And each of these rows then corresponds to Nissan and Honda. And I'll go ahead and hit enter on there. And uh finally, let's take a


04:41:54
look and put the centrids on there. Again, we're going to do a scatter plot. And on the centrids, you can just pull that from our K means, the uh model we created, dotcluster centers. And we're going to just do um all of them in the first number and all of them in the second number, which is 01 because you always start with zero and one. And then they were playing with the size and everything to make it look good. We'll do a size of 300. We're going to make the color yellow. And


04:42:26
we'll label them. It's always good to have some good labels. Centroidids. And then we do want to do a title. PLT title. And pop up there. PLT title. So you always make want to make your graphs look pretty. We'll call it clusters of carake. And one of the features of the plot library is you can add a legend. It'll automatically bring in it since we've already labeled the different aspects of the legend with Toyota, Nissan, and Honda. And finally, we want to go ahead and show so we can actually see it. And


04:43:02
remember, it's in line. Uh so if you're using a different editor that's not the Jupyter notebook, you'll get a popup of this. And you should have a nice set of clusters here. So we can look at this and we have a clusters of Honda in green, Toyota in red, Nissan in purple. And you can see where they put the centroidids to separate them. Now when we're looking at this, we can also plot a lot of other different data on here as far because we only looked at the first two columns. This is


04:43:29
just column one and two or 01 as as you label them in computer scripting. But you can see here we have a nice clusters of car making. you'll be able to pull out the data and you can see how just these two columns form very distinct clusters of data. So if you were exploring new data, you might take a look and say, well, what makes these different? Almost going in reverse, you start looking at the data and pulling apart the columns to find out why is the first group set up the way it is. Maybe


04:43:57
you're doing loans and you want to go, well, why is this group not defaulting on their loans and why is the last group defaulting on their loans? and why is the middle group 50% defaulting on their bank loans and you start finding ways to manipulate the data and pull out the answers you want. So now that you've seen how to use K mean for clustering, let's move on to the next topic. Now let's look into logistic regression. The logistic regression algorithm is the simplest classification algorithm used for binary


04:44:29
or multiclassification problems. And we can see we have our little girl from Canada who's into horror books is back. That's actually really scary when you think about that with those big eyes. In the previous tutorial, we learned about linear regression, dependent and independent variables. So to brush up, y = mx + c. Very basic algebraic function of uh y and x. The dependent variable is the target class variable. We are going to predict the independent variables. X1 all the way up to XN are the features or


04:45:04
attributes we're going to use to predict the target class. We know what a linear regression looks like. But using the graph, we cannot divide the outcome into categories. It's really hard to categorize 1.5, 3.6, 9.8. Uh for example, a linear regression graph can tell us that with increase in number of hours studied the marks of a student will increase but it will not tell us whether the student will pass or not. In such cases where we need the output as categorical value, we will use logistic


04:45:34
regression. And for that we're going to use the sigmoid function. So you can see here we have our marks 0 to 100 number of hours studied. That's going to be what they're comparing it to in this example. And we usually form a line that says y = mx + c. And when we use the sigmoid function, we have p = 1 / 1 + e the minus y, it generates a sigmoid curve. And so you can see right here when you take the ln, which is the natural logarithm. I always thought it should be nl, not ln. That's just the


04:46:07
inverse of uh e to the minus y. And so we do this, we get ln of p over 1 - p = m * x + c. That's the sigmoid curve function we're looking for. Now we can zoom in on the function and you'll see that the function as it deres goes to one or to zero depending on what your x value is. And the probability if it's greater than 0.5 the value is automatically rounded off to one indicating that the student will pass. So if they're doing a certain amount of studying, they will probably pass. Then


04:46:40
you have a threshold value at the 0.5. It automatically puts that right in the middle usually. And your probability if it's less than 0.5, the value round it off to zero indicating the student will fail. So if they're not studying very hard, they're probably going to fail. This, of course, is ignoring the outliers of that one student who's just a natural genius and doesn't need any studying to memorize everything. That's not me, unfortunately. Have to study hard to learn new stuff. problem


04:47:05
statement to classify whether a tumor is malignant or B9. And this is actually one of my favorite data sets to play with because it has so many features and when you look at them, you really are hard to understand. You can't just look at them and know the answer. So it gives you a chance to kind of dive into what data looks like when you aren't able to understand the specific domain of the data. But I also want you to remind you that in the domain of medicine, if I told you that my probability was really


04:47:34
good at classified things that say 90% or 95% and I'm classifying whether you're going to have a malignant or a B9 tumor, I'm guessing that you're going to go get it tested anyways. So you got to remember the domain we're working with. So why would you want to do that if you know you're just going to go get a biopsy? Because you know it's that serious. This is like an all or nothing. just referencing the domain. It's important. It might help the doctor know where to look just by understanding what


04:48:03
kind of tumor it is. So it might help them or aid them on something they missed from before. So let's go ahead and dive into the code and I'll come back to the domain part of it in just a minute. So use case and we're going to do our normal imports here where we're importing numpy, pandas, seabour, the mattplot library and we're going to do mattplot library in line since I'm going to switch over to Anaconda. So, let's go ahead and flip over there and get this started. So, I've opened up a new window


04:48:29
in my Anaconda Jupyter Notebook. And by the way, Jupyter Notebook, uh, you don't have to use Anaconda for the Jupyter Notebook. I just love the interface and all the tools that Anaconda brings. So, we got our import numpy aspy number array. We have our pandas pd. We're going to bring in Seabor to help us with our graphs as SNS. So many really nice tools in both Seabour and Mattplot library. And we'll do our mapplot library.pipplot as plt. And then of course we want to let it know to do


04:49:00
it in line. And let's go and just run that. So it's all set up. And we're just going to call our data data. Not creative today. Uh equals pd. And this happens to be in a CSV file. So we'll use a pdread_csv. And I happen to name the file. I renamed it data forp2.csv. You can of course um write in the comments below the YouTube and request for the data set itself or go to the simply learn website and we'll be happy to supply that for you. And let's just um open up the data before we go


04:49:33
any further and let's just see what it looks like in a spreadsheet. So when I pop it open in a local spreadsheet and this is just a CSV file, comma separated variables. We have an ID. So I guess the U categorizes for reference or what ID which test was done. The diagnosis M for malignant, B for B9. So there's two different options on there. And that's what we're going to try to predict is the M and B and test it. And then we have like the radius mean or average the texture average,


04:50:03
perimeter mean, area mean, smoothness. I don't know about you, but unless you're a doctor in the field, most of the stuff, I mean, you can guess what concave means just by the term concave, but I really wouldn't know what that means in the measurements they're taking. So, they have all kinds of stuff like how smooth it is, uh, the symmetry, and these are all float values. You just page through them real quick, and you'll see there's, I believe, 36, if I remember correctly in this one.


04:50:31
So there's a lot of different values they take and all these measurements they take when they go in there and they take a look at the different growth, the tumorous growth. So back in our data and I put this in the same folder as a code. So I saved this code in that folder. Obviously if you have it in a different location, you want to put the full path in there and we'll just do uh pandas first five lines of data with the data head. and we run that, we can see that we have pretty much what we just


04:51:00
looked at. We have an ID, we have a diagnosis. If we go all the way across, you'll see all the different columns coming across displayed nicely for our data. And while we're exploring the data, our uh Seabor, which we referenced as SNS, makes it very easy to go in here and do a joint plot. You'll notice the very similar to because it is sitting on top of the um plot library. So the joint plot does a lot of work for us. And we're just going to look at the first two columns that we're interested in,


04:51:33
the radius mean and the texture mean. We'll just look at those two columns and data equals data. So that tells it which two columns we're plotting and that we're going to use the data that we pulled in. Let's just run that. And it generates a really nice graph on here. And there's all kinds of cool things on this graph to look at. I mean, we have the texture mean and the radius mean obviously the axes. You can also see and uh one of the cool things on here is you can also see the histogram.


04:52:02
They show that for the radius mean where is the most common radius mean come up and where the most common texture is. So, we're looking at the tech the on each growth it's average texture and on each radius its average uh radius on there gets a little confusing because we're talking about the individual objects average and then we can also look over here and see the the histogram showing us the median or how common each measurement is. And that's only two columns. So, let's dig a little deeper


04:52:32
into Seabor. They also have a heat map. And if you're not familiar with heat maps, a heat map just means it's in color. That's all that means. Heat map, I guess the original ones were plotting heat density on something. And so ever since then, it's just called a heat map. And we're going to take our data and get our corresponding numbers to put that into the heat map. And that's simply data.coR for that. That's a pandas expression. Remember, we're working in a pandas data frame. So that's one of the


04:53:00
cool tools in pandas for our data. And let's just pull that information into a heat map and see what that looks like. And you'll see that we're now looking at all the different features. We have our ID, we have our texture, we have our area, our compactness, concave points. And if you look down the middle of this chart diagonal going from the upper left to bottom right, it's all white. That's because when you compare texture to texture, they're identical. So they're 100% or in this case perfect one in


04:53:31
their correspondence and you'll see that when you look at say area or right below it it has almost a black on there when you compare it to texture. So these have almost no corresponding data. They don't really form a linear graph or something that you can look at and say how connected they are. They're very scattered data. This is really just a really nice graph to get a quick look at your data. doesn't so much change what you do, but it changes verifying. So, when you get an answer or something like


04:54:00
that or you start looking at some of these individual pieces, you might go, "Hey, that doesn't match according to showing our heat map." This should not correlate with each other. And if it is, you're going to have to start asking, well, why? What's going on? What else is coming in there? But it does show some really cool information on here. I mean, we can see from the ID, there's no real one feature that just says if you go across the top line that lights up. There's no one feature that says, hey,


04:54:29
if the area is a certain size, then it's going to be B9 or malignant. It says there's some that sort of add up. And that's a big hint in the data that we're trying to ID this whether it's malignant or B9. That's a big hint to us as data scientists to go, okay, we can't solve this with any one feature. It's going to be something that includes all the features or many of the different features to come up with a solution for it. And while we're exploring the data,


04:54:55
let's explore one more area and let's look at data.isnull. We want to check for null values in our data. If you remember from earlier in this tutorial, we did it a little differently where we added stuff up and sum them up. You can actually with pandas do it really quickly. Data.isnull and summit. And it's going to go across all the columns. So when I run this, you're going to see all the columns come up with no null data. So we've just just to rehash these last few steps. We've done a lot of


04:55:30
exploration. We have looked at the first two columns and seen how they plot with the seabour with a joint plot which shows both the histogram and the data plotted on the XY coordinates. And obviously you can do that more in detail with different columns and see how they plot together. And then we took and did the seabor heat map the SNS heat mapap of the data. And you can see right here where it did a nice job showing us some bright spots where stuff correlates with each other and forms a very nice combination or points of


04:56:04
scattering points. And you can also see areas that don't. And then finally we went ahead and checked the data. Is the data null value? Do we have any missing data in there? Very important step because it'll crash later on. If you forget to do this step, it will remind you when you get that nice error code that says null values. Okay, so not a big deal if you miss it, but it it's no fun having to go back when you're when you're in a huge process and you've missed this step and


04:56:32
now you're 10 steps later and you got to go remember where you were pulling the data in. So, we need to go ahead and pull out our X and our Y. So, we just put that down here. And we'll set the X equal to. And there's a lot of different options here. Certainly, we could do X equals all the columns except for the first two because if you remember, the first two is the ID and the diagnosis. So, that certainly would be an option. But what we're going to do is we're actually going to focus


04:57:00
on the worst. the worst radius, the worst texture, parameter, area, smoothness, compactness, and so on. One of the reasons to start dividing your data up when you're looking at this information is sometimes the data will be the same data coming in. So, if I have two measurements coming into my model, it might overweigh them. It might overpower the other measurements because it's measuring it's basically taking that information in twice. That's a little bit past the scope of this


04:57:30
tutorial. I want you to take away from this though is that we are dividing the data up into pieces and our team in the back went ahead and said hey let's just look at the worst. So I'm going to create a an array and you'll see this array radius worst texture worst perimeter worst. We've just taken the worst of the worst and I'm just going to put that in my X. So this X is still a pandas data frame but it's just those columns. And our Y, if you remember correctly, is going to be Oops. Hold on


04:57:59
one second. It's not X, it's data. There we go. So X equals data. And then it's a list of the different columns, the worst of the worst. And if we're going to take that, then we have to have our answer for our Y for the stuff we know. And if you remember correctly, we're just going to be looking at the diagnosis. That's all we care about is what is it diagnosed? Is it B9 or malignant? And since it's a single column, we can just do diagnosis. Oh, I forgot to put the brackets. There we go.


04:58:28
Okay. So, it's just diagnosis on there. And we can also real quickly do like an X do. If you want to see what that looks like and Y head and run this and you'll see um it only does the last one. I forgot about that. If you don't do print, you can see that the the Y.D is just mm because the first ones are all malignant. And if I run this the X do head is this just the first five values of radius worst texture worst parameter worst area worst and so on. I'll go ahead and take that


04:59:00
out. So moving down to the next step we've built our two data sets our answer and then the features we want to look at. In data science it's very important to test your model. So we do that by splitting the data and from sklearn model selection we're going to import train test split. So we're going to split it into two groups. There are so many ways to do this. I noticed in one of the more modern ways they actually split it into three groups and then you model each group and test it against the other


04:59:37
groups. So you have all kinds and there's reasons for that which is past the scope of this and for this particular example isn't necessary. For this we're just going to split it into two groups. one to train our data and one to test our data. And the sklearn uh selection we have train tests split. You could write your own quick code to do this where you just randomly divide the data up into two groups. But they do it for us nicely and we actually can almost we can actually do it in one statement with


05:00:06
this where we're going to generate four variables. Capital X train capital X test. So we have our training data we're going to use to fit the model and then we need something to test it. And then we have our Y train. So we're going to train the answer and then we have our test. So this is the stuff we want to see how good it did on our model. And we'll go ahead and take our train test split that we just imported. And we're going to do X and our Y, our two different data that's


05:00:33
going in for our split. And then the guys in the back came up and wanted us to go ahead and use a test size equals.3. That's test size. Random state, it's always nice to kind of switch a random state around, but not that important. What this means is that the test size is we're going to take 30% of the data and we're going to put that into our test variables, our Y test and our X test. And we're going to do 70% into the X train and the Y train. So, we're going to use 70% of the data to


05:01:03
train our model and 30% to test it. Let's go ahead and run that and load those up. So now we have all our stuff split up and all our data ready to go. And now we get to the actual logistics part. We're actually going to do our create our model. So let's go ahead and bring that in from sklearn. We're going to bring in our linear model and we're going to import logistic regression. That's the actual model we're using. And let's we'll call it log model. Oops, there we go. Model. And


05:01:32
let's just set this equal to our logistic regression that we just imported. So now we have a variable log model set to that class for us to use. And with most the uh models in the sklearn, we just need to go ahead and fix it. Fit do a fit on there. And we use our X train that we separated out with our Y train. And let's go ahead and run this. So once we've run this, we'll have a model that fits this data. That's 70% of our training data. Uh, and of course it prints this out that tells us all the different


05:02:06
variables that you can set on there. There's a lot of different choices you can make, but for Word do, we're just going to let all the default set. We don't really need to mess with those on this particular example. And there's nothing in here that really stands out as super important until you start fine-tuning it. But for what we're doing, the basics will work just fine. And then let's we need to go ahead and test out our model. Is it working? So let's create a variable Y predict. And


05:02:32
this is going to be equal to our log model. And we want to do a predict. Again, very standard format for the sklearn library is taking your model and doing a predict on it. And we're going to test y predict against the y test. So we want to know what the model thinks it's going to be. That's what our y predict is. And with that, we want the capital xx test. So we have our train set and our test set. And now we're going to do our y predict. And let's go ahead and run that. And if we uh


05:03:05
print y predict, let me go ahead and run that. You'll see it comes up and it predents a prints a nice array of uh B and M for B9 and malignant for all the different test data we put in there. So, it does pretty good. We're not sure exactly how good it does, but we can see that it actually works and is functional. Was very easy to create. You'll always discover with our data science that as you explore this, you spend a significant amount of time prepping your data and making sure your data coming in is good. Uh there's


05:03:41
a saying, good data in, good answers out. Bad data in, bad answers out. That's only half the thing. That's only half of it. Selecting your models becomes the next part as far as how good your models are. and then of course fine-tuning it depending on what model you're using. So we come in here, we want to know how good this came out. So we have our Y predict here, log model.predict X test. So for deciding how good our model is, we're going to go from the sklearn.metrics, we're going to import


05:04:15
classification report. And that just reports how good our model is doing. And then we're going to feed it the model data. And let's just print this out. and we'll take our uh classification report and we're going to put into there our test our actual data. So this is what we actually know is true and our prediction what our model predicted for that data on the test side. And let's run that and see what that does. So we pull that up. You'll see that we have um a precision for B9 and


05:04:51
malignant B and M. And we have a precision of 93 at 91, a total of 92. So it's kind of the average between these two of 92. There's all kinds of different information on here. Your F1 score, your recall, your support coming through on this. And for this, I'll go ahead and just flip back to our slides that they put together for describing it. And so here we're going to look at the precision using the classification report. And you see this is the same print out I had up above. Some of the


05:05:21
numbers might be different because it does randomly pick out which data we're using. So this model is able to predict the type of tumor with 91% accuracy. So we look back here, it's you will see where we have B9 and malignant. It actually has 92 coming up here. We're looking about a 92 91% precision. And remember I reminded you about domain. So, when we're talking about the domain of a medical domain with a very catastrophic outcome, you know, at 91 or 92% precision, you're still going to go


05:05:52
in there and have somebody do a biopsy on it. Very different than if you're investing money and there's a 92% chance you're going to earn 10% and 8% chance you're going to lose 8%, you're probably going to bet the money because at that odds, it's pretty good that you'll make some money. And in the long run, you do that enough, you definitely will make money. And also with this domain, I've actually seen them use this to identify different forms of cancer. That's one of


05:06:17
the things that they're starting to use these models for because then it helps a doctor know what to investigate. So that wraps up this section. We're finally we're going to go in there and let's discuss the answers to the quiz asked in machine learning tutorial part one. Can you tell what's happening in the following cases? Grouping documents into different categories based on the topic and content of each document. This is an example of clustering where K means clustering can be used to group the


05:06:44
documents by topics using bag of words approach. So if you gotten in there that you're looking for clustering and hopefully you had at least one or two examples like K means that are used for clustering different things then give yourself a two thumbs up. B identifying handwritten digits in images correctly. This is an example of classification. The traditional approach to solving this would be to extract digit dependent features like curvature of different digits etc. and then use a classifier


05:07:13
like SVM to distinguish between images. Again, if you got the fact that it's a classification example, give yourself a thumb up. And if you're able to go, hey, let's use SVM or another model for this, give yourself those two thumbs up on it. C. Behavior of a website indicating that the site is not working as designed. This is an example of anomaly detection. In this case, the algorithm learns what is normal and what is not normal, usually by observing the logs of the website. Give yourself a thumbs up if


05:07:44
you got that one. And just for a bonus, can you think of another example of anomaly detection? One of the ones I use it for in my own business is detecting anomalies in stock markets. Stock markets are very fickled and they behave very erratic. So finding those erratic areas and then finding ways to track down why they're erratic. Was something released in social media? Was something released you can see where knowing where that anomaly is can help you to figure out what the answer is to it in another


05:08:11
area. D predicting salary of an individual based on his or her years of experience. This is an example of regression. This problem can be mathematically defined as a function between independent years of experience and dependent variables salary of an individual. And if you guess that this was a regression model, give yourself a thumbs up. And if you were able to remember that it was between independent and dependent variables and that terms, give yourself two thumbs up. Summary. So to wrap it up, we went over what is K


05:08:43
means and we went through also the chart of choosing your elbow method and assigning a random centrid to the clusters, computing the distance and then going in there and figuring out what the minimum centroidids is and computing the distance and going through that loop until it gets the perfect centrid. And we looked into the elbow method to choose K based on running our clusters across a number of variables and finding the best location for that. We did a nice example of clustering cars with K means even though we only looked


05:09:12
at the first two columns to make it simple and easy to graph. You can easily extrapolate that and look at all the different columns and see how they all fit together. And we looked at what is logistic regression. We discussed the sigmoid function. What is logistic regression? And then we went into an example of classifying tumors with logistics. I hope you enjoyed part two of machine learning. Why reinforcement learning? Training a machine learning model requires a lot of data which might not always be available to us. Further,


05:09:43
the data provided might not be reliable. Learning from a small subset of actions will not help expand the vast realm of solutions that may work for a particular problem. And you can see here we have the robot learning to walk. Um, very complicated setup when you're learning how to walk. And you'll start asking questions like if I'm taking one step forward and left, what happens if I pick up a 50 lb object? How does that change how a robot would walk? These things are very difficult to program because


05:10:13
there's no actual information on it until it's actually tried out. Learning from a small subset of actions will not help expand the vast realm of solutions that may work for a particular problem. And we'll see here it learned how to walk. This is going to slow the growth that technology is capable of. Machines need to learn to perform actions by themselves and not just learn off humans. And you see the objective climb a mountain. Real interesting point here is that as human beings, we can go into


05:10:45
a very unknown environment and we can adjust for it and kind of explore and play with it. Most of the models, the non-reinforcement models in computer uh machine learning aren't able to do that very well. Uh there's a couple of them that can be used or integrated. See how it goes is what we're talking about with reinforcement learning. So what is reinforcement learning? Reinforcement learning is a subbranch of machine learning that trains a model to return an optimum solution for a problem by


05:11:18
taking a sequence of decisions by itself. Consider a robot learning to go from one place to another. The robot is given a scenario must arrive at a solution by itself. The robot can take different paths to reach the destination. It will know the best path by the time taken on each path. It might even come up with a unique solution all by itself. And that's really important is we're looking for unique solutions. Uh we want the best solution, but you can't find it unless you try it. So


05:11:47
we're looking at uh our different systems, our different model, we have supervised versus unsupervised versus reinforcement learning. And with the supervised learning, that is probably the most controlled environment. Uh we have a lot of different supervised learning models where there's linear regression, neural networks, um there's all kinds of things in between, decision trees. The data provided is labeled data with output values specified. And this is important because we talk about


05:12:16
supervised learning. You already know the answer for all this information. You already know the picture has a motorcycle in it. So you're supervised learning. You already know that um the outcome for tomorrow for you know going back a week. You're looking at stock. You can already have like the graph of what the next day looks like. So you have an answer for it. And you have labeled data which is used. You have an external supervision and solves problems by mapping labeled input to known output.


05:12:44
So very controlled unsupervised learning and unsupervised learning is really interesting because it's now taking part in many other models. They start with an you can actually insert an unsupervised learning model um in almost either supervised or reinforcement learning as part of the system which is really cool. Uh data provided is unlabeled data. The outputs are not specified. machine makes its own predictions used to solve association with clustering problems. Unlabeled data is used. No supervision.


05:13:17
Solves problems by understanding patterns and discovering output. Uh so you can look at this and you can think um some of these things go with each other. They belong together. So it's looking for what connects in different ways. And there's a lot of different algorithms that look at this. Um, when you start getting into those, there's some really cool images that come up of what unsupervised learning is, how it can pick out, say, uh, the area of a donut. One model will see the area of the donut and the other one will


05:13:46
divide it into three sections based on its location versus what's next to it. So, there's a lot of stuff that goes in with unsupervised learning. And then we're looking at reinforcement learning. probably the biggest industry in today's market uh in machine learning or growing market. It's very it's very infant stage uh as far as how it works and what it's going to be capable of. The machine learns from its environment using rewards and errors used to solve rewardbased problems. No predefined data


05:14:15
is used. No supervision. Follows trail and error problem solving approach. Uh so again we have a random at first you start with a random I try this it works and this is my reward doesn't work very well maybe or maybe doesn't even get you where you're trying to get it to do and you get your reward back and then it looks at that and says well let's try something else and it starts to play with these different things finding the best route. So let's take a look at important terms in today's reinforcement


05:14:44
model and this has become pretty standardized over the last uh few years. So these are really good to know. We have the agent. Uh agent is the model that is being trained via reinforcement learning. So this is your actual u entity that has however you're doing it whether you're using a neural network or a Q table or whatever combination thereof. This is the actual agent that you're using. This is the model and you have your environment. Uh the training situation that the model must optimize to is called its


05:15:16
environment. Uh, and you can see here, I guess we have a robot who's trying to get a chest full of gems or whatever. And that's the output. And then you have your action. This is all possible steps that can be taken by the model. And it picks one action. And you can see here it's picked three different uh routes to get to the chest of diamonds and gems. We have a state, the current position condition returned by the model. And you could look at this uh if you're playing like a video game. And


05:15:45
this is the screen you're looking at. Uh so when you go back here, uh the environment is the whole game board. So if you're playing one of those Mobius games, you might have the whole game board going on. Uh but then you have your current position. Where are you on that game board? What's around that? What's around you? Um if you were talking about a robot, the environment might be moving around the yard, where it is in the yard, and what it can see, what input it has in that location. That


05:16:13
would be the current position condition returned by the model. And then the reward uh to help the model move in the right direction. It is rewarded. Points are given to it to appraise some kind of action. So yeah, you did good or if uh didn't do as good trying to maximize the reward and have the best reward possible. And then policy. Policy determines how an agent will behave at any time. It acts as a mapping between action and present state. This is part of the model. what what what is your


05:16:43
action that you're you're going to take? What's the policy you're using to have an output from your agent? One of the reasons they separate uh policy as its own entity is that you usually have a prediction um of a different options and then the policy well how am I going to pick the best based on those predictions I'm going to guess at different options and we'll actually weigh those options in and find the best option we think will work. Uh so it's a little tricky but the policy thing is actually pretty


05:17:15
cool how it works. Let's go ahead and take a look at a reinforcement learning example. And just in looking at this, we're going to take a look uh consider what a dog um that we want to train. Uh so the dog would be like the agent. So you have your your puppy or whatever. Uh and then your environment is going to be the whole house or whatever it is where you're training them. And then you have an action. We want to teach the dog to fetch. So action equals fetching. Uh and then we have a little biscuit. So we can


05:17:46
get the dog to perform various actions by offering incentives such as a dog biscuit as a reward. The dog will follow a policy to maximize this reward and hence will follow every command and might even learn new actions like begging by itself. Uh so you have B you know so we start off with fetching it and goes oh I get a biscuit for that it tries something else and you get a handshake or begging or something like that and it goes oh this is also reward based and so it kind of explores things to find out


05:18:13
what will bring it is biscuit and that's very much like how a reinforced model goes is it uh looks for different rewards how do I find can I try different things and find a reward that works the dog also will want to run around and play and explore it environment uh this quality audio model is called exploration. So there's a little randomness going on in exploration. It explores new parts of the house. Climbing on the sofa doesn't get a reward. In fact, it usually gets kicked off the


05:18:45
sofa. So let's talk a little bit about marov's decision process. Uh, Marov's decision process is a reinforcement learning policy used to map a current state to an action where the agent continuously interacts with the environment to produce new solutions and receive rewards. And you'll see here's all of our different uh uh vocabulary we just went over. We have a reward, our state, our agent, our environment interaction. And so even though the environment kind of contains everything


05:19:16
um that you you really when you're actually writing the program your environment is going to put out a reward and state that goes into the agent. Uh the agent then looks at this uh state or it looks at the reward usually um first and it says okay I got rewarded for whatever I just did or I didn't get rewarded and then it looks at the state and then it comes back and if you remember from policy the policy comes in um and then we have a reward. The policy is that part that's connected at the


05:19:47
bottom. And so it looks at that policy and it says, "Hey, what's a good action that will probably be similar to what I did?" Or um uh sometimes they're completely random, but what's a good action that's going to bring me a different reward? So, taking the time to just understand these different pieces as they go is pretty important in most of the models today. Um, and so a lot of them actually have templates based on this that you can pull in and start using. Um, pretty straightforward as far


05:20:20
as once you start seeing how it works. Uh, you can see your environment sends it says, "Hey, this is the agent did this. If you're a character in a game, this happened and it shoots out a reward in a state." The agent looks at the reward, looks at the new state, and then takes a little guess and says, "I'm going to try this action." And then that action goes back into the environment. it affects the environment. The environment then changes depending on what the action was and then it has a


05:20:47
new state and a new reward that goes back to the agent. So in the diagram shown, we need to find the shortest path between node A and D. Each path has a reward associated with it and the path with a maximum reward is what we want to choose. The nodes A, B, C, Denote the nodes to travel from node uh A to B is an action. Reward is the cost of each path and policy is each path taken. And you can see here A can go uh to B or A can go to C right off the bat or it can go right to D. And if you explored all


05:21:21
three of these uh you would find that A going to D was a zero reward. Um A going to C and D would generate a different reward. Or you could go AC B D. There's a lot of options here. Um and so when we start looking at this diagram, you start to realize that even though uh today's reinforced learning models do really good at u finding an answer, they end up trying almost all the different directions you see. And so they take up a lot of work uh or a lot of processing time for reinforcement learning. They're


05:21:55
right now in their infant stage and they're really good at solving simple problems and we'll take a look at one of those in just a minute in a tic-tac-toe game. Uh but you can see here uh once it's gone through these and it's explored, it's going to find the AC D is the best reward. It gets a full 30 points for it. So let's go ahead and take a look at a reinforcement learning demo. Uh and in this demo, we're going to use reinforcement learning to make a tic-tac-toe game. You'll be playing this


05:22:24
game against the machine learning model, and we'll go ahead and we're doing it in Python. So, let's go ahead and go through um I always uh not always actually have a lot of Python tools. Let's go through um Anaconda, which will open up a Jupyter notebook. Seems like a lot of steps, but it's worth it to keep all my stuff separate. And it's also has a nice display when you're in the Jupyter notebook for doing Python. So, here's our Anaconda Navigator. I open up the notebook, which


05:22:52
is going to take me to a web page. And I've gone in here and created a new uh Python folder. In this case, I've already done it and enabled it. Change the name to tic-tac-toe. Uh, and then for this example, uh, we're going to go ahead and import a couple things. We're going to, um, import numpy as np. We'll go ahead and import pickle. Numpy, of course, is our number array. And, uh, pickle is just a nice way sometimes for storing, uh, different information, uh, different states that we're going to go


05:23:20
through on here. Uh, and so we're going to create a class called state. I'm going to start with that. And there's a lot of lines of code to this uh class that we're going to put in here. Don't let that scare you too much. There's not as much here. Um it looks like there's going to be a lot here, but there really is just a lot of setup going on in the in our class state. And so we have up here, we're going to initialize it. Um we have our board. Um, it's a tic-tac-toe board, so


05:23:53
we're only dealing with nine spots on the board. Uh, we have player one, player two, uh, is end. We're going to create a board hash. Um, we'll look at that in just a minute. We're just going to store some information in there. Symbol of player equals 1. Um, so there's a few things going on as far as the initialization. Uh, then something simple. We're just going to get the hash um of the board. We get the information from the board on there, which is uh columns and rows. We want to know when a


05:24:22
winner occurs. Uh so if you get three in a row, that's what this whole section here is for. Uh let me go ahead and scroll up a little bit. And you can get a copy of this code if you send a note over to SimplyLearn. We'll send you over um this particular file and you can play with it yourself and see how it's put together. I don't want to spend a huge amount of time on this uh because this is just some real general Python coding. Uh but you can see here we're just going through um all the rows and


05:24:53
you add them together and if it equals three, three in a row. Same thing with columns. U diagonal. So you got to check the diagonal. That's what all this stuff does here is it just goes through the different areas. Actually, let me go ahead and put There we go. Um, and then it comes down here and we do our sum and it says true uh minus three. It just says did somebody win or is it a tie? So, you got to add up all the numbers on there anyway just in case they're all filled up. And next, we also


05:25:24
need to know available positions. Um, these are ones that don't no one's ever used before. This way, when you try something or the computer tries something, uh, it's not going to give it an illegal move. That's what the available positions is doing. Uh then we want to update our state. And so you have your position going in. We're just sending in the position that you just chose. And you'll see there's a little user interface we put in there. We p pick the row and column in


05:25:52
there. And again, I mean, this is a lot of code. Uh so really, it's kind of a thing you'd want to go through and play with a little bit and just read through it, get a copy of it. Uh great way to understand how this works. And here is a given reward. Um, so we're going to give a reward. Result equals self-winner. This is one of the hearts of what's going on here. Uh, is we have a result self.winner. So if there's a winner, then we have a result. If the result equals one, here's our feedback. Uh, if


05:26:26
it doesn't equal one, then it gets a zero. So it only gets a reward in this particular case if it wins. And that's important to know because different uh systems of reinforced learning do rewarding a lot differently depending on what you're trying to do. This is a very simple example with a 3x3 board. Imagine if you're playing a video game. Uh certainly you only have so many actions, but your environment is huge. You have a lot going on in the environment. And suddenly a reward system like this is going to be just um


05:27:02
is going to have to change a little bit. is going to have to have different rewards and different setup. And there's all kinds of advanced ways to do that as far as weighing you add weights to it. And so they can add the weights up depending on where the reward comes in. So it might be that you actually get a reward. In this case, you get the reward at the end of the game. And I'm spending just a little bit of time on this because this is an important thing to note. But there's different ways to add


05:27:27
up those rewards. it might have like if you take a certain path um the first reward is going to be weighed a little bit less than the last reward because the last reward is actually winning the game or scoring or whatever it is. So this reward system gets really complicated on some of the more advanced uh setups. Um, in this case though, you can see right here that they give a a 0.1 and a 0.5 reward um just for getting a picking the right value and something that's actually valid instead of picking an invalid


05:28:00
value. So, rewards again, that's like key. It's huge. How do you feed the rewards back in? Uh, then we have a board reset. That's pretty straightforward. It just goes back and resets the board to the beginning because it's going to try out all these different things while it's learning. It's going to do it by trial and error. So, you have to keep resetting it. And then, of course, there's the play. We want to go ahead and play uh rounds equals 100. Depends on what you want to


05:28:25
do on here. Um you can set this different. You obviously set that to higher level, but this is just going to go through and you'll see in here uh that we have player one and player two. This is this is the computer playing itself. Uh, one of the more powerful ways to learn to play a game or even learn something that isn't a game is to have two of these models that are basically trying to beat each other. And so they always they keep finding explore new things. This one works for this one.


05:28:57
So this one tries new things. It beats this. We've seen this in um chess I think was a big one where they had the two players in chess with reinforcement learning. uh is one of the ways they train one of the top um computer chess playing algorithms. Uh so this is just what this is. It's going to choose an action. It's going to try something and the more it tries stuff um the more we're going to record the hash. We actually have a board hash where they self get the hash set up on here where it stores all the


05:29:28
information. And then once you get to a win, one of them wins, it gets the reward. Uh then we go back and reset and try again. And then kind of the fun part we actually get down here is uh we're going to play with a human. So we'll get a chance to come in here and see what that looks like when you put your own information in. And then it just comes in here and does the same thing it did above. It gives it a reward for its things um or sees if it wins or ties um looks at available positions, all that


05:29:57
kind of fun stuff. And then finally, we want to show the board. Uh so it's going to print the board out each time. Really um as an integration is not that exciting. What's exciting uh in here is one looking at this reward system. Whoops. Play one more up. The reward system is really the heart of this. How do you reward the different uh setup and the other one is when it's playing it's got to take an action. And so what it chooses for an action is also the heart of reinforcement learning. How


05:30:30
do we choose that action? And those are really key to right now where reinforcement learning is uh in today's uh technology is uh figuring this out. How do we reward it and how do we guess the next best action? So we have our uh environment and you can see the environment is we're going to be or the state uh which is kind of like what's going on. We're going to return the state depending on what happens. And we want to go ahead and create our agent. Uh in this case, our player. So each one


05:31:04
is let me go and grab that. And so we look at a class player. Um this is where a lot of the magic is really going on is what how is this player figuring out how to maneuver around the board? And then the board of course returns a state uh that it can look at and a reward. Uh so we want to take a look at this. We have a name uh self state. This is class player. And when we say class player, we're not talking about a human player. We're talking about um just a uh the computer players. And this is kind of


05:31:35
interesting. So remember I told you depending on what you're doing, there's going to be a decay gamma. Um explore rate. Uh these are what I'm talking about is how do we train it? Um as you try different moves, it gets to the end. The first move is important, but it's not as as important as the last one. And so you could say that the last one has the heaviest weight. And then as you as you get there, the first one, let's see, the first move gives you a five reward, the second gives you a two


05:32:06
reward, and the third one gives you a 10 reward because that's the final ending. You got it. The 10's going to count more than the first step. Uh, and here's our, uh, we're going to, you know, get the board information coming in and then choose an action. This was the second part that I was talking about that was so important. Uh so once you have your training going on, we have to do a little randomness. And you can see right here is our NP random u uniform. So it's picking out a random number. Take a


05:32:36
random action. And this is going to just pick which row and which column it is. Um and so choosing the action. This one you can see we're just doing random states. uh choice, length of positions, action position, and then it skips in there and takes a look at the board uh for P and positions. And you get it's actually storing the different boards each time you go through so it has a record of what it did so it can properly weigh the values. And this simply just appends a hash state. What's the last


05:33:06
state? Pinned it to the uh to our states on here. Here's our feedback reward. the reward comes in and it's going to take a look at this and say is it none uh what is the reward and here is that formula remember I was telling you about up here um that was important because it has decay gamma times the reward this is where as it goes through each step and this is really important this is this is kind of the heart of this of what I was talking about earlier uh you have step one and This might have a a reward of


05:33:44
two. You have step two. I probably should have done ABC. This has a step three. Uh step four. So on till you get to step in. And this might have a reward of 10. Uh so reward of 10. We're going to add that. But we're not adding uh let's say this one right here. Uh let's say this reward here right before 10 was um let's say it's also 10. That just makes the the uh math easy. So we had 10 and 10. We had 10. This is 10 and 10 in whatever it is. But it's time it's 0.9. Uh so instead of


05:34:24
putting a full 10 here, we only do nine. That's uh 0.9 times 10. And so this formula um as far as the decay times the reward minus the cell state value uh it basically adds in it says here's one or here's two. I'm sorry I should have done this ABC it would have been easier. Uh so the first move goes in here and it puts two in here. Uh then we have our self uh setup on here. You can see how this gets pretty complicated in the math, but this is really the key is how do we train our


05:35:05
states and we want the the final state, the win to get the most points. If you win, you get most points. Um, and the first step gets the least amount of points. So, you're really training this almost in reverse. You're training you're training it from the last place where you have like it says okay this is now where need to sum up my rewards and I want to sum them up going in reverse and I want to find the answer in reverse. Kind of an interesting uh uh play on the mind when you're trying to


05:35:32
figure this stuff out. And of course we want to go ahead and reset the board down here. Uh save the policy load policy. These are the different things that are going in between the agent and the state to figure out what's going on. Let's go ahead and load that up. And then finally, we want to go ahead and create a human player. And the human player is going to be a little different uh in that uh you choose an action row and column. Here's your action. Uh if action is if action in positions, meaning positions that are


05:36:09
available, uh you return the action. If not, it just keeps asking you until you get an action that actually works. And then we're going to go ahead and append to the hash state, which uh we don't need to worry about because it returns the action up here and feed forward. Uh again, this is because it's a human. Um at the end of the game, bat propagate and update state values. This part isn't being done because it's not programming uh the model. Uh the model is getting its own


05:36:41
rewards. So, we've gone ahead and loaded this in here. Uh so here's all our pieces. And the first thing we want to do is set up uh P1 player one uh P2 player two. And then we're going to send our players to our state. So now it has P1 P2 and it's going to play and it's going to play 50,000 rounds. Now we can probably do a lot less than this and it's not going to get the full results. In fact, you know what? Uh, let's go ahead and just do five. Um, just to play with it because I want to show you


05:37:14
something here. Oops. Somewhere in there I forgot to load something. There we go. I must have forgot to run this run. Oops, forgot a reference there for the board rows and columns 3x3. There is actually in the state it references that. We just tack it on on the end. It was supposed to be at the beginning. Uh, so now I've only set this up with um, see where are we going here? I've only set this up to train five times. And the reason I did that is we're going to uh, come in and actually play it and


05:37:57
then I'm going to change that and we can see how it differs on there. There we go. And it didn't even make it through a run. And we're going to go ahead and save the policy. Um, so now we have our player one and our player two policy. Uh, the way we set it up, it has two separate policies loaded up in there. And then we're going to come in here and we're going to do uh player one is going to be the computer experience rate zero. Load policy one human player human. And we're going to go ahead and


05:38:27
play this. Now remember, I only went through it um uh just one round of training. In fact, minimal training. And so it puts an X there. And I'm going to go ahead and do row zero, column one. You can see this is very uh basic on here. And so I put in my zero. And then I'm going to go zero, block it, zero, zero. And you can see right here, it let me win. Uh just like that, I was able to win. Zero. Two. And woo, human wins. So I only trained it five times. We're going to run this again. And this time, uh,


05:39:07
instead of five, let's do 5,000 or 50,000. I think that's what the guys in the back had. And this takes a while to train it. This is where reinforcement learning really falls apart. Look how simple this game is. We're talking about a 3x3 set of columns. And so for me to train it on this um I could do a Q table which would take which would go much quicker. Um you could build a quick Q table with almost all the different options on there and uh you would probably get a the same result much


05:39:44
quicker. We're just using this as an example. So when we look at reinforcement learning, you need to be very careful what you apply it to. It sounds like a good deal until you do like a large neural network where you're doing um you set the neural network to a learning increment of one. So every time it goes through it learns and then you do your action. So you pick from the learning uh setup and you actually try actions on the learning setup until you get the what you think is going to be


05:40:13
the best action. So you actually feed what you think is right back through the neural network. There's a whole layer there which is really fun to play with. and then it has an output. Well, think of all those processes. I mean, that is just a huge amount of work it's going to do. Uh, let's go ahead and skip ahead here. Give it a moment. It's going to take a a minute or two to go ahead and run now to train it. Uh, we went ahead and let it run and it took a while. This this took um I got a pretty powerful


05:40:45
processor and it took about five minutes plus to run it. and we'll go ahead and uh run our player setup on here. Oops, it brought in the last Whoops, it brought in the last round. So, give me just a moment to reddo the policy save. There we go. I forgot to save the policy back in there and then go ahead and run our player again. So, we we've saved the policy and then we want to go ahead and load the policy for P1 as a computer. And we can see the computer's gone in the bottom right corner. I'm going to go


05:41:20
ahead and go uh one one which is the center and it's gone right up the top. And if you have ever played tic-tac-toe, you know the computer has me. Uh but we'll go ahead and play it out. Row zero, column two. There it is. And then it's gone here. And so I'm going to go ahead and go row 0 one two. No, 01. There we go. And column zero. That's where I wanted. Oh, and it says I Okay, you your action. There we go. Boom. Uh, so you can see here we've got a didn't catch the win on this. It said tie. Um,


05:41:57
kind of funny that it didn't catch the win on there. But if we play this a bunch of times, you'll find it's going to win more and more. The more we train it, the more the reinforcement happens. This lengthy training process uh is really the stopper on reinforcement learning. As this changes, reinforcement learning will be one of the more powerful uh packages evolving over the next decade or two. In fact, I would even go as far as to say it is the most important uh machine learning tool


05:42:28
and artificial intelligence tool out there as it learns not only a simple tic-tac-toe board, but we start learning environments. And the environment would be like in language. If you're translating a language or something from one language to the other, so much of it is lost if you don't know the context it's in, what's the environments it's in. And so being able to attach environment and context and all those things together is going to require reinforcement learning to do. So again, if you want to get a copy


05:42:57
of the tic-tac-toe board, it's kind of fun to play with. Uh run it, you can test it out, you can do uh you know, test it for different uh uh values. is you can switch from P1 computer uh where we loaded the policy one to load the policy two and just see how it varies. There's all kinds of things you can do on there. So what is Q-learning? Q-learning is reinforcement learning policy which will fill the next best action given a current state. It chooses this action at random and aims to maximize the reward. And so you can see


05:43:27
here's our standard reinforcement learning graph. U by now if you're doing any reinforcement learning you should be familiar with this where you have your agent. Your agent takes an action. The action affects the environment and then the environment sends back the reward or the feedback and the state it's the new state the agents in. Where is it at on the chessboard? Where's it at in the video game? Um, if your robots's out there picking trash up off the side of the road, where is it at on


05:43:55
the road? Consider an ad recommendation system. Usually when you look up a product online, you get ads which will suggest the same product over and over again. Using Q-learning, we can make an ad recommendation system which will suggest related products to our previous purchase. The reward will be if user clicks on the suggested product. And again, you can see um you might have a lot of products on uh your web advertisement or your pages, but it's still not a float number. It's still a set number. And that's something


05:44:32
to be aware of when you're using Q-learning. And you can see here that if you have a 100 people clicking on ads and you click on one of the ads, it might go in there and say, "Okay, this person clicked on this ad. What is the best set of ads based on clicking on this ad or these two ads afterwards, based on where they are browsing? So let's go and take a look at some important terms when we talk about Q-learning. Uh we have states the state S represents the current position of an agent in an


05:44:59
environment. Um the action the action A is the step taken by the agent when it is particular state. Rewards for every action the agent will get a positive or negative reward. And again, uh, when we talk about states, we're usually not with when you're using a Q table, you're not usually talking about float variables. You're talking about true false. Uh, and we'll take a closer look at that in a second. And episodes when an agent ends up in a terminating state and can't take


05:45:30
a new action. Uh, this might be if you're playing a video game, your character stepped in and is now dead or whatever. uh Q values used to determine how good an action A taken at a particular state S is Q A of S and temporal difference a formula used to find the Q value by using the value of the current state and action and previous state and action and very I mean there's Bellman's equation which basically is the equation that kind of uh covers what we just looked at in all those different terms. The


05:46:06
Bellman equation is used to determine the values of a particular state and deduce how good it is to be in take that state. The optimal the optimal state will give us the highest optimal value. Factor influencing Q values the current state and action that's your SA. So your current state and your action. Uh and then you have your previous state in action which is your S um I guess prime. I'm not sure how they how they reference that. S prime A prime. So this is what happened before. Uh then you have a


05:46:38
reward for action. So you have your R reward and you have your maximum expected future reward. And you can see there's also a learning rate put in there and a discount rate. Uh so when we're looking at these just like any other model, we don't want to have an absolute um final value on here. We don't want it to. If you do absolute values instead of taking smaller steps, you don't really have that approach to the solution. You just have a jump and then pretty soon if you jump one solution out, that's what's


05:47:13
going to be the new solution. Whichever one jumps up really high first. Um kind of ruining the whole idea of doing a random selection. And I'll go into the random selection in just a second. Steps in Q-learning. Step one, create an initial Q table with all values initialized to zero. Again, we're looking at 01. Uh, so are you, you know, here's our action. We start, we're an idle. We took a wrong action. We took a correct action and end. And then we have our um actions fetching, sitting,


05:47:46
and running. And of course, we're just using the dog example. And choose an action and perform it. Update values in the table. And of course, when we're choosing an action, we're going to kind of do something random and just randomly pick one. So, you start out and you sit and you have then a um then depending on that um um action you took, you can now update the value for sitting after you start from start to sitting. Get the value of the reward and calculate the val the value Q value using the Bellman


05:48:17
equation. And so now we attach a reward to sitting. And when we attach all those rewards, we continue the same until the table's filled with or an episode ends. And and I mentioned I was going to come back to the random side of this. And there's a few different formulas they use for the random uh setup to pick it. I usually let whatever Q model I'm using do their standard one because someone's usually gone in and done the math uh for the optimal uh spread. Uh but you can look at this. If I have


05:48:47
running has a reward of 10, sitting has a reward of seven, fetching has a reward of five. Um, just kind of without doing like a a means, you know, using the bell curve for the means value. And like I said, there's some math you can put in there to pick um so that you're more like so that running has even a higher chance. Uh, but even if you were just going to do an average on this, you could do an average, a random number by adding them all together. Uh so you get 10 + 7 + 5 is 22. You could do 0 to 22


05:49:20
and or 0 to 21 but 1 to 22. 1 to five would be fetching uh and so forth. You know the last 10. So you can just look at this as what percentage are you going to go for that particular option. Um and then that gets your random setup in there. And then as you slowly increment these up, uh you see that uh u if you're idle, u where's one? Here we go. Sitting at the end, if you're at the end of wherever you're at, sitting gets a reward of one. Um where's a good one on here? Oh, wrong action.


05:49:52
Running for a wrong action gets almost no reward. So that becomes very very less likely to happen, but it still might happen. It still might have a percentage of coming up. And that's where the random programming and Q-learning comes in. The below table gives us an idea of how many times an action has been taken and how positively correct action or negatively wrong action it is going to affect the next state. So let's go ahead and dive in and pull up a little piece of code and see what this looks like um in


05:50:23
Python. Uh in this demo we'll use Q-learning to find the shortest path between two given points. If getting your learning started is half the battle, what if you could do that for free? Visit Skillup by SimplyLearn. Click on the link in the description to know more. If you've seen my videos before, um I like to do it in the uh Anaconda Jupiter notebook um setup just because it's really easy to see and it's a nice demo. Uh and so here's my Anaconda. This one I'm actually using a


05:50:55
Python 36 environment that I set up in here. And we'll go ahead and launch the Jupyter Notebook on this. And once we're in our Jupyter notebook, uh, which has the kernel loaded with Python 3, we'll go ahead and create a new Python 3 uh, folder in here. And we'll call this, uh, Q learning. And to start this demo, let's go ahead and import our uh, numpy array. We'll just run that. So, it's imported. And like a lot of these uh, model programs, when you're building them, you spend a lot of time putting it


05:51:31
all together. Um and then you end up with this really short answer at the end. Uh and we'll we'll take a look at that as we come into it. So we we go ahead and start with our location to state. Uh so we have um L1, L2. These are our nine locations 1 to nine. And then of course the state is going to be 0 1 2 3 4. It's just a mapping of our location to a integer on there. And then we have our actions. Our actions are simply uh moving from uh one location to another. So I can go to I can go to location zero. I can go to


05:52:08
location 1 2 3 4 5 6 7 8. Uh so these are my actions. I can choose these are the locations of our state. And if you remember earlier, I mentioned uh um that the limitation is that you you don't want to put in um a continually growing table because you can actually create a dynamic Q table where you continually add in new values as they arise because um if you have float values, it just becomes infinite and then your memory in your computer's gone or you know does it's not going to work.


05:52:42
At the same time, you might think, well, that kind of really limits the the Q uh T learning setup, but there are ways to use it in conjunction with other systems. And so you might look at uh well I do um I've been doing some work in stock um and one of the questions that comes out is to buy or sell the stock and the stake coming in might be um you might take it and create what we call buckets um where anything that you predict is going to return more than a certain amount of money um the error for that


05:53:21
stock that you've had in the past you put those in buck buckets and suddenly you start putting the creating these buckets you realize you do have a limited amount of information coming in. You no longer have a float number. You now have um bucket 1 2 3 and four. And then you can take those buckets, put them through a a Q-learning table and come up with the best action. Which stock should I buy? It's like gambling. Stock is pretty much gambling if you're doing day trading. You're not doing


05:53:49
long-term um investments. And so you can start looking at it like that. A lot of the um current feeds say that the best algorithms used for day traders where you're doing it on your own is really to ask the question, do I want to trade the stock? Yes or no? And now you have it in a Q-learning table and now you can take it to that next level. And you can see where that can be a really powerful tool at the end of doing a basic linear regression model or something. um what is the best investment and you start


05:54:19
getting the best reward on there. Uh and so if we're going to have rewards, these rewards we just create um it says uh if basically if you're uh this should match our Q table because it's going to be uh you have your state and you have your action across the top if you remember from the dog. And so we have whatever state we're in going down and then the next action and what the reward is for it. Um, and of course, if you were actually doing a u something more connected, your reward would be based on


05:54:51
uh the actual environment it's in. And then we want to go ahead and create a state to location uh so we can map the indexes. So just like we defined our rewards, uh we're going to go and do state to location. Um and you can see here it's a a dictionary setup for location state and location to state with items. And we also need to um define what we want for learning rates. Uh you remember we had our two different rates um as far as like learning from the past and learning from the current. So we'll go ahead and set


05:55:29
those to uh 75 and the alpha set to 0.9. And we'll see that when we do the formula. And of course any of this code uh send a note to our SimplyLearn team. and you'll get your copy of this code on here. Let's go ahead and pull. There we go. The new next two sections. Um, since we're going to keep it short and sweet. Here we go. So, let's go ahead and create our agent. Um, so our agent is going to have our initialization where we send it all the information. Uh, we'll define our self


05:56:06
gamma equals gamma. We could have just set the gamma rate down here instead of uh submitting it. It's kind of nice to keep them separate because you can play with these numbers uh our self-alpha. Um then we have our location state. We'll set that in here. Um we have our choice of actions. Um we're going to go ahead and just embed the rewards right into the agent. So obviously this would be coming from somewhere else uh instead of from uh self-generated. and then a self state to location equals our state to


05:56:39
location uh dictionary. And we go ahead and create a Q-learning table. And I went ahead and just set the Q-learning table up to um uh 0 to zero. What what what the setup is uh location to state. How many of them are there? Uh and this just creates an array of zero to zero setup on there. And then the big part is the training. We have our rewards new equals a copy of self.rewards. rewards ending state equals the self-loation state end location. So this is whatever we end up at. Rewards new equals ending state plus ending state


05:57:14
equals 999. Just kind of goes to a dead end and we start going through iterations and we'll go ahead. Um let's do this. Uh so this we're going to come back and we're going to call call it on here. Uh, let me just erase that. Switch it to an arrow. There we go. Uh, so what we're doing is we're going to send in here to train it. We're going to say, hey, um, I want to iterate through this a thousand times and see what happens. Now, this part would actually be instead of


05:57:51
iterating, you might have your external environment and they're going back and forth and you iterate through outside of here. Uh, but just for ease of use, our agent's going to come in here and iterate through this. Sometimes I'll put this iteration in here and I'll have it call the environment and say, "Hey, this is what I did. What's the next state?" And the environment does its thing right in here as I iterate through it. Uh, and then we want to go ahead and pick a random state to start with.


05:58:20
That's what's going on here. You have to start somewhere. Um, and then you have your playable actions. is we're going to start with just an empty thing for playable actions and we'll fill that up. So that's what choices I have. And so we're going to iterate through the rewards matrix to get the states uh directly reachable from the randomly chosen current state. Assign those states to a list named playable actions. And so you can see here we have uh range nine. I usually use length of


05:58:49
whatever I'm looking at u which is our locations or states as they are. uh we have a reward. So we want to look at the current the rewards uh the new reward is our uh is in our chart here of rewards new uh current state um plus J. Uh J being what is the next state we want to try. And so we go ahead and do our playable actions and we append J. And so we're doing is we're randomly trying different things in here to see what's going to generate a better reward. And then of course we go ahead and


05:59:25
choose our next state. Uh so we have our random choice playable actions. And if you remember I mentioned on this let me just go ahead and uh whoops let's do a free form. When we were talking about the next state uh this right here just does a random selection. Instead of a random uh selection, you might do something where uh whatever the best selection is, which might be option three here. And then so you can see that it might use a bell curve and then option two over here might have a bell


06:00:01
curve like this. Oops. And we start looking at these averages and these spreads. Um or we can just add them all together and pick the one that kind of goes in all of those. Uh so those are some of the options we have in here. We just go with a random choice. Uh that's usually where you start, play with it. Um and then we have our reward section down here. And so we want to go ahead and find well in this case the temporal difference. Uh so you have your rewards new plus the self gamma. And this is the


06:00:32
formula we were looking at. This is Bellman's equation here. Uh so we have our current value, our learning rate, our discount rate involved in there, the reward system coming in for that. Uh and we can add it all together. This is of course our maximum expected future setup in here. Uh so this is all of our our Bellman's equation that we're looking at here. And then we come up in here and we update our Q table. That's all this is on this one. Uh that's right here. We have um self Q current state next state


06:01:05
and we add in our um alpha because we don't want to we don't want to train all of it at once in case there's slight differences coming in there. We want to slowly approach the answer. Uh and then we have our route equals the start location and next location equals start location. So we're just incrementing. We took a step forward. And then finally, remember I was telling you how uh we're going to do all this and just have some simple thing at the end where it just generates a simple path. We're going to


06:01:34
go ahead and and get the optimal route. We want to find the best route in here. And so we've created a definition for the optimal route down here. Just scroll down for that. And we get the optimal route. uh we go ahead and put the information in including the Q table self uh start location in location next location route Q and it says while next location is not equal to end location. So while we can still go our start location equals selflo to state start location. So we already have our best


06:02:05
value for the start location. Uh the next state looks at the Q table and says, "Hey, what's uh the next one with the best value?" And then the next location, we go ahead and pull that in and we just append it. That's what's going on down here. And then our start location equals the next location. And we just go through all the steps. And we'll go ahead and run this. And now that we have our Q table, our um Q agent loaded, we're going to go ahead and uh take our Q agent, load them up


06:02:35
with our alpha, gamma that we set up above um along with the location step, action, reward, state to location, and uh our goal is to plot a course between L9 and L1. And we're going to go through a 100 a thousand iterations on here. And so when I run that, it runs pretty quick. Uh why is this so fast? Um if you've been running neural networks and you've been doing all these other models, you sit here and wait a long time. Well, we're very small amount of data. These are all integers. These aren't float


06:03:08
values. There's not a the math is not heavy on the on the processing end. And this is where Q tables are so powerful. If you have a small amount of information coming in, you very quickly uh get an answer off of this even though we went through it a thousand times to train it. And you'll see here we have L98 52 and one. And that's based on our reward table we had set up on there. And this is the shortest path going between these different uh setups in here. And if you remember on our reward table, uh


06:03:41
you can see that if you start here, you can go to here. there's places you can't go. That's how this reward table was set up. So, I can only go to certain places. Uh so, kind of a little maze setup in there. And you can play with it. This is really fun uh setup to play with. Uh and you can see how you can take this whole code and you can like I was saying earlier, you can embed it into another setup and model and predictions where you put things into buckets and you're trying to guess the


06:04:09
best investment, the best course of action. long as you can take that course of action and and uh uh reduce it down to a yes no um or if you're using text, you can use one hot encoder, which word is next. There's all kinds of things you can do with a Q table, uh depending on just how much information you're putting in there. So, that wraps up our demo. In this demo, we've uh found the shortest distance between two paths based on whatever rules or state rewards we have to get from point A to point B and what


06:04:40
available actions there are. Hello and welcome to this tutorial on deep learning. My name is Moan and in the next about one one and a half hours I will take you through what is deep learning and into TensorFlow environment to show you an example of deep learning. Now there are several applications of deep learning really very interesting and innovative applications and one of them is identifying the geographic location based on a picture. And how does this work? The way it works is pretty much we train an artificial


06:05:15
neural network with millions of images which are tagged. their geolocation is tagged and then when we feed a new picture, it will be able to identify the geoloccation of this new image. For example, you have all these images especially with maybe some significant monuments or or u significant locations and you train with millions of such images and then when you feed another image it need not be exactly one of those that you have trained. It can be completely different. That is the whole idea of cleaning. It will be able to


06:05:51
recognize for example that this is a picture from Paris because it is able to recognize the Eiffel Tower. So the way it works internally if we have to look a little bit under the hood is these images are nothing but this is digital information in the form of pixels. So each image could be a certain size. It can be 256 x 256 pixel kind of a resolution and then each pixel is either having a certain grade of color and all that is fed into the neural network and it then gets trained in and it's able to


06:06:30
based on these pixels pixel information it is able to get trained and able to recognize the features and extract the features and thereby it is able to identify these images and the location of these images and then when you feed a new image it kind of based on the training it will be able to figure out where this images from. So that's the way a little bit under the hood how it works. So what are we going to do in this tutorial? We will see what is deep learning and what do we need for deep


06:07:03
learning and one of the main components of deep learning is neural network. So we will see what is neural network what is a perceptron and how to implement logic gates like and or nor and so on using perceptrons the different types of neural networks and then applications of deep learning and we will also see how neural networks works. So how do we do the training of neural networks and at the end we will end up with a small demo code which will take you through in TensorFlow. Now in order to implement


06:07:39
deep learning code there are multiple libraries or development environments that are available and TensorFlow is one of them. So the focus at the end of this would be on how to use TensorFlow to write a piece of code using Python as a programming language. And we will take up a an example which is a very common one which is like the hello world of deep learning the handwriting number recognition which is a mnest commonly known as mnest database. So we will take a look at mnest database and how we can


06:08:12
train a neural network to recognize handwritten numbers. So that's what you will see in this particular video. So let's get started. What is deep learning? Deep learning is like a subset of what is known as a highlevel concept called artificial intelligence. You must be already familiar must have heard about this term artificial intelligence. So artificial intelligence is like the highle concept if you will and in order to implement artificial intelligence applications we use what is known as


06:08:47
machine learning. And within machine learning, a subset of machine learning is deep learning. Machine learning is a little bit more generic concept. And deep learning is one type of machine learning if you will. And we will see a little later in maybe the following slides a little bit more in detail how deep learning is different from traditional machine learning. But to start with we can mention here that deep learning uses one of the differentiators between deep learning and traditional machine learning is that deep learning


06:09:19
uses neural networks and we will talk about what are neural networks and how we can implement neural networks and so on and so forth as a part of this tutorial. So a little deeper into deep learning. Deep learning primarily involves working with complicated unstructured data compared to traditional machine learning where we normally use structured data. In deep learning the data would be primarily images or voice or maybe text file. So and it is large amount of data as well. And deep learning can handle complex


06:09:54
operations. It involves complex operations. And the other difference between traditional machine learning and deep learning is that the feature extraction happens pretty much automatically. In traditional machine learning, feature engineering is done manually. The data scientists, we data scientists have to do feature engineering, feature extraction. But in deep learning that happens automatically and of course deep learning for large amounts of data, complicated unstructured data, deep learning gives


06:10:23
very good performance. Now as I mentioned one of the secret sources of deep learning is neural networks. Let's see what neural networks is. Neural networks is based on our biological neurons. The whole concept of deep learning and artificial intelligence is based on human brain and human brain consists of billions of tiny stuff called neurons. And this is how a biological neuron looks. And this is how an artificial neuron looks. So neural networks is like a simulation of our human brain. Human brain has billions of


06:11:07
biological neurons and we are trying to simulate the human brain using artificial neurons. This is how a biological neuron looks. It has dendrites and the corresponding component with an artificial neural network is or an artificial neuron are the inputs. They receive the inputs through dendrites and then there is the cell nucleus which is basically the processing unit in a way. So in artificial neuron also there is a piece which is an equivalent of this cell nucleus and based on the weights and


06:11:44
biases. We will see what exactly weights and biases are as we move the input gets processed and that results in an output. In a biological neuron the output is sent through a synapse and in an artificial neuron there is an equivalent of that in the form of an output. And biological neurons are also interconnected. So there are billions of neurons which are interconnected. In the same way artificial neurons are also interconnected. So this output of this neuron will be fed as an input to another neuron and so on. Now in neural


06:12:16
network one of the very basic units is a perceptron. So what is a perceptron? A perceptron can be considered as one of the fundamental units of neural networks. It can consist at least one neuron but sometimes it can be more than one neuron but you can create a perceptron with a single neuron and it can be used to perform certain functions. It can be used as a basic binary classifier. It can be trained to do some basic binary classification. And this is how a basic perceptron looks like. And this is nothing but a neuron.


06:12:56
You have inputs x1, x2, x uh to xn and there is a summation function and then there is what is known as an activation function and based on this input what is known as the weighted sum. The activation function either gets gives an output like a zero or a one. So we say the neuron is either activated or not. So that's the way it works. So you get the inputs. These inputs are each of the inputs are multiplied by a weight and there is a bias that gets added and that whole thing is fed to an activation


06:13:28
function and then that results in an output. And if the output is correct, it is accepted. If it is wrong, if there is an error, then that error is fed back and the neuron then adjusts the weights and biases to give a new output and so on and so forth. So that's what is known as the training process of a neuron or a neural network. There's a concept called perceptron learning. So perceptron learning is again one of the very basic learning processes. The way it works is somewhat like this. So you have all


06:14:01
these inputs like x1 to xn and each of these inputs is multiplied by a weight and then that sum this is the formula the equation. So that sum wi x i sigma of that which is a sum of all these product of x and w is added up and then a bias is added to that. The bias is not dependent on the input but or the input values but the bias is common for one neuron. However the bias value keeps changing during the training process. Once the training is completed, the values of these weights W1, W2 and so on


06:14:39
and the value of the bias gets fixed. So that is basically the whole training process and that is what is known as the perceptron training. So the weights and biases keep changing till you get the accurate output and the summation is of course passed through the activation function. As you see here, this wixi summation plus b is passed through activation function and then the neuron gets either fired or not and based on that there will be an output that output is compared with the actual or expected


06:15:17
value which is also known as labeled information. So this is the process of supervised learning. So the output is already known and um that is compared and thereby we know if there is an error or not and if there is an error the error is fed back and the weights and biases are updated accordingly till the error is reduced to the minimum. So this iterative process is known as perceptron learning or perceptron learning rule and this error needs to be minimized. So till the error is minimized this


06:15:53
iteratively the weights and biases keep changing and that is what is the training process. So the whole idea is to update the weights and the bias of the perceptron till the error is minimized. The error need not be zero. The error may not ever reach zero. But the idea is to keep changing these weights and bias so that the error is minimum. the minimum possible that it can have. So this whole process is an iterative process and this is the iteration continues till either the error is zero which is uh unlikely


06:16:32
situation or it is the minimum possible within these given conditions. Now in 1943 two scientists Warren Melikch and Walter Pittz came up with an experiment where they were able to implement the logical functions like and or and nor using neurons and that was a significant breakthrough in a sense. So they were able to come up with the most common logical gates. they were able to implement some of the most common logical gates which could take two inputs like a and b and then give a corresponding result. So for example in


06:17:14
case of an and gate a and b and then the output is a b in case of an orgate it is a plus b and so on and so forth and they were able to do this using a single layer perceptron. Now most of these gates it was possible to use single layer perceptron except for XR and we will see why that is in a little bit. So this is how an AND gate works. The inputs A and B the output should be fired or the neuron should be fired only when both the inputs are one. So if you have 0 0 the output should be zero. For


06:17:50
0 1 it is again 0 1 0 again 0 and 1 one the output should be one. So how do we implement this with a neuron? So it was found that by changing the values of weights it is possible to achieve this logic. So for example if we have equal weights like 7.7 and then if we take the sum of the weighted product so for example 7 into 0 and then 7 into 0 will give you zero and so on and so forth. And in the last case when both the inputs are one you get a value which is greater than one which is the threshold.


06:18:26
So only in this case the neuron gets activated and the output is there is an output. In all the other cases there is no output because the threshold value is one. So this is implementation of an AND gate using a single perceptron or a single neuron. Similarly an orgate. In order to implement an argate in case of an argate the output will be one if either of these inputs is one. So for example 01 will result in one or rather in all the cases it is one except for 0 0. So how do we implement this using a


06:18:58
perceptron once again if you have a perceptron with weights for example 1.2. Now if you see here if in the first case when both are zero the output is zero. In the second case when it is 0 and 1 1.2 2 into 0 is 0 and then 1.2 into 1 is 1 and in the second case similarly the output is 1.2 in the last case when both the inputs are one the output is 2.4. So during the training process these weights will keep changing and then at one point where the weights are equal to w1 is equal to 1.2 and w2 is equal to


06:19:35
1.2 the system learns that it gives the correct output. So that is implementation of orgate using a single neuron or a single layer perceptron. Now XR gate this was one of the challenging ones. They tried to implement an XR gate with a single level perceptron but it was not possible and therefore in order to implement an XR. So this was like a a roadblock in the progress of U neural network. However, subsequently they realized that this can be implemented an XR gate can be implemented using a multi-level perceptron or MLP. So in


06:20:13
this case there are two layers instead of a single layer. And this is how you can implement an XR gate. So you will see that X1 and X2 are the inputs and there is a hidden layer and that's why it is denoted as H3 and H4. And then you take the output of that and feed it to the output at 05 and provide a threshold here. So we will see here that this is the numerical calculation. So the weights are in this case for X1 it is 20 and minus 20 and once again 20 and minus 20. So these inputs are fed into H3 and


06:20:49
H4. So you'll see here for H3 the input is 0 1 1 and for H4 it is 1 0 1 1 and if you now look at the output final output where the threshold is taken as one if you use a sigmoid with the threshold one you will see that in these two cases it is zero and in the last two cases it is one. So this is the implementation of XR. In case of XR only when one of the inputs is one you will get an output. So that is what we are seeing here. If we have either both the inputs are one or both the inputs are zero then the output


06:21:30
should be zero. So that is what is an exclusive or gate. So it is exclusive because only one of the inputs should be one and then only you'll get an output of one which is satisfied by this condition. So this is a special implementation. XR gate is a special implementation of a perceptron. Now that we got a good idea about perceptron, let's take a look at what is a neural network. So we have seen what is a perceptron, we have seen what is a neuron. So we will see what exactly is a neural network. So neural network is


06:21:59
nothing but a network of these neurons and they are different types of neural networks. There are about five of them. These are artificial neural network, convolutional neural network, then recursive neural network or recurrent neural network, deep neural network and deep belief network. So, and each of these types of neural networks have a special you know they can solve a special kind of problems. For example, convolutional neural networks are very good at performing image processing and image recognition and so on. Whereas RNN


06:22:35
are very good for speech recognition and also text analysis and so on. So each type has some special characteristics and they can uh they are good at performing certain special kind of tasks. What are some of the applications of deep learning? Deep learning is today used extensively in gaming. You must have heard about Alph Go which is a game created by a startup called Deep Mind which got acquired by Google and Alph Go is an AI which defeated the human world champion Lee Sadal in this game of go.


06:23:14
So gaming is an area where deep learning is being extensively used and a lot of research happens in the area of gaming as well. In addition to that nowadays there are neural networks of special type called generative adversarial networks which can be used for synthesizing either images or music or text and so on and they can be used to compose music. So the neural network can be trained to compose a certain kind of music and autonomous cars. You must be familiar with Google. Google's self-driving car and today a lot of


06:23:51
automotive companies are investing in this space and uh deep learning is a core component of this autonomous cars. The cars are trained to recognize for example the road the the lane markings on the road signals any objects that are in front any obstruction and so on and so forth. So all this involves deep learning. So that's another major application and uh robots we have seen several robots including Sophia you may be familiar with Sophia who was given a citizenship by Saudi Arabia and there


06:24:28
are several such robots which are very humanlike and the underlying technology in many of these robots is deep learning. Medical diagnostics and healthc care is another major area where deep learning is being used. And within healthcare diagnostics again there are multiple areas where deep learning and image recognition image processing can be used. For example for cancer detection as you may be aware if cancer is detected early on it can be cured. And one of the challenges is in the availability of specialists who can


06:25:05
diagnose cancer using these diagnostic images and various scans and and so on and so forth. So the idea is to train neural network to perform some of these activities so that the load on the cancer specialist doctors or oncologists uh comes down and there is a lot of research happening here and there are already quite a few applications that are claimed to be performing better than human beings in this space. Can be lung cancer, it can be breast cancer and so on and so forth. So healthcare is a


06:25:40
major area where deep learning is being applied. Let's take a look at the inner working of a neural network. So how does an artificial neural network let's say identify can we train a neural network to identify the shapes like squares and circles and triangles when these images are fed. So this is how it works. Any image is nothing but it is a digital information of the pixels. So in this particular case, let's say this is an image of 28x 28 pixel and this is an image of a square. There's a certain way


06:26:16
in which the pixels are lit up. And so these pixels have a certain value maybe from 0 to 256 and 0 indicates that it is black or it is dark and 256 indicates it is completely it is white or lit up. So that is like an indication or a measure of the how the pixels are lit up. And so this is an image is let's say consisting of information of 784 pixels. So all the information what is inside this image can be kind of compressed into this 784 pixels. The way each of these pixels is lit up provides information about what


06:27:00
exactly is the image. So we can train neural networks to use that information and identify the images. So let's take a look how this works. So each neuron the value if it is close to one that means it is white whereas if it is close to zero that means it is black. Now this is a an animation of how this whole thing works. So these pixels one of the ways of doing it is we can flatten this image and take this complete 784 pixels and feed that as input to our neural network. The neural network can consist


06:27:43
of probably several layers. There can be a few hidden layers and then there is an input layer and an output layer. Now the input layer takes these 784 pixels as input. The values of each of these pixels and then you get an output which can be of three types or three classes. One can be a square, a circle or a triangle. Now during the training process there will be initially obviously you feed this image and it will probably say it's a circle or it will say it's a triangle. So as a part of the training process, we then send


06:28:16
that error back and the weights and the biases of these neurons are adjusted till it correctly identifies that this is a square. That is the whole training mechanism that happens out here. Now let's take a look at a circle. Same way. So you feed these 784 pixels. There is a certain pattern in which the pixels are lit up and the neural network is trained to identify that pattern. And during the training process once again it would probably initially identify it incorrectly saying this is a square or a


06:28:57
triangle and then that error is fed back and the weights and biases are adjusted finally till it finally gets the image correct. So that is the training process. So now we will take a look at same way a triangle. So now if you feed another image which is consisting of triangles. So this is the training process. Now we have trained our neural network to classify these images into a triangle or a circle and a square. So now this neural network can identify these three types of objects. Now if you feed


06:29:36
another image and it will be able to identify whether it's a square or a triangle or a circle. Now what is important to be observed is that when you feed a new image it is not necessary that the image or the the triangle is exactly in this position. Now the neural network actually identifies the patterns. So even if the triangle is let's say positioned here not exactly in the middle but maybe at the corner or in the side it would still identify that it is a triangle and that is the whole idea


06:30:12
behind pattern recognition. So how does this training process work? This is a quick view of how the training process works. So we have seen that a neuron consists of inputs. It receives inputs and then there is a weighted sum which is nothing but this x i wi summation of that plus the bias and this is then fed to the activation function and that in turn gives us a output. Now during the training process initially obviously when you feed these images when you send maybe a square it will identify it as a


06:30:48
triangle and when you maybe feed a triangle it will identify as a square and so on. So that error information is fed back and initially these weights can be random. Maybe all of them have zero values and then it will slowly keep changing. So the as a part of the training process the values of these weights W1, W2 up to WN keep changing in such a way that towards the end of the training process it should be able to identify these images correctly. So till then the weights are adjusted and that is known as the training process. So and


06:31:22
these weights are numeric values. It could be 0.5.25.35 and so on. It could be positive or it could be negative. And the value that is coming here is the pixel value as we have seen. It can be anything between 0 to 1. You can scale it between 0 to 1 or 0 to 256 whichever way. 0 being black and 256 being white. And then all the other colors in between. So that is the input. So these are numerical values. this multiplication or the product wi xi is a numerical value and the bias is also a numerical value. We need to keep in mind


06:31:56
that the bias is fixed for a neuron it doesn't change with the inputs whereas the weights are one per input. So that is one important point to be noted. So but the bias also keeps changing. Initially it will again have a random value but as a part of the training process the weights the values of the weights w1 w2 wn and the value of b will change and ultimately once the training process is complete these values are fixed for this particular neuron. W1 w2 up to wn and plus the value of the b is


06:32:30
also fixed for this particular neuron. And in this way there will be multiple neurons and each there may be multiple levels of neurons here and that's the way the training process works. So this is another example of multilayer. So there are two hidden layers in between and then you have the input layer values coming from the input layer. Then it goes through multiple layers hidden layers and then there is an output layer. And as you can see there are weights and biases for each of these neurons in in each layer. And all of


06:33:02
them gets keeps changing during the training process. And at the end of the training process all these weights have a certain value and that is a trained model and those values will be fixed once the training is completed. All right. Then there is something known as activation function. Neural networks consists of uh one of the components in neural networks is activation function and every neuron has an activation function and there are different types of activation functions that are used. It could be a relu, it could be sigmoid


06:33:35
and so on and so forth and the activation function is what decides whether a neuron should be fired or not. So whether the output should be zero or one is decided by the activation function and the activation function in turn takes the input which is the weighted sum. Remember we talked about wixi plus b. That weighted sum is fed as a input to the activation function and then the output can be either a zero or a one. And there are different types of activation functions which are covered in an earlier video you might want to


06:34:07
watch. All right. So as a part of the training process, we feed the inputs the labeled data or the training data and then it gives an output which is the predicted output by the network which we indicate as Yhat and then there is a labeled data because we for supervised learning we already know what should be the output. So that is the actual output and in the initial process before the training is complete obviously there will be error. So that is measured by what is known as a cost function. So the


06:34:38
difference between the predicted output and the actual output is the error and u the cost function can be defined in different ways. There are different types of cost functions. So in this case it is like the average of the squares of the error. So and then all the errors are added which can sometimes be called as sum of squares sum of square errors or SSC and that is then fed as a feedback in what is known as backward propagation or back propagation and that helps in the network adjusting the weights and biases and so the weights


06:35:16
and biases get updated till this value the error value or the cost function is minimum. Now there is a optimization technique which is used here called gradient descent optimization and this algorithm works in a way that the error which is the cost function needs to be minimized. So there's a lot of mathematics that goes behind this. For example, they find the uh local minima and the global minima using the differentiation and so on and so forth. But the idea is this. So as a training process as the as the part of training


06:35:55
the whole idea is to bring down the error which is like let's say this is the function the cost function at certain levels it is very high the cost value of the cost function or the output of the cost function is very high. So the weights have to be adjusted in such a way and also the bias of course that the cost function is minimized. So there is this optimization technique called gradient descent that is used and this is known as the learning rate. Now gradient descent you need to specify


06:36:29
what should be the learning rate and the learning rate should be optimal because if you have a very high learning rate then the optimization will not converge because at some point it will cross over to the side. On the other hand, if you have very low learning rate, then it might take forever to convert. So you need to come up with the optimum value of the learning rate. And once that is done using the gradient descent optimization, the error function is reduced and that's like the end of the


06:37:06
training process. All right. So this is another view of gradient descent. So this is how it looks. This is your cost function. the output of the cost function and that has to be minimized using gradient descent algorithm and these are like the parameters and weight could be one of them. So initially we start with certain random values so cost will be high and then the weights keep changing and in such a way that the cost function needs to come down and at some point it may reach the minimum value and


06:37:35
then it may increase. So that is where the gradient descent algorithm decides that okay it has reached the minimum value and it will kind of try to stay here. This is known as the global minima. Now sometimes these curves may not be just for explanation purpose this has been drawn in a nice way but sometimes these curves can be pretty erratic. There can be some local minima here and then there is a peak and then and so on. So the whole idea of gradient descent optimization is to identify the global minima and to find the weights


06:38:10
and the bias at that particular point. So that's what is gradient descent and then this is another example. So you can have these multiple local minima. So as you can see at this point when it is coming down it may appear like this is a minimum value but then it is not. This is actually the global minimum value and the gradient descent algorithm will make an effort to reach this level and not get stuck at this point. So the algorithm is already there and it knows how to identify this global minimum and


06:38:43
that's what it does during the training process. Now in order to implement deep learning there are multiple platforms and languages that are available but the most common platform nowadays is tensorflow and so that's the reason we have uh this tutorial we created this tutorial for tensorflow. So we will take you through a quick demo of how to write a tensorflow code using python and tensorflow is uh an opensource platform created by Google. So let's just take a look at the details of TensorFlow. And


06:39:16
so this is a a library a Python library. So you can use Python or any other languages. It's also supported in other languages like Java and R and so on. But Python is the most common language that is used. So it is a library for developing deep learning applications especially using neural networks and it consists of primarily two parts if you will. So one is the tensors and then the other is the graphs or the flow. That's the way the name that's the reason for this kind of a name called tensorflow.


06:39:50
So what are tensors? Tensors are like multi-dimensional arrays if you will. That's one way of looking at it. So usually you have a one-dimensional array. So first of all you can have what is known as a scalar which means a number. And then you have a one-dimensional array something like this which means this is like a set of numbers. So that is a one-dimensional array. Then you can have a two-dimensional array which is like a matrix and beyond that sometimes it gets difficult. So this is a three-dimensional array. But TensorFlow


06:40:22
can handle many more dimensions. So it can have multi-dimensional arrays. That is the strength of TensorFlow and which makes computation deep learning computation much faster and that's the reason why TensorFlow is used for developing deep learning applications. So TensorFlow is a deep learning tool and this is the way it works. So the data basically flows in the form of tensors and the way the programming works as well is that you first create a graph of how to execute it and then you actually execute that particular graph


06:40:59
in the form of what is known as a session. We will see this in the TensorFlow code as we move forward. So all the data is managed or manipulated in tensors and then the processing happens using this graphs. There are certain terms called like for example ranks of a tensor. The rank of a tensor is like a dimensional dimensionality in a way. So for example if it is scalar so there is just a number just one number. The rank is supposed to be zero and then it can be a one-dimensional vector in which case the rank is supposed to be


06:41:36
one and then you can have a two-dimensional vector typically like a matrix. then in that case we say the rank is two and then if it is a three-dimensional array then it rank is three and so on. So it can have more than three as well. So it is possible that you can store multi-dimensional arrays in the form of tensors. So what are some of the properties of tensorflow? I think today it is one of the most popular platform. TensorFlow is the most popular deep learning platform or library. It is open source. So


06:42:11
developed by Google, developed and maintained by Google, but it is open source. One of the most important things about TensorFlow is that it can run on CPUs as well as GPUs. GPU is a graphical processing unit just like CPU is central processing unit. Now in earlier days GPU was used for primarily for graphics and that's how the name has come and one of the reasons is that it cannot perform generic activities very efficiently like CPU but it can perform iterative actions or computations extremely fast and much


06:42:48
faster than a CPU. So they are really good for computational activities and in deep learning there is a lot of iterative computation that happens. So in the form of matrix multiplication and so on. So GPUs are very well suited for this kind of computation and TensorFlow supports both GPU as well as CPU. And there's a certain way of writing code in TensorFlow. We will see as we go into the code and of course TensorFlow can be used for traditional machine learning as well but then that would be an overkill


06:43:19
but just for understanding it may be a good idea to start writing code for a normal machine learning use case so that you get a hang of how TensorFlow code works and then you can move into neural networks. So that is u just a suggestion but if you're already familiar with how tensorflow works then probably yeah you can go straight into the neural networks part. So in this tutorial we will take the use case of recognizing handwritten digits. This is like a hello world of deep learning. And this is a nice little


06:43:56
M&S database is a nice little database that has images of handwritten digits nicely formatted because very often in deep learning and neural networks. We end up spending a lot of time in preparing the data for training. And with MNEST database, we can avoid that. you already have the data in the right format which can be directly used for training and MNEST also offers a bunch of built-in utility functions that we can straight away use and call those functions without worrying about writing


06:44:33
our own functions and that's one of the reasons why MNEST database is very popular for training purposes initially when people want to learn about deep learning and TensorFlow this is the database that is used and it has a collection of 70,000 handwritten digits and a large part of them are for training. Then you have test just like in any machine learning process and then you have validation and all of them are labeled. So you have the images and their label and these images they look somewhat like this. So they are


06:45:07
handwritten images collected from a lot of individuals. People have these are samples written by human beings. They have handwritten these numbers. These numbers going from 0 to 9. So people have written these numbers and then the images of those have been taken and formatted in such a way that it is very easy to handle. So that is mnest database and the way we are going to implement this in our tensorflow is we will feed this data especially the training data along with the label information and uh the data is basically


06:45:46
these images are stored in the form of the pixel information as we have seen in one of the previous slides. All the images are nothing but these are pixels. So an image is nothing but an arrangement of pixels and the value of the pixel either it is lit up or it is not or in somewhere in between. That's how the images are stored and that is how they are fed into the neural network and for training. Once the network is trained, when you provide a new image, it will be able to identify within a


06:46:19
certain error of course. And for this we will use one of the simpler neural network configurations called softmax. And for simplicity what we will do is we will flatten these pixels. So instead of taking them in a two-dimensional arrangement, we just flatten them out. So for example, it starts from here. It is a 28x 28. So there are 784 pixels. So pixel number one starts here. It goes all the way up to 28. Then 29 starts here and goes up to 56 and so on. And the pixel number 784 is here. So we take all these pixels, flatten them


06:46:59
out and feed them like one single line into our neural network. And this is a what is known as a softmax layer. What it does is once it is trained it will be able to identify what digit this is. So there are in this output layer there are 10 neurons each signifying a digit and at any given point of time when you feed an image only one of these 10 neurons gets activated. So for example, if this is trained properly and if you feed a number nine like this, then this particular neuron gets activated. So you


06:47:42
get an output from this neuron. Let me just use uh a pen or a laser to show you here. Okay. So you're feeding a number nine. Let's say this has been trained. And now if you're feeding a number nine, this will get activated. Now let's say you feed one to the trained network then this neuron will get activated. If you feed two this neuron will get activated and so on. I hope you get the idea. So this is one type of a neural network or an activation function known as softmax layer. So that's what we will be using


06:48:18
here. This is one of the simpler ones for quick and easy understanding. So this is how the code would look. We will go into our lab environment in the cloud and uh we will show you there directly but very quickly this is how the code looks and uh let me run you through briefly here and then we will go into the Jupyter notebook where the actual code is and we will run that as well. So as a first step first of all we are using Python here and that's why the syntax of the language is Python and the


06:48:51
first step is to import the TensorFlow library. So and we do this by using this line of code saying import tensorflow as TF. TF is just for convenience. So you can name give any name and once you do this TF is TensorFlow is available as an object in the name of TF and then you can run its uh methods and accesses its attributes and so on and so forth. And mnest database is actually an integral part of tensorflow. And that's again another reason why we as a first step we always use this example mnest database


06:49:25
example. So you just simply import mnest database as well using this line of code and you slightly modify this so that the labels are in this format what is known as one hot true which means that the label information is stored like an array and uh let me just uh use pen to show what exactly it is. So when you do this one hot true what happens is each label is stored in the form of an array of 10 digits and let's say the number is uh 8. Okay. So in this case all the remaining values there will be a bunch


06:50:08
of zeros. So this is like array at position zero. This is at position one position two and so on and so forth. Let's say this is position 7. Then this is position 8 that will be one because our input is 8 and again position 9 will be zero. Okay. So one hot encoding this one hot encoding true will kind of load the data in such a way that the labels are in such a way that only one of the digits has a value of one and that indicates. So based on which digit is one we know what is the label. So in


06:50:46
this case the eighth position is one. And therefore we know this sample data the value is 8. Similarly if you have a two here let's say then the labeled information will be somewhat like this. So you have your labels. So you have this as zero. The zeroth position the first position is also zero. The second position is one because this indicates number two. And then you have third as zero and so on. Okay. So that is the significance of this one hot true. All right. And then we can check how the


06:51:20
data is uh looking by displaying the the data. And as I mentioned earlier, this is pretty much in the form of digital form like numbers. So all these are like pixel values. So you will not really see an image in this format. But there is a way to visualize that image. I will show you in a bit. And uh this tells you how many images are there in each set. So the training there are 55,000 images. In training and in the test set there are 10,000 and then validation there are 5,000. So altogether there are 70,000


06:51:53
images. All right. So let's uh move on and we can view the actual image by uh using the mattplot clip library. And this is how you can view this is the code for viewing the images. And you can view them in color or you can view them in grayscale. So the cap is what tells in what way we want to view it. And what are the maximum values and the minimum values of the pixel values. So these are the max and minimum values. So of the pixel values. So maximum is one because this is a scaled value. So one means it


06:52:33
is uh white and uh zero means it is black and in between is it can be anywhere in between black and white. And the way to train the model there is a certain way in which you write your TensorFlow code and um the first step is to create some placeholders and then you create a model. In this case, we will use the softmax model, one of the simplest ones. And um placeholders are primarily to get the data from outside into the neural network. So this is a very common mechanism that is used. And uh then of course you will have


06:53:08
variables which are your you remember these are your weights and biases. So for in our case there are 10 neurons and uh each neuron actually has 784 because each neuron takes all the inputs. So if we go back to our slide here actually every neuron takes all the 784 inputs right this is the first neuron it has it receives all the 784 this is the second neuron this also receives all the 7. So each of these inputs needs to be multiplied with a weight and that's what we are talking about here. So these are this is a a


06:53:46
matrix of 784 values for each of the neurons and uh so it is like a 10x 784 matrix because there are 10 neurons and uh similarly there are biases. Now remember I mentioned bias is only one per neuron. So it is not one per input unlike the weights. So therefore there are only 10 biases because there are only 10 neurons in this case. So that is what we are creating a variable for biases. So this is uh something little new in tensorflow you will see unlike our regular programming languages where everything


06:54:25
is a variable here the variables can be of three different types. You have placeholders which are primarily used for feeding data. You have variables which can change during the course of computation. And then a third type which is not shown here are constants. So these are like fixed numbers. All right. So in a regular programming language you may have everything as variables or at the most variables and constants. But in TensorFlow you have three different types placeholders, variables and constants. And then you create what is


06:54:57
known as a graph. So TensorFlow programming consists of graphs and tensors as I mentioned earlier. So this can be considered ultimately as a tensor and then the graph tells how to execute the whole implementation. So that the execution is stored in the form of a graph and in this case what we are doing is we are doing a multiplication. TF you remember this TF was created as a TensorFlow object here. One more level one more. So TF is available here. Now, TensorFlow has what is known as a matrix multiplication or matal function. So


06:55:34
that is what is being used here in this case. So we are using the matrix multiplication of TensorFlow so that you multiply your input values X with W. Right? This is what we were doing. XW plus B. You're just adding B. And this is in very similar to one of the earlier slides where we saw sigma xi wi. So that's what we are doing here. Matrix multiplication is multiplying all the input values with the corresponding weights and then adding the bias. So that is the graph we created. And then


06:56:08
we need to define what is our loss function and what is our optimizer. So in this case we again use the tensorflow's APIs. So tf.n NN softmax cross entropy with logits is the uh API that we will use and reduce mean is what is like the mechanism whereby which says that you reduce the error and optimizer for doing deduction of the error. What optimizer are we using? So we are using gradient descent optimizer. We discussed about this in couple of slides uh earlier. And for that you need to specify the learning rate. You remember


06:56:50
we saw that there was a a slide somewhat like this and then you define what should be the learning rate. How fast you need to come down. That is the learning rate and this again needs to be tested and tried and to find out the optimum level of this learning rate. It shouldn't be very high in which case it will not converge or shouldn't be very low because it will in that case it will take very long. So you define the optimizer and then you call the method minimize for that optimizer and that


06:57:21
will kickstart the training process and so far we've been creating the graph and in order to actually execute that graph we create what is known as a session and then we run that session and once the training is completed we specify how many times how many iterations we want it to run. So for example in this case we are saying thousand steps. So that is a exit strategy in a way. So you specify the exit condition. So a training will run for thousand iterations. And once that is done we can then evaluate the


06:57:52
model using some of the techniques shown here. So let us get into the code quickly and see how it works. So this is our cloud environment. Now you can install TensorFlow on your local machine as well. I'm showing this demo on our existing cloud but you can also install TensorFlow on your local machine and uh there is a separate video on how to set up your TensorFlow environment. You can watch that if you want to install your local environment or you can go for other any cloud service like for example


06:58:30
Google cloud Amazon or cloud labs any of these you can use and u run and try the code okay so it has got started we will log in all right so this is our deep learning tutorial uh code and uh this is our TensorFlow environment and uh so let's get started. The first we have seen a little bit of a code walk through uh in the slides as well. Now you will see the actual code in action. So the first thing we need to do is import tensorflow and then we will import the data and we need to adjust the data in such a way that


06:59:19
the one hot is encoding is set to true one hot encoding right as I explained earlier. So in this case the label values will be shown appropriately. And if we just check what is the type of the data. So you can see that this is a uh data sets Python data sets. And if we check the number of images the way it looks. So this is how it looks. It is an array of type float 32. Similarly, the number if you want to see what is the number of training images, there are 55,000. Then there are test images 10,000 and


06:59:58
then validation images 5,000. Now let's take a quick look at the data itself visualization. So we will use um mattplot lip for this. And um if we take a look at the shape now shape gives us like the dimension of the tensors or or or the arrays if you will. So in this case the training data set if we sees the size of the training data set using the method shape it says there are 55,000 and 55,000 by 784. So remember this 784 is nothing but the 28x 28 28 into 28. So that is equal to 784. So that's what it is uh showing. Now we can


07:00:40
take just uh one image and just see what is the the first image and see what is the shape. So again size obviously it is only 784. Similarly you can look at the image itself the data of the first image itself. So this is how it it shows. So large part of it will probably be zeros because as you can imagine in the image only certain areas are written rest is black. So that's why you will mostly see zeros either it is black or white but then there are these values are so the values are actually they are scaled. So


07:01:16
the values are between zero and one. Okay. So this is what you're seeing. So certain locations there are some values and then other locations there are zeros. So that is how the data is stored and loaded. If we want to actually see what is the value of the handwritten image. Uh if you want to view it, this is how you view it. So you create like do this reshape and um mattplot lib has this um feature to show you these images. So we will actually use the function called um im show. And then if


07:01:54
you pass this parameters appropriately, you will be able to see the different images. Now I can change the values in this position. So which image we are looking at, right? So we can say if I want to see what is there in maybe 5,000, right? So 5,000 has three. Similarly, you can just say five. What is in five? Five has eight. What is in 50 again? H. So basically, by the way, if you're wondering uh how I'm executing this code, shift enter. In case you're not familiar with Jupyter notebooks,


07:02:35
shift enter is how you execute each cell, individual cell. And if you want to execute the entire program, you can go here and say run all. So that is how this code gets executed. And um here again we can check what is the maximum value and what is the minimum value of this pixel values. As I mentioned this is it is scaled. So therefore it is between the values lie between 1 and zero. Now this is where we create our model. The first thing is to create the required placeholders and variables and that's what we are doing here as we have


07:03:13
seen in the slides. So we create one placeholder and we create two variables which is for the weights and biases. These two variables are actually matrices. So each variable has 784x 10 actual values. Okay. So one for this 10 is for each neuron. There are 10 neurons and 784 is for the pixel values inputs that are given which is 28 into 28. And the biases as I mentioned one for each neuron. So there will be 10 biases. They are stored in a variable by the name B. And this is the graph which is basically


07:03:54
the multiplication of these matrix multiplication of X into W. And then the bias is added for each of the neurons. And the whole idea is to minimize the error. So let me just execute. I think this code is executed. Then we define what is our the y-value is basically the label value. So this is another placeholder. We had x as one placeholder and y true as a second placeholder. And this will have values in the form of uh 10digit 10digit uh arrays. And uh since we said one hot encoded the position


07:04:36
which has a one value indicates what is the label for that particular number. All right. Then we have cross entropy which is nothing but the loss loss function and we have the optimizer. We have chosen gradient descent as our optimizer. Then the training process itself. So the training process is nothing but to minimize the cross entropy which is again nothing but the loss function. So we define all of this in the form of a graph. So the up to here remember what we have done is we have not exactly executed any tensorflow


07:05:16
code till now we are just preparing the graph the execution plan. That's how the TensorFlow code works. So the whole structure and format of this code will be completely different from how we normally do programming. So even with people with programming experience may find this a little difficult to understand it and it needs quite a bit of practice. So you may want to view this uh video also maybe a couple of times to understand this flow because the way tensorflow programming is done is slightly different from the normal


07:05:55
programming. Some of you who let's say have done uh maybe spark programming to some extent will be able to easily understand this. uh but even in spark the the programming the code itself is pretty straightforward behind the scenes the execution happens slightly differently but in tensorflow even the code has to be written in a completely different way so the code doesn't get executed uh in the same way as you have written so that that's something you need to understand and a little bit of


07:06:26
practice is needed for this so so far what we have done up to here is creating the variables and feeding the variables and um or rather not feeding but setting up the variables and uh the graph that's all defining maybe the uh what kind of a network you want to use for example we want to use a softmax and so on so you have created the variables how to load the data loaded the data viewed the data and prepared everything but you have not yet executed anything in TensorFlow now the next step is the execution in


07:07:02
TensorFlow. So the first step for doing any execution in TensorFlow is to initialize the variables. So anytime you have any variables defined in your code, you have to run this piece of code always. So you need to basically create what is known as a a node for initializing. So this is a node. You still are not yet executing anything here. You just created a node for the initialization. So let us go ahead and create that. And here onwards is where you will actually execute your code uh in TensorFlow. And in order to execute


07:07:39
the code, what you will need is a session. TensorFlow session. So tf session will give you a session. And there are a couple of different ways in which you can do this. But one of the most common methods of doing this is with what is known as a with loop. So you have with TF dot session as says and with a colon here and this is like a block starting of the block and these indentations tell how far this block goes and this session is valid till this block gets executed. So that is the purpose of creating this width block.


07:08:22
This is known as a width block. So with tf do session as ces you say ces do.run in it. Now ces.run will execute a node that is specified here. So for example here we are saying ces.run. Ces is basically an instance of the session right. So here we are saying tf dot session. So an instance of the session gets created and we are calling that sess and then we run a node within that one of the nodes in the graph. So one of the nodes here is in it. So we say run that particular node and that is when the initialization of the variables


07:09:05
happens. Now what this does is if you have any variables in your code in our case we have w is a variable and b is a variable. So any variables that we created you have to run this code you have to run the initialization of these variables otherwise you will get an error. Okay. So that is the that's what this is doing. Then we within this width block we specify a for loop and we are saying we want the system to iterate for thousand steps and perform the training. That's what this for loop does. run


07:09:44
training for thousand iterations. And what it is doing basically is it is fetching the data or these images. Remember there are about 50,000 images but it cannot get all the images in one shot because it will take up a lot of memory and performance issues will be there. So this is a very common way of performing deep learning training. You always do in batches. So we have maybe 50,000 images but you always do it in batches of 100 or maybe 500 depending on the size of your system and so on and so forth. So in this case


07:10:21
we are saying okay get me 100 uh images at a time and get me only the training images. Remember we use only the training data for training purpose and then we use test data for test purpose. You must be familiar with machine learning. So you must be aware of this. But in case you are not in machine learning also not this is not specific to deep learning but in machine learning in general you have what is known as training data set and test data set. Your available data typically you will be splitting into two parts and using


07:10:54
the training data set for training purpose. And then to see how well the model has been trained, you use the test data set to check or test the validity or the accuracy of the model. So that's what we are doing here. And you observe here that we are actually calling an MNEST function here. So we are saying MNEST train next batch. Right? So this is the advantage of using MNEST database because they have provided some very nice helper functions which are readily available. Otherwise this activity


07:11:26
itself we would have had to write a piece of code to fetch this data in batches that itself is a a lengthy exercise. So we can avoid all that if we are using MNEST database and that's why we use this for the initial learning phase. Okay. So when we say fetch what it will do is it will fetch the images into X and the labels into Y and then you use this batch of 100 images and you run the training. So ces.run basically what we are doing here is we are running the training mechanism which is nothing


07:12:02
but it passes this through the neural network passes the images through the neural network finds out what is the output and if the output obviously initially it will be wrong. So all that feedback is given back to the neural network and thereby all the W's and B's get updated till it reaches thousand iterations. In this case the exit criteria is th00and but you can also specify probably accuracy rate or something like that for the as an exit criteria. So here it is it it just says that okay this particular image was


07:12:39
wrongly predicted so you need to update your weights and biases. That's the feedback given to each neuron and that is run for thousand iterations and typically by the end of this thousand iterations the model would have learned to recognize these handwritten images. Obviously it will not be 100% accurate. Okay. So once that is done after so this happens for thousand iterations. Once that is done, you then test the accuracy of these models by using the test data set. Right? So this is what we are


07:13:16
trying to do here. The code may appear a little complicated because if you're seeing this for the first time, you need to understand uh the various methods of TensorFlow and so on. But it is basically comparing the output with what has been what is actually there. That's all it is doing. So you have your test data and uh you're trying to find out what is the actual value and what is the predicted value and seeing whether they are equal or not. TF doe equal right and how many of them are correct and so on


07:13:44
and so forth and based on that the accuracy is uh calculated as well. So this is the accuracy and uh that is what we are trying to see how accurate the model is in predicting these uh numbers or these digits. Okay. So let us run this. This entire thing is in one cell. So we will have to just run it in one shot. It may take a little while. Let us see. And uh not bad. So it has finished the thousand iterations. And what we see here as an output is the accuracy. So we see that the accuracy of this model is


07:14:23
around 91%. Okay. Now which is pretty good for such a short exercise within such a short time we got 90% accuracy. However, in real life this is probably not sufficient. So there are other ways in to increase the accuracy. We will see probably in some of the later tutorials how to improve this accuracy, how to change maybe the hyperparameters like number of neurons or number of layers and so on and so forth and uh so that this accuracy can be increased beyond 90%. Welcome to the RNN tutorial. That's


07:15:01
the recurrent neural network. So we talk about a feed forward neural network. In a feed forward neural network, information flows only in the forward direction from the input nodes through the hidden layers, if any, and to the output nodes. There are no cycles or loops in the network. And so you can see here we have our input layer. I was talking about how it just goes straight forward into the hidden layers. So each one of those connects and then connects to the next hidden layer, connects to the output layer. And of course, we have


07:15:25
a nice simplified version where it has a predicted output. And they refer to the input as X a lot of times and the output as Y. decisions are based on current input, no memory about the past, no future scope. Why recurrent neural network? Issues in feed forward neural network. So one of the biggest issues is because it doesn't have a scope of memory or time, a feed forward neural network doesn't know how to handle sequential data. Uh it only considers only the current input. So if you have a


07:15:54
series of things and because three points back affects what's happening now and what your output affects what's happening. That's very important. So whatever I put as an output is going to affect the next one. Um a feed forward doesn't look at any of that. It just looks at this is what's coming in and it cannot memorize previous inputs. So it doesn't have that list of inputs coming in. Solution to feed forward neural network. You'll see here where it says recurrent neural network and we have our


07:16:17
X on the bottom going to H going to Y. That's your feed forward. Uh but right in the middle it has a value C. So there's a whole another process. So it's memorizing what's going on in the hidden layers. And the hidden layers as they produce data feed into the next one. So your hidden layer might have an output that goes off to Y. Uh but that output goes back into the next prediction coming in. What this does is this allows it to handle sequential data. It considers the current input and also the


07:16:45
previously received inputs. And if we're going to look at general drawings and um solutions, we should also look at applications of the RNN. Image captioning. RNN is used to caption an image by analyzing the activities present in it. A dog catching a ball in midair. Uh that's very tough. I mean, you know, we have a lot of stuff that analyzes images of a dog and they of a ball, but it's able to add one more feature in there that's actually catching the ball in midair. Time series


07:17:13
prediction. Any time series problem like predicting the prices of stocks in a particular month can be solved using RNN. And we'll dive into that in our use case and actually take a look at some stock. One of the things you should know about analyzing stock today is that it is very difficult and if you're analyzing the whole stock, the stock market at the New York Stock Exchange in the US produces somewhere in the neighborhood if you count all the individual trades and fluctuations by the second um it's like 3 terabytes a


07:17:42
day of data. So, we're only going to look at one stock. Just analyzing one stock is really tricky in here will give you a little jump on that. So, that's exciting, but don't expect to get rich off of it immediately. Another application of the RNN is natural language processing. Text mining and sentiment analysis can be carried out using RNN for natural language processing. And you can see right here the term natural language processing when you stream those three words together is very different than I if I


07:18:08
said processing language naturally. So the time series is very important when we're analyzing sentiments. It can change the whole value of a sentence just by switching the words around. or if you're just counting the words, you might get one sentiment where if you actually look at the order they're in, you get a completely different sentiment. When it rains, look for rainbows. When it's dark, look for stars. Both of these are positive sentiments and they're based upon the order of which the sentence is going in.


07:18:34
Machine translation. Given an input in one language, RNN can be used to translate the input into a different languages as output. I myself am very linguistically challenged. But if you study languages and you're good with languages, you know right away that if you're speaking English, you would say big cat. And if you're speaking Spanish, you would say cat big. So that translation is really important to get the right order to get uh there's all kinds of parts of speech that are important to know by the order of the


07:19:03
words. Here this person is speaking in English and getting translated. And you can see here a person is speaking in English and this little diagram. I guess that's denoted by the flags. I have a flag. I own it. No. Um, but they're speaking in English and it's getting translated into Chinese, Italian, French, German, and Spanish languages. Some of the tools coming out are just so cool. So, somebody like myself who's very linguistically challenged, I can now travel into worlds I would never


07:19:31
think of because I can have something translate my English back and forth readily and I'm not stuck with a communication gap. So let's dive into what is a recurrent neural network. Recurrent neural network works on the principle of saving the output of a layer and feeding this back to the input in order to predict the output of the layer. Sounds a little confusing. When we start breaking it down, it'll make more sense. And usually we have a propagation forward neural network with the input layers, the hidden layers, the


07:19:58
output layer. With the recurrent neural network, we turn that on its side. So here it is. And now our X comes up from the bottom into the hidden layers into Y. Okay. And they usually draw it very simplified. X to H with C is a loop. A to Y where A, B, and C are the perimeters. A lot of times you'll see this kind of drawing in here. Digging closer and closer into the H and how it works. Going from left to right, you'll see that the C goes in and then the X goes in. So the X is going upward bound


07:20:27
and C is going to the right. A is going out and C is also going out. That's where it gets a little confusing. So here we have x in uh c in and then we have y out and c out and c is based on ht minus one. So our value is based on the y and the h value are connected to each other. They're not necessarily the same value because h can be its own thing. And usually we draw this or we represent it as a function h of t equals a function of c where h of t minus one that's the last h output and x of t


07:21:00
going in. So it's the last output of H combined with the new input of X. Uh where HT is the new state. FC is a function with the parameter C. That's a common way of denoting it. Uh HT minus one is the old state coming out and then X of T is an input vector at time of step T. Well, we need to cover types of recurrent neural networks. And so the first one is the most common one which is a one one single output. one to one neural network is usually known as a vanilla neural network used for regular


07:21:32
machine learning problems. Why? Because vanilla is usually considered kind of a just a real basic flavor but because it's a very basic a lot of times they'll call it the vanilla neural network uh which is not the common term but it is you know like kind of a slang term people will know what you're talking about usually if you say that. Then we run one to many. So you have a single input and you might have a uh multiple outputs. In this case uh image captioning as we looked at earlier where


07:21:56
we have not just looking at it as a dog but a dog catching a ball in the air. And then you have many to one network takes in a sequence of inputs. Examples sentiment analysis where a given sentence can be classified as expressing positive or negative sentiments. And we looked at that as we were discussing if it rains look for a rainbow. So positive sentiment where rain might be a negative sentiment if you were just adding up the words in there. And then of course if you're going to do a one one mini to one


07:22:24
one to many there's many to many networks takes in a sequence of inputs and generates a sequence of outputs. Example machine translation. So we have a lengthy sentence coming in in English and then going out in all the different languages. Uh you know just a wonderful tool very complicated set of computations. You know, if you're a translator, you realize just how difficult it is to translate into different languages. One of the biggest things you need to understand when we're working with this neural network is


07:22:50
what's called the vanishing gradient problem. While training an RNN, your slope can be either too small or very large. And this makes training difficult. When the slope is too small, the problem is known as vanishing gradient. And you'll see here they have a nice uh image, lots of information through time. So if you're pushing not enough information forward, that information is lost and then when you go to train it, you start losing the third word in the sentence or something like that or it doesn't quite follow the full


07:23:19
logic of what you're working on. Exploding gradient problem. Oh, this is one that runs into everybody when you're working with this particular neural network. When the slope tends to grow exponentially instead of decaying, this problem is called exploding gradient. issues in gradient problem. Long training time, poor performance, bad accuracy, and I'll add one more in there. Uh, your computer, if you're on a a lower-end computer testing out a model, will lock up and give you the


07:23:46
memory error. Explaining gradient problem. Consider the following two examples to understand what should be the next word in the sequence. The person who took my bike and blank, a thief, the students who got into engineering with blank from Asia. And you can see in here we have our x value going in. We have the previous value going forward. And then you back propagate the error like you do with any neural network. And as we're looking for that missing word, maybe we'll have the person took my bike and blank was a


07:24:16
thief. And the student who got into engineering with a blank were from Asia. Consider the following example. The person who took the bike. So we'll go back to the person who took the bike was blank a thief. In order to understand what would be the next word in the sequence, the RNN must memorize the previous context, whether the subject was singular noun or a plural noun. So, was a thief is singular. The student who got into engineering, well, in order to understand what would be the next word


07:24:42
in the sequence, the RNN must memorize the previous context whether the subject was singular noun or a plural noun. And so, you can see here the students who got into engineering with blank were from Asia. It might be sometimes difficult for the error to back propagate to the beginning of the sequence to predict what should be the output. So when you run into the gradient problem, we need a solution. The solution to the gradient problem. First, we're going to look at exploding gradient where we have three different


07:25:10
solutions depending on what's going on. One is identity initialization. So the first thing we want to do is see if we can find a way to minimize the identities coming in instead of having it identify everything just the important information we're looking at. Next is to truncate the back propagation. So instead of having uh whatever information it's sending to the next series, we can truncate what it's sending, we can lower that particular uh set of layers, make those smaller. And


07:25:39
finally is a gradient clipping. So when we're training it, we can clip what that gradient looks like and narrow the training model that we're using. When you have a vanishing gradient, the option problem, uh we can take a look at weight initialization. Very similar to the identity, but we're going to add more weights in there so it can identify different aspects of what's coming in better. Choosing the right activation function, that's huge. So we might be activating based on one thing and we


07:26:05
need to limit that. We haven't talked too much about activation functions, so we'll look at that just minimally. Uh there's a lot of choices out there. And then finally, there's long shortterm memory networks, the LSTMs, and we can make adjustments to that. So just like we can clip the gradient as it comes out, we can also um expand on that. We can increase the memory network, the size of it, so it handles more information. And one of the most common problems in today's uh setup is what


07:26:33
they call longterm dependencies. Suppose we try to predict the last word in the text. The clouds are in the and you probably said sky. Here we do not need any further context. It's pretty clear that the last word is going to be sky. Suppose we try to predict the last word in the text. I have been staying in Spain for the last 10 years. I can speak fluent. Maybe you said Portuguese or French. No, you probably said Spanish. The word we predict will depend on the previous few words in context. Here we


07:27:00
need the context of Spain to predict the last word in the text. It's possible that the gap between the relevant information and the point where it is needed to become very large. LSTMs help us solve this problem. So the LSTMs are a special kind of recurrent neural network capable of learning long-term dependencies. Remembering information for long periods of time is their default behavior. All recurrent neural networks have the form of a chain of repeating modules of neural network connections. In standard RNNs, this


07:27:33
repeating module will have a very simple structure such as a single tangent H layer. LSTMs's also have a chain-like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four interacting layers communicating in a very special way. LSTMs are a special kind of recurrent neural network capable of learning long-term dependencies, remembering information for long periods of time as their default behavior. LSTMS's also have a chain-like


07:28:02
structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four interacting layers communicating in a very special way. As you can see, the deeper we dig into this, the more complicated the graphs get in here. I want you to note that you have x at t minus one coming in. You have x of t coming in and you have x at t + one. And you have h of t minus one and h of t coming in and h of t + one going out. And of course uh on the other side is


07:28:31
the output a um in the middle we have our tangent h but it occurs in two different places. So not only when we're computing the x of t + one are we getting the tangent h from x of t but we're also getting that value coming in from the x of t minus one. So the short of it is as you look at these layers not only does it does the propagate through the first layer goes into the second layer back into itself but it's also going into the third layer. So now we're kind of stacking those up and this can


07:29:01
get very complicated as you grow that in size. It also grows in memory too and in the amount of resources it takes. Uh but it's a very powerful tool to help us address the problem of complicated long sequential information coming in like we were just looking at in the sentence. And when we're looking at our long shortterm memory network uh there's three steps of processing processing in the LSTMs that we look at. The first one is we want to forget irrelevant parts of the previous state. You know, a lot of


07:29:30
times like you know is, as in, a unless we're trying to look at whether it's a plural noun or not, they don't really play a huge part in the language. So, we want to get rid of them. Then, selectively update cell state values. So, we only want to update the cell state values that reflect what we're working on. And finally, we want to put only output certain parts of the cell state. So, whatever is coming out, we want to limit what's going out, too. And let's dig a little deeper into this.


07:29:56
Let's just see what this really looks like. Uh so step one decides how much of the past it should remember. First step in the LSTM is to decide which information to be omitted in from the cell in that particular time step. It is decided by the sigmoid function. It looks at the previous state h of t minus one and the current input x of t and computes the function. So you can see over here we have a function of t equals the sigmoid function of the weight of f the h at t minus one and then x of t


07:30:27
plus of course you have a bias in there with any of our neural networks. So we have a bias function. So f of t equals forget gate decides which information to delete that is not important from the previous time step. Considering an STM is fed with the following inputs from the previous and present time step. Alice is good in physics. John on the other hand is good in chemistry. So previous output, John plays football well. He told me yesterday over the phone that he had served as a captain of his college football team. That's our


07:30:56
current input. So as we look at this, the first step is the forget gate realizes there might be a change in context after encounting the first full stop. Compares with the current input sentence of x. So we're looking at that full stop and then compares it with the input of the new sentence. The next sentence talks about John. So the information on Alice is deleted. Okay, that's important to know. So we have this input coming in. And if we're going to continue on with John, then that's


07:31:23
going to be the primary information we're looking at. The position of the subject is vacated and is assigned to John. And so in this one, we've seen that we've weeded out a whole bunch of information and we're only passing information on John since that's now the new topic. So step two is then to decide how much should this unit add to the current state. In the second layer, there are two parts. One is a sigmoid function and the other is a tangent h. In the sigmoid function, it decides


07:31:49
which values to let through zero or one. Tangent h function gives the weightage to the values which are passed deciding their level of importance minus1 to 1. And you can see the two formulas that come up. Uh the i of t equals the sigmoid of the weight of i h of t minus one x of t plus the bias of i. And the C of T equals the tangent of H of the weight of C of H of T minus one X of T plus the bias of C. So our I of T equals the input gate determines which information to let through based on its


07:32:20
significance in the current time step. If this seems a little complicated, don't worry because a lot of the programming is already done when we get to the case study. Understanding though that this is part of the program is important when you're trying to figure out these what to set your settings at. You should also note when you're looking at this, it should have some semblance to your forward propagation neural networks where we have a value assigned to a weight plus a bias. Very important


07:32:46
steps in any of the neural network layers, whether we're propagating into them the information from one to the next or we're just doing a straightforward neural network propagation. Let's take a quick look at this what it looks like from the human standpoint. Um, as I step out in my suit again, consider the current input at XT. John plays football. Well, he told me yesterday over the phone that he had served as a captain of his college football team. That's our input. Input gate analysis the important information.


07:33:14
John plays football and he was a captain of his college team is important. He told me over the phone yesterday is less important. Hence, it is forgotten. This process of adding some new information can be done via the input gate. Now, this example is as a human form, and we'll look at training this stuff in just a minute. Uh, but as a human being, if I wanted to get this information from a conversation, maybe it's a Google voice listening in on you or something like that. Um, how do we weed out the


07:33:41
information that he was talking to me on the phone yesterday? Well, I don't want to memorize that he talked to me on the phone yesterday. Or maybe that is important, but in this case, it's not. I want to know that he was the captain of the football team. I want to know that he served. I want to know that John plays football and he was the captain of the college football team. Those are the two things that I want to take away as a human being. Again, we measure a lot of this from the human viewpoint and that's


07:34:04
also how we try to train them so we can understand these neural networks. Finally, we get to step three. Decides what part of the current cell state makes it to the output. The third step is to decide what will be our output. First, we run a sigmoid layer which decides what parts of the cell state make it to the output. Then we put the cell state through the tangent h to push the values to be between minus1 and one and multiply it by the output of the sigmoid gate. So when we talk about the output of t, we set that equal to the


07:34:33
sigmoid of the weight of zero of the h of t minus one back one step in time by the x of t plus of course the bias. The h of t equals the out of t times the tangent of the tangent h of c of t. So our o equals the output gate allows the passed in information to impact the output in the current time step. Let's consider the example to predicting the next word in the sentence. John played tremendously well against the opponent and one for his team. For his contributions, brave blank was awarded


07:35:05
player of the match. There could be a lot of choices for the empty space. Current input. Brave is an adjective. Adjectives describe a noun. John could be the best output after brave. Thumbs up for John. Awarded player of the match. And if you were to pull just the nouns out of the sentence, team doesn't look right because that's not really the subject we're talking about. Contributions, uh, you know, brave contributions or brave team, brave player, brave match. Um, so you look at this and you can start to train this


07:35:35
these this neural network. So it starts looking at and goes, "Oh no, John is what we're talking about. So brave is an adjective. Jon's going to be the best output." and we give John a big thumbs up. And then of course we jump into my favorite part, the case study. Use case implementation of LSTM. Let's predict the prices of stocks using the LSTM network based on the stock price data between 2012 2016. We're going to try to predict the stock prices of 2017. And this will be a narrow set of data. We're


07:36:07
not going to do the whole stock market. It turns out that the New York Stock Exchange generates roughly three terabytes of data per day. That's all the different trades up and down of all the different stocks going on. And each individual one uh second to second or nancond to nancond. Uh but we're going to limit that to just some very basic fundamental information. So don't think you're going to get rich off this today. But at least you can give an eye you can give a step forward in how to start


07:36:33
processing something like stock prices. a very valid use for machine learning in today's markets. Use case implementation of LSTM. Let's dive in. We're going to import our libraries. We're going to import the training set and uh get the scaling going. Um now, if you watch any of our other tutorials, a lot of these pieces should start to look very familiar. It's very similar setup. Uh but let's take a look at that. And um just a reminder, we're going to be using Anaconda, the Jupiter notebook. So here


07:37:04
I have my Anaconda navigator. When we go under environments, I've actually set up a Kass Python 3.6. I'm in Python 3.6. And u nice thing about Anaconda, especially the newer version. I remember a year ago messing with Anaconda in different versions of Python and different environments. Um Anaconda now has a nice interface. Um and I have this installed both on a Ubuntu Linux machine and on Windows, so it works fine on there. You can go in here and open a terminal window. And then in here, once


07:37:33
you're in the terminal window, this is where you're going to start uh installing uh using pip to install your different modules and everything. Now, we've already pre-installed them, so we don't need to do that in here. Uh but if you don't have them installed in your particular environment, you'll need to do that. And of course, you don't need to use the Anaconda or the Jupiter. You can use whatever favorite Python IDE you like. I'm just a big fan of this because


07:37:54
it keeps all my stuff separate. You can see on this machine I have specifically installed one for K crossass since we're going to be working with KAS under TensorFlow. We go back to home. I've gone up here to application and that's the environment I've loaded on here and then we'll click on the launch Jupyter notebook. Now I've already in my Jupyter notebook um have set up a lot of stuff so that we're ready to go. Kind of like uh Martha Stewarts in the old cooking shows. We want to make sure we have all


07:38:20
our tools for you so you're not waiting for them to load. And uh if we go up here to where it says new, you can see where you can um create a new Python 3. That's what we did here underneath the setup. So it already has all the modules installed on it. And I'm actually renamed this. So if you go under file, you can rename it. We I'm calling it RNN stock. And let's just take a look at start diving into the code. Let's get into the exciting part. Now we've looked at the tool. And of course, you might be


07:38:45
using a different tool, which is fine. Uh let's start putting that code in there and seeing what those imports and uploading everything looks like. Now, first half is kind of boring when we hit the run button because we're going to be importing numpy as np that's uh uh the number python which is your numpy array and the mattplot library. So, we're going to do some plotting at the end and our pandas for our data set. Our pandas is pd. And when I hit run, uh it really doesn't do anything except for load


07:39:11
those modules. Just a quick note, let me just do a quick uh draw here. Whoops. Shift alt. There we go. You'll notice when we're doing this setup, if I was to divide this up, oops, I'm going to actually um let's overlap these. Here we go. Uh this first part that we're going to do is our data prep. A lot of prepping involved. Um in fact, depending on what your system is, since we're using KAS, I put an overlap here. Uh but you'll find that almost maybe even half of the code we do


07:39:49
is all about the data prep. And the reason I overlap this with uh KAS, let me just put that down because that's what we're working in. Uh is because KASS has like their own preset stuff. So it's already pre-built in which is really nice. So there's a couple steps a lot of times that are in the KAS setup. Uh we'll take a look at that to see what comes up in our code as we go through and look at stock. And then the last part is to evaluate. And if you're working with um shareholders or uh you


07:40:17
know classroom whatever it is you're working with uh the evaluate is the next biggest piece. Um so the actual code here cross is a little bit more but when you're working with uh some of the other packages you might have like three lines that might be it. All your stuff is in your pre-processing in your data. Since KASS has is is cutting edge and you load the individual layers, you'll see that there's a few more lines here and cross is a little bit more robust. And then you spend a lot of times uh like I said


07:40:44
with the evaluate. You want to have something you present to everybody else to say, "Hey, this is what I did. This is what it looks like." So let's go through those steps. This is like a kind of a just general overview and let's just take a look and see what the next set of code looks like. And in here we have a data set train and it's going to be read using the PD or pandas read CSV and it's a Google stockpric train.csv. And so under this we have training set equals data set


07:41:12
train.loation and we've kind of sorted out part of that. So what's going on here? Let's just take a look at that. Let's let's look at the actual file and see what's going on there. Now, if we look at this, uh, ignore all the extra files on this. Um, I already have a train and a test set where it's sorted out. This is important to notice because a lot of times we do that as part of the pre-processing of the data. We take 20% of the data out so we can test it and then we train the rest of it. That's


07:41:38
what we use to create our neural network. That way, we can find out how good it is. Uh, but let's go ahead and just take a look and see what that looks like as far as the file itself. And I went ahead and just opened this up in a basic word pad text editor just so we can take a look at it. Certainly you can open up in Excel or any other kind of spreadsheet. Um and we note that this is a commaepparated variables. We have a date uh open, high, low, close, volume. This is the standard stuff that we


07:42:04
import into our stock and we're the most basic set of information you can look at in stock. It's all free to download. Um in this case we downloaded it from uh Google. That's why we call it the Google stock price. Um and this specifically is Google. This is the Google stock values from uh as you can see here we started off at 1320. So when we look at this first setup up here uh we have a data set train equals pd_csv. And if you noticed on the original frame uh let me just go back there. They had it set to home Ubuntu


07:42:38
downloads Google stock price train. I went ahead and changed that because we're in the same file where I'm running the code. So, I've saved this particular Python code and I don't need to go through any special paths or have the full path on there. And then, of course, we want to take out um certain values in here. And you're going to notice that we're using um our data set and we're now in pandas. Uh so, pandas basically it looks like a spreadsheet. Um, and in this case, we're going to do I location,


07:43:09
which is going to get specific locations. The first value is going to show us that we're pulling all the rows in the data. And the second one is we're only going to look at columns one and two. And if you remember here from our data, as we switch back on over columns, we always start with zero, which is the date. And we're going to be looking at open and high, which would be one and two. We'll just label that right there so you can see. Now, when you go back and do this, you certainly can extrapolate and


07:43:41
do this on all the columns. Um, but for the example, let's just limit a little bit here so that we can focus on just some key aspects of stock. And then we'll go up here and run the code. And, uh, again, I said the first half is very boring. Whenever we hit the run button, it doesn't do anything because we're still just loading the data and setting it up. Now that we've loaded our data, we want to go ahead and scale it. We want to do what they call feature scaling. And in here, we're going to pull it up from the


07:44:11
sklearn or the SK kit pre-processing import minm max scaler. And when you look at this, you got to remember that um biases in our data, we want to get rid of that. So if you have something that's like a really high value, um let's just draw a quick graph. And I have something here like the maybe the stock has a value one stock has a value of 100 and another stock has a value of five. Um you start to get a bias between different stocks. And so when we do this we go ahead and say okay 100 is going to


07:44:43
be the max and five is going to be the min and then everything else goes and then we change this and we just squish it down. I like the word squish. So it's between one and zero. So 100 equals 1 or 1 equals 100 and 0 equals 5. And you can just multiply. It's usually just a simple multiplication. We're using uh multiplication. So it's going to be uh minus5 and then 100 divided or 95 divided by one. So or whatever value is is divided by 95. And uh once we've actually created our scale, we've


07:45:17
tolding it's going to be from 0 to one. We want to take our training set and we're going to create a training set scaled. And we're going to use our scaler SC and we're going to fit we're going to fit and transform the training set. Uh so we can now use the SC this this particular object. We'll use it later on our testing set because remember we have to also scale that when we go to test our uh model and see how it works. And we'll go ahead and click on the run again. Uh it's not going to


07:45:43
have any output yet because we're just setting up all the variables. Okay. Okay, so we pasted the data in here and we're going to create the data structure with the 60 time steps and output. First note, we're running 60 time steps and that is where this value here also comes in. So the first thing we do is we create our Xrain and Y train variables and we set them to an empty Python array. Very important to remember what kind of array we're in and what we're working with. And then we're


07:46:12
going to come in here. We're going to go for I in range 60 to 1258. There's our 60 60 time steps. And the reason we want to do this is as we're adding the data in there, there's nothing below the 60. So if we're going to use 60 time steps, uh we have to start at 60 because it includes everything underneath of it. Otherwise, you'll get a pointer error. And then we're going to take our X train and we're going to append training set scaled. This is a scaled value between


07:46:39
zero and one. And then as I is equal to 60, this value is going to be um 60 - 60 is zero. So this actually is 0 to I. So it's going to be 0 to 60, 1 to 61. Let me just circle this part right here. 1 to 61. Uh 2 to 62. And so on and so on. And if you remember, I said 0 to 60. That's incorrect because it does not count. Remember, it starts at zero. So this is a count of 60. So it's actually 59. Important to remember that as we're looking at this. And then the second part of this that we're looking at. So


07:47:14
if you remember correctly, here we go. we go from uh 0 to 59 of i and then we have a comma a zero right here and so finally we're just going to look at the open value. Now I know we did put it in there for one to two um if you remember correctly it doesn't count the second one so it's just the open value we're looking at just open um and then finally we have ytrain.append appended training set I to 0 and if you remember correctly I to or I comma 0 if you remember correctly this is 0 to 59 so there's 60


07:47:47
values in it uh so when we do I down here this is number 60 so we're going to do this is we're creating an array and we have 0 to 59 and over here we have number 60 which is going into the y train it's being appended on there and then this just goes all the way up. So this is down here is uh 0 to 59 and we'll call it 60 since that's the value over here and it goes all the way up to 12 58. That's where this value here comes in. That's the length of the data we're loading. So


07:48:23
we've loaded two arrays. We loaded one array that has uh which is filled with arrays from 0 to 59. And we loaded one array which is just the value. And what we're looking at, you want to think about this as a time sequence. Uh here's my open open open open. What's the next one in the series? So we're looking at the Google stock and each time it opens we want to know what the next one uh 0 through 59 what's 60 1 through 60 what's 61 2 through 62 what's 62 and so on and


07:48:52
so on going up. And then once we've loaded those in our for loop we go ahead and take xra and yra equals nparray x-train. npray ytrain. We're just converting this back into a numpy array. That way, we can use all the cool tools that we get with numpy array, including reshaping. So, if we take a look and see what's going on here, we're going to take our x train. We're going to reshape it. Wow. What the heck does reshape mean? Uh, that means we have an array, if you remember correctly, um, so many numbers


07:49:23
by 60. That's how wide it is. And so, we're when you when you do x-ra.shape, shape that gets one of the shapes and you get um x-ray.shape of one gets the other shape. And we're just making sure the data is formatted correctly. And so you use this to pull the fact that it's 60 by um in this case, where's that value? 60 by 1199. 1258 minus 60 1199. And we're making sure that that is shaped correctly. So the data is grouped into uh 1199 by 60 different arrays. And then the one on the end just means at the end


07:50:02
because this when you're dealing with shapes and numpy they look at this as layers and so the end layer needs to be one value. That's like the leaf of a tree where this is the branch and then it branches out some more um and then you get the leaf. np.resshape comes from and using the existing shapes to form it. We'll go ahead and run this piece of code. Again, there's no real output. And then we'll import our different cross modules that we need. So from cross models, we're going to import the


07:50:32
sequential model. We're dealing with sequential data. We have our dense layers. We have actually three layers. We're going to bring in our dense, our LSTM, which is what we're focusing on, and our dropout. And we'll discuss these three layers more in just a moment. But you do need the with the LSTM, you do need the dropout, and then the final layer will be the dents. But let's go ahead and run this and that'll bring our modules and you'll see we get an error on here. And if you read it closer, it's


07:50:57
not actually an error, it's a warning. What does this warning mean? These things come up all the time when you're working with such cutting edge modules are completely being updated all the time. We're not going to worry too much about the warning. All it's saying is that the H5PY module, which is part of KAS, is going to be updated at some point. And uh if you're running new stuff on KAS and you start updating your KAS system, you better make sure that your H5 PI is updated, too. Otherwise, you're going to


07:51:24
have an error later on. And you can actually just run an update on the H5 PI now if you wanted to. Not a big deal. We're not going to worry about that today. And I said we were going to jump in and start looking at what those layers mean. I meant that. And uh we're going to start off with initializing the RNN. And then we'll start adding those layers in. And you'll see that we have the LSTM and then the dropout. LSTM then dropout. LSTM then dropout. What the heck is that doing? So let let's explore


07:51:52
that. We'll start by initializing the RNN regressor equals sequential because we're using the sequential model. And we'll run that and load that up. And then we're going to start adding our LSTM layer and some dropout regularization. And right there should be the Q dropout regularization. And if we go back here and remember our exploding gradient, well, that's what we're talking about. The uh dropout drops out unnecessary data so we're not just shifting huge amounts of data


07:52:19
through um the network. So, and so we go in here. Let's just go ahead and uh add this in. I'll go ahead and run this. And we had three of them. So, let me go ahead and put all three of them in. And then we can go back over them. There's the second one. And let's put one more in. Let's put that in. And we'll go and put uh two more in. I meant to put I said one more in, but it's actually two more in. And then let's add one more after that. And as you can see, each time I run these, they don't actually


07:52:43
have an output. So let's take a closer look and see what's going on here. So we're going to add our first LSTM layer in here. We're going to have units 50. The units is the positive integer and it's the dimensionality of the output space. This is what's going out into the next layer. So we might have 60 coming in, but we have 50 going out. We have a return sequence because it is a sequence data. So we want to keep that true. And then you have to tell it what shape it's


07:53:08
in. Well, we already know the shape by just going in here and looking at X train shape. So input shape equals the X train shape of one comma one. Makes it really easy. You don't have to remember all the numbers that put in 60 or whatever else is in there. You just let it tell the regressor what model to use. And so we follow our STM with a dropout layer. Now understanding the dropout layer is kind of exciting because one of the things that happens is we can overtrain our network. That means that


07:53:36
our neural network will memorize such specific data that it has trouble predicting anything that's not in that specific realm. To fix for that, each time we run through the training mode, we're going to take 0 2 or 20% of our neurons, they just turn them off. So, we're only going to train on the other ones and it's going to be random. That way, each time we pass through this, we don't overtrain. These nodes come back in in the next training cycle. We randomly pick a different 20. And


07:54:03
finally, I see a big difference as we go from the first to the second and third and fourth. The first thing is we don't have to input the shape because the shapes already the output units is 50 here. This auto the next step automatically knows this layer is putting out 50. And because it's the next layer, it automatically sets that and says, oh, 50 is coming out from our last layer that's coming out, you know, goes into the regressor. And of course, we have our dropout and that's what's


07:54:27
coming into this one. And so on and so on. And so the next three layers, we don't have to let it know what the shape is. It automatically understands that. And we're going to keep the units the same. We're still going to do 50 units. It's still a sequence coming through. 50 units and a sequence. Now, the next piece of code is what brings it all together. Let's go ahead and take a look at that. And we come in here, we put the output layer, the dense layer. And if you remember up here, we had the three


07:54:52
layers. We had uh LSTM, dropout, and dense. uh dense just says we're going to bring this all down into one output. Instead of putting out a sequence, we just know want to know the answer at this point. And let's go ahead and run that. And so in here, you notice all we're doing is setting things up one step at a time. So far, we've brought in our uh way up here, we brought in our data. We brought in our different modules. We formatted the data for training it. We set it up. You know, we


07:55:18
have our YX train and our Y train. We have our source of data and the answers we're we know so far that we're going to put in there. We've reshaped that. We've come in and built our Kass. We've imported our different layers. And we have in here, if you look, we have what uh five total layers. Now, KAS is a little different than a lot of other systems because a lot of other systems put this all in one line and do it automatic, but they don't give you the options of how those layers interface


07:55:45
and they don't give you the options of how the data comes in. KAS is cutting edge for this reason. So even though there's a lot of extra steps in building the model, this has a huge impact on the output and what we can do with this these new models from KAS. So we brought in our dense, we have our full model put together, our regressor. So we need to go ahead and compile it and then we're going to go ahead and fit the data. We're going to compile the pieces so they all come together and then we're


07:56:11
going to run our training data on there and actually recreate our regressor so it's ready to be used. So let's go ahead and compile that. And I can go ahead and run that. And uh if you've been looking at any of our other tutorials on neural networks, you'll see we're going to use the optimizer atom. Atom is optimized for big data. There's a couple other optimizers out there uh beyond the scope of this tutorial, but certainly Adam will work pretty good for this. And loss equals mean squared value. So when we're


07:56:37
training it, this is what we want to base the loss on. How bad is our error? We're going to use the mean squared value for our error and the atom optimizer for its differential equations. You don't have to know the math behind them, but certainly it helps to know what they're doing and where they fit into the bigger models. And then finally, we're going to do our fit. Fitting the RNN to the training set. We have the regressor.fit, X-rain, Y train, epics, and batch size. So, we know where


07:57:02
this is. This is our data coming in for the X-ray. Our Y train is the answer we're looking for of our data, our sequential input. Epics is how many times we're going to go over the whole data set. We created a whole data set of X-ray. So this is each each of those rows which includes a time sequence of 60 and batch size. Another one of those things where KASS really shines is if you were pulling this say from a large file instead of trying to load it all into RAM, it can now pick smaller


07:57:30
batches up and load those indirectly. We're not worried about pulling them off a file today because this isn't big enough to uh cause the computer too much of a problem to run. Not too straining on the resources. But as we run this, you can imagine what would happen if I was doing a lot more than just one column in one set of stock. In this case, Google stock. Imagine if I was doing this across all the stocks and I had instead of just the open, I had open, close, high, low, and you can actually find yourself with about 13


07:57:59
different variables times 60 because there's a time sequence. suddenly you find yourself with a gig of memory you're loading into your RAM which will just completely, you know, if it's just if you're not on multiple computers or cluster, you're going to start running into resource problems. But for this, we don't have to worry about that. So, let's go ahead and run this. And this will actually take a little bit on my computer cuz it's an older laptop. And give it a second to kick in there. There


07:58:22
we go. All right. So, we have epic. So, this is going to tell me it's running the first run through all the data. And as it's going through, it's batching them in 32 pieces. So 32 lines each time and there's 1198. I think I said 1199 earlier, but it's 1198. I was off by one. And each one of these is 13 seconds. So you can imagine this is roughly 20 to 30 minutes run time on this computer. Like I said, it's an older laptop running at uh 0.9 GHz on a dual processor. And that's fine. What


07:58:50
we'll do is I'll go ahead and stop, go get a drink of coffee, and come back and let's see what happens at the end and where this takes us. And like any good cooking show. I've kind of gotten my latte. I also had some other stuff running in the background. So you'll see these numbers jumped up to like 19 seconds, 15 seconds, which you can scroll through and you can see we've run it through 100 steps or 100 epics. So the question is, what does all this mean? One of the first things you'll


07:59:13
notice is that our loss can is over here. It kind of stopped at 0.0014, but you can see it kind of goes down until we hit about 0.014 times in a row. So we guessed our epic pretty close since our loss has remained the same on there. So to find out what we're looking at, we're going to go ahead and load up our test data, the test data that we didn't process yet, and real stock price data set test location. This is the same thing we did when we prepped the data in the first place. So let's go ahead and


07:59:40
go through this code. And we can see we've labeled it part three, making the predictions and visualizing the results. So the first thing we need to do is go ahead and read the data in from our test CSV. You see I've changed the path on it for my computer. And uh then we'll call it the real stock price. And again, we're doing just the one column here and the values from IO location. So it's all the rows and just the values from these that one location. That's the open stock


08:00:07
open. And let's go ahead and run that. So that's loaded in there. And then let's go ahead and uh create. We have our inputs. We're going to create inputs here. And this should all look familiar. This is the same thing we did before. We're going to take our data set total. We're going to do a little pandas concat from the data set train. Now remember the end of the data set train is part of the data going in. And let's just visualize that just a little bit. Here's


08:00:31
our train data. Let me just put tr for train. And it went up to this value here. But each one of these values generated a bunch of columns. It was 60 across. And this value here equals this one. And this value here equals this one. And this value here equals this one. And so we need these top 60 to go into our new data. So to find out what we're looking at, we're going to go ahead and load up our test data, the test data that we didn't process yet, and a real stock price data set test I


08:01:01
location. This is the same thing we did when we prepped the data in the first place. So let's go ahead and go through this code. And we can see we've labeled it part three, making the predictions and visualizing the results. So the first thing we need to do is go ahead and read the data in from our test CSV. You see I've changed the path on it for my computer. And uh then we'll call it the real stock price. And again we're doing just the one column here and the values from IO location. So it's all the


08:01:29
rows and just the values from these that one location. That's the open stock open. And let's go ahead and run that. So that's loaded in there. And then let's go ahead and uh create. We have our inputs. We're going to create inputs here. And this should all look familiar. This is the same thing we did before. We're going to take our data set total. We're going to do a little pandas concat from the data set train. Now remember the end of the data set train is part of the data going in. And let's just


08:01:55
visualize that just a little bit. Here's our train data. Let me just put tr for train. And it went up to this value here. But each one of these values generated a bunch of columns. So it was 60 across. And this value here equals this one. And this value here equals this one. And this value here equals this one. And so we need these top 60 to go into our new data cuz that's part of the next data or it's actually the top 59. So that's what this first setup is over here is we're going in. We're doing


08:02:24
the real stock price and we're going to just take the data set test and we're going to load that in. And then the real stock price is our data test location. So we're just looking at that first uh column, the open price. And then our data set total, we're going to take pandas and we're going to concat. And we're going to take our data set train for the open and our data set test open. And this is one way you can reference these columns. We've referenced them a couple different ways. We've referenced


08:02:49
them up here with the one two, but we know it's labeled as a panda set as open. So pandas is great that way. Lots of versatility there. And we'll go ahead and go back up here and run this. There we go. And uh you'll notice this is the same as what we did before. We have our open data set. We pended our two different or concatenated our two data sets together. We have our inputs equals data set total length data set total minus length of data set minus test minus 60 values. So we're going to run


08:03:14
this over all of them and you'll see why this works because normally when you're running your test set versus your training set, you run them completely separate. But when we graph this, you'll see that we're just going to be we'll be looking at the part that uh we didn't train it with to see how well it graphs. And we have our inputs equals inputs reshapes or reshaping like we did before. We're transforming our inputs. So if you remember from the transform between 0ero and one and uh finally we


08:03:39
want to go ahead and take our x test and we're going to create that x test and for i in range 60 to 80. So here's our x test and we're appending our inputs i to 60 which remember is 0 to 59 and i comm, zero on the other side. So it's just the first column which is our open column. And uh once again we take our x test we convert it to a numpy array. We do the same reshape we did before and uh then we get down to the final two lines. And here we have something new right here on these last two lines. Let me just


08:04:07
highlight those or or mark them. Predicted stock price equals regressor.predictsx test. So we're predicting all the stock including both the training and the testing model here. And then we want to take this prediction and we want to inverse the transform. So remember we put them between zero and one. Well, that's not going to mean very much to me to look at a at a float number between 0 and one. I want the dollar amounts. I want to know what the cash value is. And we'll go ahead and


08:04:31
run this. And you'll see it runs much quicker than the training. That's what's so wonderful about these neural networks. Once you put them together, it takes just a second to run the same neural network that took us what a half hour to train ahead and plot the data. We're going to plot what we think it's going to be and we're going to plot it against the real data. What what the Google stock actually did. So let's go ahead and take a look at that in code. And let's uh pull this code up. So we


08:04:54
have our plt. That's our uh oh, if you remember from the very beginning, let me just go back up to the top. We have our mattplot library.pipplot. pi plot as plt. That's where that comes in. And we come down here. We're going to plot. Let me get my drawing thing out again. We're gonna go ahead and plt is basically kind of like an object. It's one of the things that always threw me when I'm doing graphs in Python because I always think you have to create an object and then it loads that class in there. Well,


08:05:20
in this case, plt is like a canvas you're putting stuff on. So, if you've done HTML 5, you'll have the canvas object. This is the canvas. So, we're going to plot the real stock price. That's what it actually is. And we're going to give that color red. So, it's going to be in bright red. We're going to label it real Google stock price. And then we're going to do our predicted stock. And we're going to do it in blue. And it's going to be labeled predicted.


08:05:44
And we'll give it a title because it's always nice to give a title to your uh graph, especially if you're going to present this to somebody, you know, to your uh shareholders in the office. And uh the x label is going to be time because it's a time series. And we didn't actually put the actual date and times on here, but that's fine. We just know they're incremented by time. And then, of course, the Y label is the actual stock price. plt.leend tells us to build the legend on here so that the


08:06:08
color red and and real Google stock price show up on there. And then the plot shows us that actual graph. So, let's go ahead and run this and see what that looks like. And you can see here we have a nice graph. And let's talk just a little bit about this graph before we wrap it up. Here's our legend I was telling you about. That's why we have the legend to showed the prices. We have our title and everything. And you'll notice on the bottom we have a time sequence. We didn't put the actual time


08:06:32
in here. Now, we could have we could have gone ahead and um plotted the X since we know what the the dates are and plotted this to dates, but we also know it's only the last piece of data that we're looking at. So, last piece of data, which ends somewhere probably around here on the graph. I think it's like about 20% of the data, probably less than that. We have the Google price. And the Google price has this little up jump and then down. And you'll see that the actual Google instead of uh


08:06:59
a turndown here just didn't go up as high and didn't low go uh down. So our prediction has the same pattern, but the overall value is pretty far off as far as um stock. But then again, we're only looking at one column. We're only looking at the open price. We're not looking at how many volumes were traded. Like I was pointing out earlier when we talk about stock, just right off the bat, there's six columns. There's open, high, low, close, volume. Then there's whether uh I mean volume shares. Then


08:07:27
there's the adjusted open, adjusted high, adjusted low, adjusted close. They have a special formula to predict exactly what it would really be worth based on the value of the stock. And then from there, there's all kinds of other stuff you can put in here. So, we're only looking at one small aspect, the opening price of the stock. And as you can see here, we did a pretty good job. This curve follows the curve pretty well. It has like uh you know little jumps on it, bends, they don't quite


08:07:53
match up. So this bend here does not quite match up with that bend there, but it's pretty darn close. We have the basic shape of it and the prediction isn't too far off. And you can imagine that as we add more data in and look at different aspects in the specific domain of stock, we should be able to get a better representation each time we drill in deeper. Of course, this took a half hour for my program, my computer to train. So you can imagine that if I was running it across all those different


08:08:19
variables, it might take a little bit longer to train the data. Not so good for doing a quick tutorial like this. So we're going to dive right into what is KAS. We'll also uh go all the way through this into a couple of tutorials because that's where you really learn a lot is when you roll up your sleeves. So we talk about what is KAS. Kass is a highlevel deep learning API written in Python for easy implement implementation of neural networks. uses deep learning frameworks such as TensorFlow, PyTorch,


08:08:49
etc. as backend to make computation faster and this is really nice because as a uh programmer there is so much stuff out there and it's evolving so fast it can get confusing and having some kind of highlevel order in there where you can actually view it and easily program these different neural networks uh is really powerful. It's really powerful to to um uh have something out really quick and also be able to start testing your models and seeing where you're going. So, KAS works by using complex deep learning


08:09:21
frameworks such as TensorFlow, PyTorch, um ML played, etc. as a backend for fast computation while providing a userfriendly and easy tolearn front end. And you can see here we have the cross API uh specifications and under that you'd have like TF Karass for TensorFlow, Thano Karass and so on. And then you have your TensorFlow workflow that this is all sitting on top of. And this is like I said it organizes everything. The heavy lifting is still done by TensorFlow or whatever you know underlying package you put in there. And


08:09:56
this is really nice because you don't have to um dig as deeply into the heavy end stuff while still having a very robust package. You can get up and running rather quickly and it doesn't distract from the processing time because all the heavy lifting is done by packages like TensorFlow. This is the organization on top of it. So the working principle of Kass uh the working principle of KASS is Kass uses computational graphs to express and evaluate mathematical expressions. You can see here we put them in blue.


08:10:29
They have the expression u expressing complex problems as a combination of simple mathematical operators uh where we have like the percentage or in this case in Python that's usually your uh left your remainder or multiplication uh you might have the operator of x uh to the power of.3 and it uses useful for calculating derivatives by using uh back propagation. So if we're doing with neural networks, we send the error back up to figure out how to change it. Uh this makes it really easy to do that


08:11:01
without really having not banging your head and having to handwrite everything. It's easier to implement distributed computation and for solving complex problems uh specify input and outputs and make sure all nodes are connected. And so this is really nice as you come in through is that um as your layers are going in there, you can get some very complicated uh different setups nowadays, which we'll look at in just a second. And this just makes it really easy to start spinning this stuff up and


08:11:30
trying out the different models. So we look at cross models. Uh cross model, we have a sequential model. Sequential model is a linear stack of layers where the previous layer leads into the next layer. And this, if you've done anything else, even like the sklearn with their neural networks and propagation and any of these setups, this should look familiar. You should have your input layer. It goes into your layer 1, layer two, and then to the output layer. And it's useful for simple classifier decoder


08:12:00
models. And you can see down here we have the model equals a cross sequential. And this is the actual code. You can see how easy it is. Uh we have a layer that's dense. your layer one as an activation. Uh they're using the RLU in this particular example. And then you have your name layer one, layer dense RLU, name layer two, and so forth. Uh and they just feed right into each other. So it's really easy just to stack them as you can see here. And it automatically takes care of everything


08:12:26
else for you. And then there's a functional model. And this is really where things are at. This is new. Make sure you update your KAS or you'll find yourself running this um doing the functional model. you'll run into an error code because this is a fairly new release and it uses multi-input and multi-output model. The complex model which forks into two or more branches and you can see here we have our image inputs equals your cross input shape equals 32x 32x3. You have your uh dense layers


08:12:58
dense 64 activation ru similar to what you already saw before. Uh but if you look at the graph on the right, it's going to be a lot easier to see what's going on. You have two different inputs. Uh and one way you could think of this is maybe one of those is a small image and one of those is a full-sized image. And that feedback goes into you might feed both of them into one node because it's looking for one thing and then only into one node for the other one. And so you can start to get kind of an idea that there's a


08:13:30
lot of use for this kind of split and this kind of setup uh where you have multiple information coming in, but the information's very different even though it overlaps. And you don't want it to send it through the same neural network. Um and they're finding that this trains faster and is also has a better result depending on how you split the data up and you and how you fork the models coming down. And so in here we do have the two complex uh models coming in. Uh we have our image inputs which is a 32x 32x3


08:14:01
your three channels or four if you're having an alpha channel. Uh you have your dense your layers dense is 64 activation using the RLU very common. Uh X equals dense inputs X layers dense X64 activation equals RLU X outputs equals layers dense 10 X model equals cross model inputs equals inputs outputs equals outputs name equals minced model. Uh so we add a little name on there. And again this is this kind of split here. This is setting us up to um have the input go into different areas. So if


08:14:36
you're already looking at caress, you probably already have this answer. What are neural networks? Uh but it's always good to get on the same page and for those people who don't fully understand neural networks to dive into them a little bit or do a quick overview. Neural networks are deep learning algorithms modeled after the human brain. They use multiple neurons which are mathematical operations to break down and solve complex math problems. And so just like the neuron, one neuron fires in and it fires out to


08:15:05
all these other neurons or nodes as we call them. And eventually they all come down to your output layer. And you can see here we have the really standard graph input layer, a hidden layer, and an output layer. One of the biggest parts of any data processing is your data prep-processing. Uh so we always have to touch base on that with a neural network like many of these models. They're kind of uh when you first start using them they're like a black box. You put your data in, you train it and you test it and see how


08:15:39
good it was and you have to pre-process that data because bad data in is uh bad outputs. So in data prep-processing we will create our own data examples set with KAS. The data consists of a clinical trial conducted on 2100 patients ranging from ages 13 to 100 with a the patients under 65 and the other half over 65 years of age. We want to find the possibility of a patient experiencing side effects due to their age. And you can think of this in today's world with uh COVID uh what's going to happen on there. And we're


08:16:14
going to go ahead and do an example of that in our uh live hands-on. Like I said, most of this you really need to have hands-on to understand. So, let's go ahead and bring up our Anaconda and uh open that up and open up a Jupyter notebook for doing the Python code in. Now, if you're not familiar with those, you can use pretty much any of your uh setups. I just like those for doing demos and uh showing people, especially shareholders. It really helps because it's a nice visual. So, let me go and


08:16:42
flip over to our Anaconda. And the Anaconda has a lot of cool tools. They just added data lore and IBM Watson Studio clad into the Anaconda framework, but we'll be in the Jupyter lab or Jupyter notebook. Um, I'm going to do Jupyter notebook for this because I use the lab for like large projects with multiple pieces because it has multiple tabs where the notebook will work fine for what we're doing. And this opens up in our browser window because that's how Jupyter notebook so Jupyter notebook is


08:17:12
set to run. and we'll go under new create a new Python 3 and uh it creates an untitled Python. We'll go ahead and give this a title and we'll just call this uh KAS tutorial. And let's change that to a capital. There we go. We'll go and just rename that. And the first thing we want to go ahead and do is uh get some pre-processing tools involved. And so we need to go ahead and import some stuff for that like our numpy do some random number generation. Um I mentioned sklearn or your scik kit if you're


08:17:50
installing sklearn the sklearn stuff it's a scite kit you want to look up that should be a tool of anybody who is uh doing data science. If you if you're not if you're not familiar with the sklearn toolkit, it's huge. Uh but there's so many things in there that we always go back to. And we want to go ahead and create some train labels and train samples uh for training our data. And then just a note of what we're we're actually doing in here. Uh let me go ahead and change this. This is kind


08:18:25
of a fun thing you can do. We can change the code to markdown. And then markdown code is nice for doing examples once you've already built this. Uh our example data we're going to do experimental there we go experimental drug was tested on 2100 individuals between 13 to 100 years of age. Half the participants under 65 and 95% of participants are under 65 experience no side effects. Well 95% of participants over 65 u experience side effects. So, that's kind of where we're starting at.


08:19:01
Um, and this is just a real quick example because we're going to do another one with a little bit more uh complicated information. Uh, and so we want to go ahead and generate our setup. Uh, so we want to do for I and range and we want to go ahead and create, if you look here, we have random integers, train the labels, append. So, we're just creating some random data. Uh, let me go ahead and just run that. And so once we've created our random data and if you if I mean you can certainly ask for a copy of the code


08:19:36
from simply learn they'll send you a copy of this or you can zoom in on the video and see how we went ahead and did our train samples a pin u and we're just using this I do this kind of stuff all the time. I was running a thing on uh that had to do with errors following a bellshaped curve on uh a standard distribution error. And so what do I do? do I generate the data on a standard distribution error to see what it looks like and how my code processes it since that was the baseline I was looking for in this we're just


08:20:07
doing uh uh generating random data for our setup on here and uh we could actually go in uh print some of the data let's just do this print um we'll do train samples and we'll just do the first um five pieces of data in there to see what that looks like. And you can see the first five pieces of data in our train samples is 49, 85, 41, 68, 19. Just random numbers generated in there. That's all that is. Uh, and we generated significantly more than that. Um, let's see, 50 up here, 1,000. Yeah. So,


08:20:45
there's 1,00 here. 1,00 numbers we generated. And we could also, if we wanted to find that out, we could do a quick uh print the length of it. And So, or you could do a shape kind of thing. And if you're using numpy, although the length for this is just fine. And there we go. It's actually 2100 like we said in the demo setup in there. And then we want to go ahead and take our labels. Oh, that was our train labels. We also did samples, didn't we? Uh, so we could also print do the same


08:21:21
thing. labels. Um, and let's change this to labels and [Music] labels and run that just to double check. And sure enough, we have 2100 and they're labeled one 0 1 0 1 0. I guess that's if they have symptoms or not. One symptoms uh zero none. And so we want to go ahead and take our train labels and we'll convert it into a numpy array. And the same thing with our samples. And let's go ahead and run that. And we also shuffle. Uh this is just a neat feature you can do in uh numpy array here. Put


08:22:07
my drawing thing on which I didn't have on earlier. Um I can take the data and I can shuffle it. Uh so we have our so it's it just randomizes it. That's all that's doing. um we've already randomized it so it's kind of an overkill. It's not really necessary. But if you're doing uh a larger package where the data is coming in and a lot of times it's organized somehow and you want to randomize it just to make sure that that you know the input doesn't follow a certain pattern


08:22:37
uh that might create a bias in your model. And we go ahead and create a scaler. Uh the scaler range uh minimum max scaler feature range 0 to one. Uh then we go ahead and scale the uh scaled train samples. We're going to go ahead and fit and transform the data uh so it's nice and scaled. And that is the age. Uh so you can see up here we have 49, 85, 41. We're just moving that so it's going to be uh between zero and one. And so this is true with any of your neural networks. you really want to convert the data uh


08:23:11
to zero and one otherwise you create a bias. Uh so if you have like a 100 creates a bias versus the math behind it gets really complicated. Uh if you actually start multiplying stuff there's a lot of multiplication addition going on in there that higherend value will eventually multiply down and it will have a huge bias as to how the model fits it and then it will not fit as well. And then one of the fun things we can do in Jupyter Notebook is that if you have a variable and you're not doing


08:23:42
anything with it, it's the last one on the line, it will automatically print um and we're just going to look at the first five samples on here. And so it's going to print the first five samples. And you can see here we go uh 95 791. So everything's between zero and one. And that just shows us that we scaled it properly and it looks good. Uh it really helps a lot to do these kind of printups halfway through. Uh you never know what's going to go on there. I don't know how many times I've


08:24:15
gotten down and found out that the data sent to me that I thought was scaled was not and then I have to go back and track it down and figure it out on there. Uh so let's go ahead and create our artificial neural network. And for doing that, this is where we start diving into TensorFlow and KAS. Uh TensorFlow, if you don't know the history of TensorFlow, it helps to uh jump into we'll just use Wikipedia. Careful, don't quote Wikipedia on these things because you get in trouble. Uh but it's a good place


08:24:53
to start. Uh back in 2011, Google Brain built disbelief as a proprietary machine learning setup. TensorFlow became the open source for it. Uh so TensorFlow was a Google product and then it became uh open- sourced and now it's just become probably one of the de factos when it comes for neural networks as far as where we're at. Uh so when you see the TensorFlow setup, it it's got like a huge following. There are some other setups like a um the scikit under the skarn has their own little neural network. uh but


08:25:28
the TensorFlow is the most robust one out there right now and KAS sitting on top of it makes it a very powerful tool so we can leverage both the KAS uh easiness in which we can build a sequential setup on top of TensorFlow. And so in here we're going to go ahead and do our input of TensorFlow. Uh and then we have the rest of this is all KAS here from number two down. Uh we're going to import from TensorFlow the cross uh connection and then you have your TensorFlow cross models import sequential. It's a


08:26:04
specific kind of model. We'll look at that in just a second. If you remember from the files that means it goes from one layer to the next layer to the next layer. There's no funky splits or anything like that. Uh and then we have from TensorFlow across layers. We're going to import our activation and our dense layer. And we have our optimizer, Adam. Um, this is a big thing to be aware of how you optimize uh your data. When you first do it, Adam's as good as any. Atom is usually uh there's a number of


08:26:37
optimizer out there. There's about uh there's a couple main ones, but atom is usually assigned to bigger data. Uh it works fine. Usually the lower data does it just fine, but atom is probably the mostly used, but there are some more out there. And depending on what you're doing with your layers, your different layers might have different activations on them. And then finally down here, you'll see um our setup where we want to go ahead and use the metrics. And we're going to use the


08:27:04
TensorFlow cross metrics um for categorical cross entropy. Uh so we can see how everything performs when we're done. That's all that is. Um, a lot of times you'll see us go back and forth between TensorFlow and then Scikit has a lot of really good metrics also for measuring these things. Um, again it's the end of the, you know, at the end of the story, how good does your model do? And we'll go ahead and load all that. And then comes the fun part. Um, I actually like to spend hours messing


08:27:34
with these things. And uh, four lines of code. You're like, "Ah, you're going to spend hours on four lines of code." Um, no, we don't spend hours on four lines of code. That's not what we're talking about when I say spend hours on four lines of code. Uh, what we have here, I'm going to explain that in just a second. We have a model, and it's a sequential model. If you remember correctly, we mentioned the sequential up here where it goes from one layer to the next. And our first


08:28:00
layer is going to be your input. It's going to be uh what they call dense, which is u usually it's just dense. And then you have your input and your activation. Um, how many units are coming in? We have 16. Uh, what's the shape? What's the activation? And this is where it gets interesting. Uh, because we have in here, uh, RLU on two of these and softmax activation on one of these. There are so many different options for what these mean um, and how they function. How does the ReLU, how does the softmax function?


08:28:42
and they do a lot of different things. Um, we're not going to go into the activations in here. That is what really you spend hours doing is looking at these different activations. Um, and just some of it is just um almost like you're playing with it like an artist. You start getting a feel for like a uh inverse tangent activation or the tanh activation takes up a huge processing amount. Uh so you don't see it a lot yet. It comes up with a better solution especially when you're doing uh when


08:29:17
you're analyzing word documents and you're tokenizing the words and so you'll see this shift from one to the other because you're both trying to build a better model and if you're working on a huge data set um you'll crash the system. It'll just take too long to process. Um, and then you see things like softmax. Uh, softmax generates an interesting um, setup where a lot of these when you talk about RLU it Oops, let me do this. Uh, RLU. There we go. RLU has um a setup


08:29:51
where if it's less than zero, it's zero and then it goes up. Um, and then you might have what they call lazy uh, setup where it has a slight negative to it so that the errors can translate better. Same thing with softmax. It has a slight laziness to it so that errors translate better. All these little details make a huge different on your model. Um, so one of the really cool things about data science that I like is you build your uh what they call you build to fail. And it's an interesting


08:30:24
uh design setup. Oops, I forgot the end of my code here. The concept of build a fail is you want the model as a whole to work. So you can test your model out so that you can do uh you can get to the end and you can do your let's see where was it overshot down here. You can test your test out how the quality of your setup on there and see where did I do my tensorflow. Oh, here we go. I did it was right above me. There we go. we start doing your cross entropy and stuff like that is you need a full functional set of code so


08:31:02
that when you run it, you can then test your model out and say, "Hey, it's either this model works better than this model and this is why." Um, and then you can start swapping in these models. And so when I say spend a huge amount of time on pre-processing data, it's probably 80% of your programming time. Um, well between those two, it's like 8020. You'll spend a lot of time on the models. Once you get the model down, once you get the whole code and the flow down, uh, set depending on


08:31:31
your data, your models get more and more robust as you start experimenting with different inputs, different data streams and all kinds of things. And we can do a simple model summary here. Uh, here's our sequential, here's our layer, our output, a parameter. This is one of the nice things about KAS is you just, you can see right here, here's our sequential one model. Boom, boom, boom, boom. everything set and clear and easy to read. So once we have our model built, uh the next thing we're going to want to


08:32:02
do is we want to go ahead and uh train that model. And so the next step is of course model training. And when we come in here, this a lot of times it's just paired with the model because it's so straightforward. It's nice to print out the model setup so you can have a tracking. But here's our model. Uh the keyword in Kass is compile optimizer atom. Learning rate. Another term right there that we're just skipping right over that really becomes the meat of uh the setup is your


08:32:40
learning rate. Uh so whoops, I forgot that I had an arrow, but I'll just underline it. A lot of times the learning rate set to 0.0 uh set to 0.01 01. Uh depending on what you're doing, this learning rate um can overfit and underfit. Uh so you'd want to look up I know we have a number of tutorials out on overfitting and underfitting that are really worth reading once you get to that point and understanding. And we have our loss um sparse categorical cross entropy. So this is going to tell how far to go


08:33:14
until it stops. And then we're looking for metrics of accuracy. So we'll go ahead and run that. And now that we've compiled our model, we want to go ahead and um run it, fit it. So here's our model fit. Um we have our scale to train samples, our train labels, our validation split. Um in this case, we're going to use 10% of the data for validation. Uh batch size, another number you kind of play with. Not a huge difference as far as how it works, but it does affect how long it takes to run


08:33:51
and it can also affect the bias a little bit. Uh most of the time though, a batch size is between 10 to 100. Um depending on just how much data you're processing in there, we want to go ahead and shuffle it. Uh we're going to go through 30 epics and uh put a verbose of two. Let me just go ahead and run this. And you can see right here, here's our epic. Here's our training. Um here's our loss. Now, if you remember correctly up here, we set the loss. See, where was it? Um, compiled our


08:34:21
data. There we go. Loss. Uh, so it's looking at the sparse categorical cross entropy. This tells us that as it goes, how how how much um how how much does the um error go down? Uh is the best way to look at that. And you can see here, the lower the number, the better. It just keeps going down. and vice versa. Accuracy we want, let's see, where's my accuracy value accuracy at the end. Uh, and you can see 619 69.74. It's going up. We want the accuracy would be ideal if it made it all the way to one. But we also the loss


08:35:01
is more important because it's a balance. um you can have 100% accuracy and your model doesn't work because it's overfitted. Uh again, you want to look up overfitting and underfitting models. And we went ahead and went through uh 30 epics. It's always fun to kind of watch your code going. Um to be honest, I usually uh uh the first time I run it, I'm like, "Oh, that's cool. I get to see what it does." And after the second time of running it, I'm like, "I'd like to just not see that." And you


08:35:33
can repress those of course in your code. Uh repress the warnings and the printing. And so the next step is going to be building a test set and predicting it. Now uh so here we go. We want to go ahead and build our test set. And we have uh just like we did our training set. A lot of times you just split your your initial setup. Uh but we'll go ahead and do a separate set on here. And this is just what we did above. Uh there's no difference as far as um the randomness that we're using to


08:36:06
build this set on here. Uh the only difference is that we already uh did our scaler up here. Well, it doesn't matter because the the data is going to be across the same thing, but this should just be just transform down here instead of fit transform. Uh because you don't want to refit your data um on your testing data. There we go. And now we're just transforming it because you never want to transform the test data. Um, easy mistake to make especially on an example like this where we're not doing um, you


08:36:43
know, we're randomizing the data anyway. So, it doesn't matter too much because we're not expecting something weird. And then we want ahead and do our predictions. The whole reason we built the model is we take our model, we predict, and we're going to do here's our Xcale data batch size 10 verbose. And now we have our predictions in here. And we could go ahead and do a um oh, we'll print predictions. And then I guess I could just put down predictions and five. So we can look at the first five of the


08:37:17
predictions. And what we have here is we have our age and uh the prediction on this age versus what is what we think it's going to be what what we think is going to going to have uh symptoms or not. And the first thing we notice is that's hard to read because we really want a yes no answer. Uh so we'll go ahead and just uh round off the predictions using the argax um the numpy argax uh for prediction. And so it just goes to a zero one. And if you remember this is a Jupyter notebook. So I don't


08:37:53
have to put the print. I can just put in uh rounded predictions. And we'll just do the first five. And you can see here 0 1 0 0. So that's what the predictions are that we have coming out of this um is no symptoms, symptoms, no symptoms, symptoms, no symptoms. And just as uh we were talking about at the beginning, we want to go ahead and um take a look at this. There we go. confusion matrixes for accuracy check. Uh most important part when you get down to the end of the story, how accurate is your model before


08:38:27
you go and play with the model and see if you can get a better accuracy out of it. And for this we'll go ahead and use the scikit um the sklearn metrics. Uh scikit being where that comes from. import confusion matrix uh some iteration tools and of course a nice map plot library that makes a big difference so it's always nice to u have a nice graph to look at um pictures worth a thousand words um and then we'll go ahead and do call it cm for confusion matrix y true equals test labels y predict rounded


08:39:03
predictions and we'll go ahead and load in our cm M and I'm not going to spend too much time on the plotting um going over the different plotting code. Um you can spend uh like whole we have whole tutorials on how to do your different plotting on there. Uh but we do have here is we're going to do a plot confusion matrix. There's our CM, our classes, normalized false title confusion matrix. Cap is going to be in blues. And you can see here we have uh to the nearest cap titles, all the


08:39:40
different pieces whether you put tick marks or not, the marks, the classes, the color bar. Um so a lot of different information on here as far as how we're doing the printing of the of the confusion matrix. You can also just dump the confusion matrix um into a seaborn and real quick get an output. It's worth knowing how to do all this uh when you're doing a presentation to the shareholders. You don't want to do this on the fly. You want to take the time to make it look really nice uh like our


08:40:08
guys in the back did. And uh let's go ahead and do this. Forgot to put together our CM plot labels. We'll go ahead and run that. And then we'll go ahead and call the little the definition for our mapping. And you can see here plot confusion matrix. That's our the the little script we just wrote. And we're going to dump our data into it. Um, so our confusion matrix, our classes, um, title confusion matrix. And let's just go ahead and run that. And you can see here we have our


08:40:42
basic setup. Uh, no side effects, 195 had side effects, uh, 200, no side effects that had side effects. So we predicted that 10 of them who actually had side effects. And that's pretty good. I mean, I I don't know about you, but you know, that's 5% error on this. And this is because there's 200 here. That's where I get 5% is uh divide these both by by two and you get five out of 100. Uh you can do the same kind of math up here. Not as quick on the fly because it's 15 and 195. Not an easily rounded


08:41:15
number, but you can see here where they have 15 people who predicted to have no uh with the no side effects but had side effects kind of set up on there. And these confusion matrix are so important at the end of the day. This is really where where you show uh whatever you're working on comes up and you can actually show them hey this is how good we are or not how messed up it is. So this was a uh I spent a lot of time on some of the parts uh but you can see here is really simple. uh we did the


08:41:51
random generation of data, but when we actually built the model coming up here, uh here's our model summary and we just have a the layers on here that we built with our model on this and then we went ahead and trained it and ran the prediction. Now, we can get a lot more complicated. Uh let me flip back on over here because we're going to do another uh demo. So, that was our basic introduction to it. We talked about the uh oops, here we go. Okay, so implementing a neural network with KASS. After creating our samples


08:42:19
and labels, we need to create our KASS neural network model. We will be working with a sequential model which has three layers. And this is what we did. We had our input layer, our hidden layers, and our output layers. And you can see the input layer uh coming in uh was the age factor. We had our hidden layer and then we had the output. Are you going to have symptoms or not? So, we're going to go ahead and go with something a little bit more complicated. Uh training our model is a two-step process. We first compile


08:42:47
our model and then we train it in our training data set. Uh so we have compiling. Compiling converts the code into a form of understandable by machine. We used the atom in the last example, a gradient descent algorithm to optimize a model. And then we trained our model which means it let it uh learn on training data. Uh and I actually had a little backwards there. This is what we just did is we if you remember from our code we just had oops let me go back here um here's our model that we created summarized


08:43:22
uh we come down here and we compile it. So it tells it hey we're ready to build this model and use it uh and then we train it. This is the part where we go ahead and fit our model and and put that information in here and it goes through the training on there. And of course we scaled the data which was really important to do. And then you saw we did the creating a confusion matrix with KAS. Um as we are performing classifications on our data we need a confusion matrix to check the results. A


08:43:51
confusion matrix breaks down the various misclassifications as well as correct classifications to get the accuracy. Um and so you can see here this is what we did with a true positive false positive true negative false negative. And that is what we went over. need to scroll down here on the end. We printed it out and you can see we have a nice print out of our confusion matrix uh with the true positive, false positive, false negative, true negative. And so the blue ones uh we want those to be the biggest


08:44:23
numbers because those are the better side. And then uh we have our false predictions on here uh as far as this one. So it had no side effects but we predicted let's see no side effects predicting side effects and vice versa. So here's the open a documentation and you could see the new features introduced with the chat GP4. So these are the improvements. Uh one is the updated and interactive bar graphs or pie charts that you can create. And these are the features that you could see here. You could change the


08:44:54
color. You could download it. And what you have is you could update the latest file versions directly from Google Drive. and Microsoft one drive and we have the interaction with tables and charts in an new expandable view that I showed you here that is here you can expand it in the new window and you can customize and download charts for presentations and documents moreover you can create the presentation also that we'll see in further and here we have how data analysis works in chat GBT you could directly upload the files


08:45:28
from Google drive and Microsoft one drive I will show you guys how we can do that and where this option is and we can work on tables in real time and there we have customized presentation ready charts that is you can create a presentation with all the charts based on a data provided by you and moreover a comprehensive security and privacy feature. So with that guys we'll move to chat GPT and here we have the chat GPT for version. So this is the pin section or the insert section where you can have the options to connect to


08:46:03
Google drive, connect to Microsoft one drive and you can upload it from the computer. This option was already there that is upload from computer and you can upload at least or at max the 10 files that could be around excel files or documents. So the max limit is 10 and if you have connected to Google drive I'll show you guys uh I'm not connecting you but you guys can connect it too and you could upload it from there also and there's another cool update that is ability to code directly in your chat.


08:46:40
Uh so while chatting with chat GBT I'll show you guys how we can do that and you could find some new changes that is in the layout. So this is the profile section. It used to be at the left bottom but now it's moved to the top right and making it more accessible than ever. So let's start with the data analysis part and the first thing we need is data. So you can find it on kegle or you could ask chatgypty forro to provide the data. I will show you guys. So this is the kegle website. You


08:47:14
can sign in here and click on data sets. You can find all the data sets here that would be around computer science, education, classification, computer vision or else you could move back to chat GPT and you could ask the chat GP forum modeler to generate a data and provide it in Excel format. So we will ask him we'll not ask him can you we'll just ask him provide a data set that I can use for data analysis and provide in CSV format. So you could see that it has responded that I can provide a sample


08:47:55
data set and he has started generating the data set here. So you could see that he has provided only 10 rows and he is saying that I will now generate this data set in CSV format. First he has provided the visual presentation on the screen and now it's generating the CSV format. So if you want more data like if you want 100 rows or thousand rows you could specify in the prompt and chat GP will generate that for you. So we already have the data. I will import that data. You could import it from here or else you can import it from


08:48:31
your Google drive. So we have a sales data here. We will open it. So we have the sales data here. So the first step we need to do is data cleaning. So this is the crucial step to ensure that the accuracy of our analysis is at its best. So we can do that by handling missing values. That is missing values can distort our analysis. And here chatg4 can suggest methods to impute these values such as using the mean, median or a sophisticated approach based on data patterns. And after handling the missing values, we will


08:49:05
remove duplicates and outlier detection. So we'll ask JPD clean the data if needed. So we can just write a simple prompt that would be clean the data if needed. And this is also a new feature. You can see the visual presentation of the data here that we have 100 rows here and the columns provided that is sales ID, date, product, category, quantity and price per unit and total sales. So this is also a new feature that okay uh we just had it back. We'll move back to our chat GP chat here.


08:49:59
Okay, so here we are. So you could see that Jupy has cleaned the data and he has provided that it has checked for missing values, checked for duplicates and ensured consistent formatting and he's saying okay. Okay. So now we will ask him that execute these steps and provide the clean data as chip has provided that these are the steps to clean the data and let's see so he has provided a new CSV file with the clean sales data we will download it and Ask him to use the same file only. Use this new


08:50:54
cleaned sales data CSV file for further analysis. So you could see that he has providing what analysis we can do further. But once our data is clean, the next step is visualization. So visualizations help us understand the data better by providing a graphical representation. So the first thing we will do is we will create a prompt for generating the histograms and we'll do that for the age distribution part. So we'll write a prompt that generate a histogram. Generate a histogram to visualize the distribution of customer


08:51:39
ages. to visualize the distribution of customer ages. And what I was telling you guys is this code button. If you just select the text and you would find this reply section. Just click on that and you could see that it has selected the text or what you want to get all the prompts started with chat GPD. So we'll make it cross and you could see that it has provided the histogram here and these are the new features here and we could see that he's providing a notification that interactive charts of


08:52:27
this type are not yet supported. that is histogram don't have the color change option. I will show you the color change option in the bar chart section. So these features are also new. You can download the chart from here only. And this is the expand chart. If you click on that you could see that you could expand the chart here and continue chat with chat gypy here. So this is the interactive section. So you could see that he has proed the histogram that is showing the distribution of customer ages and the


08:52:57
age range are from 18 to 70 years with the distribution visualized in 15 bins that he has created 15 bins here. And now moving to another visualization that we'll do by sales region. So before that I will open the CSV file that is provided by the chat GPT. So you guys can also see what data he has provided. So this is the clean sales data and you could see that we have columns sales ID, date, product, category, quantity, price per item, total sales region and salesperson. So now moving back to


08:53:38
chativity. So now we will create a bar chart showing total sales by region. So we'll enter this prompt that create a bar chart showing total sales by region. So what we are doing here is we are creating bar charts or histogram charts but we can do that for only two columns. If we want to create these data visualization charts we need two columns to do so. So you could see that he has provided the response and created the bar chart here. And this is the interactive section. You could see that


08:54:18
here's an option to switch to static chart. If we click on that, we can't like we are not getting any information. If we scroll on that and if I enable this option, you could see that I can visually see how many numbers this bar is indicating. And after that, we have the change color section. You can change the color of the data set provided. So we can change it to any color that is provided here or you could just write the color code here. And similarly we have other two options that is download and under is


08:54:52
the expand chart section. And if you need uh what code it has done to figure out this bar graph. So this is the code you could use any ID to do so. If you don't want the presentations or the visualizations of the bar charts here, you could use your ID and use the Python language and he will provide the code for you. Just take your data set and read it through pandas and generate the bar charts. So moving to next section that is category wise sales section. So here we will generate a pie chart showing the


08:55:33
proportion of sales for each product category. So for that we'll write a prompt generate a pie chart showing the proportion of sales for each product category. So you could see that it has started generating the pie chart and this is also an interactive section. If you click on that you would be seeing a static pie chart. And if you want to change the color you can change for any section that could be clothing, electronics, furniture or kitchen. And similarly we have the download section and the expand chart


08:56:24
section. So this is how this new chart JPD forum model is better than chat JPD 4. that you could use a more interactive pie charts. You could change the colors for that and you can just hover over these bar charts and found all the information according to them. So after this data visualization, now we'll move to statistical analysis. So this will help us uncover patterns and relationships in the data. So the first thing we'll do is correlation analysis and for that we'll write the prompt


08:56:58
analyze the correlation between age and purchase amount. So this correlation analysis help us understand the relationship between two variables. So this can indicate if older customers tend to spend more or less. So we will find out that by analyzing the data and we'll provide a promp to chat gypy that analyze the correlation between age and purchase amount. So let's see what it provides. Uh so here's the response by chity. You could see a scatter plot that shows the relationship between customer age


08:57:43
and total sales. That is with a calculated correlation coefficient of approximately 0.16. So this indicates a weak positive correlation between age and purchase amount suggesting that as customer age increases there's a slight tendency for total sales to increase as well. So you could just see the scatter plot here that if the age increases so it is not correlated to sales as you could see an empty graph here. So till 40 to 50 years of age or the 70 years of age you could find what amount they have spent here


08:58:22
that is the total sales accumulated by these ages. So now moving to sales trend. So here we will perform a time series analysis of purchase amount over the given dates. So what does this do is time series analysis allows us to examine how sales amount changes over time helping us identify trends and seasonal patterns. So for that we'll write a prompt perform a time series analysis of purchase amount over given dates. So you could see that CHD has provided us the response and here's the time series plot showing total sales


08:59:20
over the given dates and each point on the plot represents the total sales for a particular day. So through this you can find out and the businesses find out which is the seasonal part of the year and where to stock up their stocks for these kind of dates and after that you could also do customer segmentation. So what does this do is so we can use clustering here to segment customers based on age, income and purchase amount. So clustering groups customers into segments based on similarities. This is useful for


08:59:57
targeted marketing and personalized services. And after that we have the advanced usage for data analysis. Here we can draw a predictive modeling table and do the market basket analysis and perform a customer lifetime value analysis. So we will see one of those and what we'll do is we'll perform a market basket analysis and perform an association rule mining to find frequently bought together products. So the theory behind this is the association rule mining helps identify patterns of products that are often


09:00:33
purchased together aiding in inventory management and cross-selling strategies. So for that we'll write a prompt that so perform an association rule mining to find frequently bought together products. So for that we'll write a prompt here. perform an association rule mining to find frequently bought products together. So let's see for this prompt what does chat4 respond to us. Uh so you could see that he's providing a code here but we don't need a code here. We need the analysis.


09:01:20
Don't provide code. Do the market basket analysis and provide visualizations. So you could see that uh Chad Gypty has provided the response that given the limitations in this environment. So he's not able to do the market basket analysis here. So but he can help us how we can perform this in an id. So he's providing you can install the required libraries then prepare the data and here is providing the example code. So you could see there are some limitations to chat 4 also that he can't do advanced


09:02:16
data analysis. So you could use the code in your ID and do the market basket analysis there. So there are some limitations to CHP4 also. And now we will ask chat GPT can you create a presentation based on the data set and we'll provide a data set to it also. So we'll provide uh sample sales data and we'll ask him can you create a presentation or PowerPoint presentation based on this data set and only provide data visualization. graphs. So you can see that JP4 has started analyzing the data and he is


09:03:20
stating that and he will start by creating a data visualization from the provided data set and compile them into PowerPoint presentation. So you could see that charge JFT4 has provided us the response and these are all the presentations or the bar graphs that he has created and now we have downloaded the presentation here. We will open that and here's the presentation that is created by Chat GPT for Hello everyone, I am M and welcome to today's video where we will be talking about LLM benchmarks tools used to test and


09:04:08
measure how well large language models like GPT and Google Gemini performs. If you have ever wondered how AI models are evaluated, this video will explain it in simple terms. LLM benchmarks are used to check how good these models are at tasks like coding, answering questions and translating languages or summarizing text. These tests use sample data and a specific measurement to see how will the model perform. For example, the model might be tested with a few example like few short learning or none at all like


09:04:38
zero short learning to see how it handles new task. So now the question arises why are these benchmarks important? They help developers understand where a model is strong and where it needs improvement. They also make it easier to compare different models, helping people choose the best one for their needs. However, LLM benchmarks do have some limits. They don't always predict how well a model will work in real world situation and sometimes model can overfit meaning they perform well on test data but struggle


09:05:07
in practical use. We will also cover how LLM leaderboards rank different model based on their benchmark scores, giving us a clear picture of which models are performing the best. So stay tuned as we dive into how LLM benchmarks work and why they are so important for advancing AI. So without any further ado, let's get started. So what are LLM benchmarks? LLM benchmarks are standardized tools used to evaluate the performance of LA language models. They provide a structure way to test LLMs on a specific


09:05:34
task or question using sample data and predefined metrics to measure their capabilities. These benchmark assess various skills such as coding, common sense reasoning and NLP tasks like machine translation, question answering and text summarization. The importance of LLM benchmark lies in their role in advancing model development. They track the progress of an LLM offering quantitative insights into where the model performs well and where improvement is needed. This feedback is crucial for guiding the fine-tuning


09:06:02
process, allowing researchers and developers to enhance model performance. Additionally, benchmarks offers an objective comparison between different LLMs, helping developers and organization choose the best model for their needs. So, how LLM benchmarks work? LLM benchmark follow a clear and systematic process. They present a task for LLM to complete, evaluate its performance using a specific metrics and assign a score based on how well the model performs. So here is a breakdown of how this process work. The first one


09:06:33
is setup. LLM benchmark come with pre-prepared sample data including coding challenges, long documents, math problems and real world conversation. The task span various areas like common sense reasoning, problem solving, question answering, summary generation and translation. all present to the model at the start of testing. The second step is testing. The model is tested on one of the three ways. Few short. The LLM is provided with a few example before being prompted to complete a task demonstrating its


09:07:03
ability to learn from limited data. The second one is zero short. The model is asked to perform a task without any prior examples, testing its ability to understand new concept and adapt to unfamiliar scenarios. The third one is fine-tune. The model is trained on a data set similar to the one used in the benchmark aiming to enhance its performance on the specific task involved. The third step is the scoring. So after completing the task, the benchmark compares the model's output with the expected answer and generates a


09:07:31
score typically ranging from zero to 100 reflecting how accurately the LLM perform. So now let's moving forward. Let's see key metrics for benchmarking LLMs. So, Llance benchmark uses various metrics to assess performance of large language model. So, here are some commonly used metrices. The first one is accuracy of precision. Measure the percentage of correct prediction made by the model. The second one is recall also known as sensitivity. Measure the number of true positive reflecting the correct


09:08:00
prediction made by the model. The third one is F1 score. Combines both accuracy and recall into a single metric weighing them equally to address any false positive or negatives. F1 score ranging from zero to one where one indicates perfect precision and recall. The fourth one is exact match tracks the percentage of predictions that exactly match the correct answer which is especially used for the task like translation and question answering. The fifth one is perplexity goes. Here it will tell you


09:08:29
how well a model predicts the next word or token. A lower perplexity score indicates better task comprehension by the model. The sixth one is blue bilingual evaluation under study is used for evaluating machine translation by comparing engrams sequence of adjacent text element between the model's output and the human produced translation. So these quantitative metrics are often combined for more through evaluation. So in addition human evaluation introduces qualitatively factors like coherence,


09:08:58
relevance and semantic meaning provide a nuance assessment. However, human evaluation can be timeconuming and subjective making a balance between quantitative and qualitative measures important for comprehensive evaluation. So now let's moving forward see some limitation of LLM benchmarking. While LM benchmarking are valuable for assessing model performance, they have several limitation that prevents them from the fully predicting real world effectiveness. So here are some few. The first one is bounded scoring. Once a


09:09:26
model achieves the highest possible scores on the benchmark, that benchmark loses its utility and must be updated with more challenging task to remain a meaningful assessment tool. The second one is broad data set. LLM benchmark often rely on sample data from diverse subject and task. So this wide scope may not effectively evaluate a model performance in edge cases, specialized fields or specific use cases where more tailored data would be needed. The third one is finite assessment. Benchmark only


09:09:53
tests a model current skills and as LLMs evolve and a new capabilities emerge new benchmarks must be created to measure these advancement. The fourth one is overfitting. So if an LLM is trained on the same data used for benchmarking it can be lead to overfitting where the model performs well on the test data but struggles with the real task. So this result is scores that don't truly represent the model's broader capabilities. So now what are LLM leaderboards? So LLM leaderboards publish a ranking of LLMs based on the


09:10:24
variety of benchmarks. Leaderboard provide a way to keep track to the myat LLMs and compare their performance. LLM leaderboards are especially beneficial in making decision on which models to you. So here are some. So in this you can see here open AI is leading and GPD 40 second and the llama third with 405 parameter B and 3.5 is there. So this is best in multitask reasoning. What about the best in coding? So here OpenAI 01 is leading. I guess this is the Oran one and the second one is 3.5 and after that


09:10:59
in the third position there is GPT4. So this is in best encoding. So next comes fastest and most affordable models. So fastest models are Llama 8B parameter 8B parameter and the second one is LLA MA Lama 70B and the third one is 1.5 flash. This is Gemini 1 and lowest latency and here it is leading llama again in cheapest models again Llama 8B is leading and in the second number we have Gemini flash 1.5 and in third we have GPT4 mini moving forward let's see standard benchmarks between claw 3 opus


09:11:40
and GPT4 so in general they are equal in reasoning claw 3 opus is leading and in coding GP4 4 is leading in math again. GPT4 is leading in tool use. Cloud3 opus is leading and in multilingual cloud 3 OPUS is leading. Hello everyone, I am M and welcome to this video on hugging face a leading tool in NLP natural language processing. So this video is for you if you want to use advanced model to understand and generate language. So we will start by introducing what is hugging face and how it's changing the world of NLP. So


09:12:18
hugging face provides models and tools that help developers and researchers build language application easily like chair GPD. So hugging face is pioneering companies in the field of machine learning especially in NLP. It offers an extensive library called transformers which provides pre-trained models for a variety of NLP task making it easier for anyone to integrate the state-of-the-art machine learning models into their projects. So in this video I will show you three cool demos using hugging face.


09:12:50
First one is speech to text. In this we will see how we can turn spoken words into written text quickly and accurately. The second one sentiment analysis. Learn how to figure out if a piece of text is positive, negative or neutral. The third one text generation in which we will see how we can create humanlike text that makes sense to and fits the context. We will also talk about important concept like pipelines and tokenization. Pipeline make it easy to use different models for various task while tokenization breaks text into


09:13:23
smaller parts that models can understand. So by end of this video you will know how to use hugging face for your own projects. So without any further ado, let's jump into the demo part. So welcome to this demo part of this video. So here as you know we will be doing three things. First thing speech to text recognition. The second thing text generation from a particular sentence or a word and the third thing we will do sentiment analysis. Okay. So we will perform this one by one. So let's start with something called


09:13:56
speechto text recognition. Okay. using hugging face. So first I will write rename this. I will write here hugging face speech to text. Okay. Yeah. So first I will install transformers library. It is already installed on my system but again just for you guys I'm doing this okay pip install transformer. So now let's see what is transformers. So transformers is a powerful python library created by hugging face. So that allows you to download manipulate and run thousands of pre-trained open-source


09:14:45
AI model. Fine. So as you can see requirement already satisfied. it means like already installed. So these transformer models cover multiple tasks across uh you know modities like NLP, natural language processing, computer vision, audio and multimodel learning like many things. Fine. So now I will do from transformers import pipeline. Okay. So now let's run it. So now what is pipeline? Okay, so a transformer pipeline describes the flow of data from origin system to destination system. Fine. Because you


09:15:32
already know transformer means you can you know uh run or manipulate thousands of predained opensource AI model. So pipeline describes the flow of data from origin system to destination system and defines how to transform you the data along the way. Okay. So let's check the versions transform version. Okay. That long tab. Okay. Why it is coming? because I have to write this import transformers. Yeah. So we have currently 4.42.4 version of the transformer. Okay. So I'm using Google Collab. Okay. So you


09:16:34
can use either Jupyter notebook, Visual Code Studio or Collab. Okay. So now let's import libraries. import librosa. Okay. So what is librosa? So librosa is a python package for audio and music analysis. Right? Because we are doing speech to text. So we need this. So it provides various functions to quickly extract key audio features and metrics from the audio files. Okay. And this librosa can also be used to analyze and manipulate audio files in a variety of formats such as wav, mp3, m4a and like that. Okay. So next we


09:17:24
will import torch. I hope everyone know torch. So torch is nothing pytorch. It is a machine learning library based on the torch library used for applications such as computer vision and NLP and originally it was developed by the meta and now the part of Linux foundation you know umbrella let's import one more thing import so now let's import IPython Okay, it should be capital IPython dot display as display. So now you will be wondering what is this ipython display. Okay. So it is an interactive command


09:18:22
line uh for the terminal for Python and it uh you know it can provide a IPython terminal and the web-based notebook platform for Python computing and this uh IPython have you know more advanced features than the Python standard interpreter and we quickly execute a single line of Python code nothing okay for that I'm using this And the next is from transformers import wav [Music] to vectorizer for CTC and W A V2 vectorizer to tokenizer. Okay. So now what is this so WA2 vectorzer for CTC is supported by the notebook on how to


09:19:30
fine-tune a speech recognition model. Okay. And this tokenizer. So tokenizer is nothing tokenization is a conversion of a text into meaningful lexical tokens belonging to categories defined you know by a lexa program in case of NLP. So those categories include nouns, verbs, adjective, punctuation, ATC. Okay. Now and the last library let's import numpy snp. I hope everyone know numpy. What is numpy? Again numpy is a python library used for working with arrays. It als it also has a function you know for


09:20:11
working in domain like linear algebra, fia transform and matrices and many others. Fine. I hope everyone now know about these library which we have imported. Okay. So let's move forward by tokenizer to tokenizer dot from pre-trained. This is why I love this Google collab. You'll decide just some words and it will show the suggestions you know Facebook. So this is again one pre uh trained model. Okay. So we are just importing it base 960th. This is nothing the name. Okay. Then model equals


09:21:23
to [Music] WAV2 for CTC dot from retrained then Facebook WV2 vectorizer 2 then is 960 fine. Now let's run it. So now you can see. So we are loading this. Okay. They're downloading. Okay. So you can ignore this warning. So now let's load the audio file. So here I will write audio sampling rate equals to librosa dot load. Okay. So you know about librosa right? Then I have v dot m4 a. So we I have already one speech or one audio you can say. So I will play it. Don't worry. Before the final output I will you


09:22:56
know show you 16,000. Okay. So now I will write. Okay. Okay. already loaded I guess. Okay. Okay. No issues. I will load again. Okay. File is loaded. I will rename it to V. Yeah. Fine. So now it won't there won't come. Again we have V.MPP4A. Okay. So now what we have to do? Okay. Let me try it. We have V.M.mpp4 but I don't know why it is not coming. Okay, let me copy the path. And now it will run I guess it's running. Yeah. So now I will write here audio comma sampling rate. Okay. Now listen carefully. Then I


09:24:33
will write display display dot audio then path comma comma auto play to true. Okay. Hello and welcome. This is an AI voice message. I guess you heard. But again, let me play it again. Hello and welcome. This is an AI voice message. Okay. So, it is saying hello and welcome. This is an AI voice message. Fine. So now what I will do? I will input some values. Input values equals tokenizer then audio comma return tensors equals to 80. Fine. Dot input values. Okay. Then here I will run input values. Okay. Input


09:26:07
values. Okay. Yeah. So now logits will come here. So we have to now store the logits which means non-normalized predictions. Okay. So logits equals to model input values dot logits. Okay. Then here I'll write logits. Okay. It's running. Done. So now what we will do? We will store predicted ids. Then we will pass the logits to values to softmax to get the predictive value. Okay. So here I will write predicted ids equals torch dot arg max. Okay. Then logit dimension equals to minus one. Okay, then I will pass the you know


09:27:24
prediction uh to the tokenizer decode to the transcription. Okay. So here I will write transcriptions equals to tokenizer tokenizer dot decode predicted ids zero running fine. So now let's see our output transcriptions. So now you can see hello and welcome. This is an AI voice message. So now let's play it again. Hello and welcome. This is an AI voice message. Amazing right? So this is how you can use hugging face for this piece to text recognition. It's a very small code line of code as you can see. Okay. So now


09:28:26
let's move forward and do the sentiment analysis. Okay. Or text generation. So let me open new drive. So yeah. So now I will write here. I will write sentiment slash text [Music] generation hugging face. Okay. So first of all let's import the files. So first I'll write import warnings then warnings dot filter warnings okay I will write here ignore then we'll import numpy you already know what is numpy and import panda so these are some bas basic uh Python library and I hope everyone


09:29:36
knows and import mattplot li for the plots mplot lib dot piplot as plt then I will import cbond for the graphs the statistics graph okay sns then sns dot set. So now we'll import skarn model train split. So skarn is nothing scikitlearn is a probably the most useful library for machine learning python. So, so this uh library contains a lot of efficient tools for machine learning and you know statical modeling including classification you can do regression you can do clustering you can do from this


09:30:28
escal okay from skarn dot model [Music] selection Import import train test split from SQL dot matrix import f1 comma confusion matrix comma ro a cur or this code. So again we'll import tim from transformers import pipeline everyone know what is this then import torch okay now let's run it you have too many active session terminate existing to continue if you interesting using more session Okay. Wait. Yeah. Fine. Now, which one is open, bro? Terminate this. Terminate this. Terminate this.


09:32:10
No. Okay. Fine. So now let's do sentiment analysis. So uh we will explore sentiment analysis using a pre-trained transform model. Okay. The hugging face library provides you know convenient pipeline as you already know function that allows us to easily perform sentiment on the text. So now let's first import the necessary dependencies and create a sentiments pipeline using this line. So I will write here classifier equals to pipeline sentiment analysis then type classifier. Okay. So it will download the


09:33:08
pre-trained sentiment analysis pipeline. Pipeline is not defined. How? Okay. Okay. Error is there. I hope it will run now. Yeah. So now we can uh pass a single sentence or a list of sentence to the classifier and now and get the predictive sentiment labels and associate confidence score. Fine. So now just for the testing classifier. So, let's write this is a great movie. Okay, now let me run it. So now you can see here label positive. So label positive typically refers to the outcome or a class of interest that the


09:34:11
model is designed to predict. So here we are just checking the sentiment analysis model. Okay. So, this is why I wrote this. Okay. So, let's check one more. This was a great course. Then I did not understood any of it. Now let's check this. Yeah, perfect. And the score you can see the accuracy score 99%. 99% which is almost close to 100%. And which is amazing. So now you have access to a GPU. So you can also utilize this for the faster processing by specifying the device using the parameter. Okay. So


09:35:20
now what I will do first I will okay wait. Yeah fine. So now I will import data set a line tweets equals to pd dot read. PD means pandas library here we are using. Okay. Now first import will import here itself. So I have twitter dot tweets dot csv. Okay. Tweets dot CSV. Let it upload. Yeah. Done. So now let me airline tweets. Y dot head means you it will show me the top five rows. Okay. 0 1 2 3 4 5. Okay. You can see the idea line sentiment neutral positive neutral negative. Then this is this. Okay. So


09:36:39
now let's do something. Okay. So what I will do df equal df means data frame a line to its then I will write a line sentiment we have a line sentiment this column okay text I need these two columns again df do head five. So yeah, air cime neutral and this is what this text is all about. Okay, because these two are the main things text and the sentiment. So now let's make plot count plot then I will write df comma x = to a line sentiment then p validate equals to is then here I will add plt dot x


09:38:06
label airline sentiment. Okay. Then plt dot label count plt dot show. So it will draw one graph. Okay. Okay. Spelling is wrong. Yeah. So it will show neutral positive sentiment and the negative. So as you can see the negative sentences or the sentiments are the more okay. So now so we have now three classes which do not match the two class available in the hugging face pipeline. So therefore we will filter out all the rows which have been labeled as neutral. Okay. So DF equals to DF then again DF airline


09:39:16
sentiment fine was not equals to neutral. Okay. Then DF target equals to DF airline sentiment dot uh map then I will write here positive positive 1 and the negative 0 find negative will be zero. Okay. Then print number of rows comma df dot shape. Okay. So now you can see number of rows are 11,541. Okay. So now I will write predictions five it will okay predictions is not defined. Okay. So now what I will do? So here I will write text equals to TF text text and to list dot to list then here I will write


09:41:13
predictions equals to classifier text. Okay. So here write probabilities equals to predictions then score. If predictions label dot starts with starts with P means positive. Okay. else 1 minus prediction score prediction score okay then I will write for prediction and prediction why it is not running this is taking time too much time I don't know why So as you can see finally we have the output predictions values. So this depends on you know system. So system my system took almost 17 minutes 47 seconds


09:42:37
to complete. Okay. So now let's run it. Okay. Predictions is now defined. Now predictions. Yeah. So now I will write here predictions equals to np dot array. np means numpy numpy dot array then I will write one if prediction label is positive dot starts with p. Okay. else zero for prediction and prediction values. Yep. Okay. Fine. So now let's check the accuracy of our model branch accuracy. Okay. Then I will do round up the values. So here I will write np dot mean then df then the target


09:44:10
right. Yeah then I will write equals equals to predictions. Then I will write into 100 comma two. Then I will write percentage. Okay. Fine. Looks good. Yes. So as you can see our accuracy is 88.99%. Which is you know very good again. So now let's do some confusion matrix confusion matrix then TF target predictions comma normalize equals to true. Okay. Then I will plot confusion matrix. Okay. DF then target comma predictions comma normalize equals to true. Okay. Okay. My bad. My bad. My bad.


09:45:50
Let confusion matrix comma labels. Okay. So here uh I will you know plot a confusion matrix using cbond. Okay. So here I will use args which is confusion matrix np dot an array. So which is you know labels list. So I will write plt dot figure then figure size equals to 8 comma 6 that's sn dot set fault scale equals to 1.4. Okay. Then let's create the heat map fusion matrix comma and not will be true. Okay. Then I will write empty equals to g then confusion map equals to [Music] blues. Then x tick


09:47:23
labels equals to labels. Then y tick labels equals to labels. Okay. Yes. So now I will write plt dot title will be confusion matrix then plt do.x X label will be predicted values then plt dot y label will be actual values then plt dot show okay why the chart is not coming okay so I have to write plot confusion matrix then write cm then negative comma positive. Okay. Now the chart will come. Okay. Yeah. So now you can see here this is you know actual and this is a confusion matrix and the negative and the positive


09:49:08
ratio is there. Okay. So now let's print the let's check the ROC score. So I will write here print ro score then here I will write ro A score then TF target props. Okay. So 94. Okay. So first let me tell you what is this ROC AC score. So this is the area under the ROC curve. So what it does it sum up how well a model can produce relative scores to discriminate between positive and the negative instance across all the classification threshold. So with this ROC score of 940.94 which is 94% we can conclude that


09:50:14
the that a pre-trained sentiment analysis model has achieved the high level of accuracy and effectiveness. So in predicting the sentiment labels. So this indicates that the model is capable of accurately classifying text into positive or the negative sentiment categories. Okay. So now we'll do text generation. Okay. So text generation involves generating creative and coherent text based thing. So I will write here text. Okay. So text generation involves generating creative and coherent text


09:50:50
based on a given prompt or starting point. So what we will do first we will import the necessary dependencies and load the data data set of the poem. Okay. So here I am write poems. Let me add poems equals to PD read CSV. Don't worry, I will give you these files. Okay, description box below I will add. So here I will write robot frost with dot CSC. Fine. And then I will write poems dot head five or head just have to write it module pandas has okay not doc csv yeah okay it will show the top five


09:51:53
stopping by woods on a snowy evening fire and ice the aim was song collection content, year of publish. Okay. So now what we will do? We will write content equals to poems content dot drop na to to list. Okay. So to generate text we extract individual lines from the poems and use the pipeline text generation function to create a text generation pipeline. Fine. So here I will write lines equals to then I will write for poem and content. Okay. For line in poem dot split into next line. Okay. Then lines dot


09:53:17
append. Then I will add line dot write strip to the right. It will add. Okay. Fine. So now lines equals to line for line and lines. If length of line zero then show me the lines five. Okay. So now you have seen here whose woods these are I think I know. Then the next line his house is in the village though. That's the next line. He will not sing. Okay. Like this. So now let's you know import that pipeline text generation module. Okay. Gen equals to [Music] pipeline. So these are the pre-trained


09:54:28
model you already know generation. Okay. So now I will write here lines. Let's run it. Yeah. So in line zero you have we have whose words these are. I think I know. Okay. So now we can now generate text by providing a prompt and specifying the parameters such as max length and num return sequence. Okay. Why? Because we have imported this type generation module. Okay. For example, see [Music] gen. Okay, sorry. Lines zero dot max length max length equals to 20. So now it will generate the this to the maximum


09:55:34
20. Okay. Till 20 word maximum length. Okay. Check. Okay. Uh okay. Ch expression cannot contain perhaps double equals to where okay wait now it will run. So see the line was this much only whose words these are. I think I know. Whose words these are? I think I know. But here the generated text I wish to go to church because I feel like okay this is how you can do you know text generation. Now let's check for more gen 10 lines 1. Okay. Then max length equals to 30, [Music] num return sequence


09:56:36
says sequences equals to two. Okay. So our first line was this. His house is in the village though. Okay. 0 1 2 3 4 5. Okay. So here you can see see his house in the village though however you might say that the place was the same with the place this this this. So these are the generated text. Okay. And this is another you know return sequence second. So there are two one and the second. Okay. So now let me you know import text. Okay. Then creating function wrap x. I don't need this. Then return text


09:57:41
wrap dot fill x comma replace wide space wide space equals to false false comm sentence endings equals to true. Okay. So now out equals to generated lines zero to maximum length equals to 30. Then print zap out to zero. [Music] Then generated text. Okay. So now we are setting uh the T tokens to EOS tokens. Okay. So whose whose these are? I think I know. And this is the maximum 30 till 30. Okay. So now I will write here okay preview equals to okay. So now what you can do you can you know generate a prompt to


09:59:21
generate text on a specific topic like this prompt equals to transformers have a wide variety of applications application in NLB B. Okay. So this is my prompt. Okay. This I'm not importing from the data set. Okay. So I will write out equals to gen prompt comma max length equals to 100. print wrap out. Okay, here I will write anyways here I will write generated text prompt. Okay, so it is running. Let's wait. Yeah. So, okay. Somewhere is there. A full course on generative AI. If you really like this video, do hit


10:01:03
the notification bell for more videos by simply learn. Thank you for watching. Staying ahead in your career requires continuous learning and upskilling. Whether you're a student aiming to learn today's top skills or a working professional looking to advance your career, we've got you covered. Explore our impressive catalog of certification programs in cuttingedge domains, including data science, cloud computing, cyber security, AI, machine learning, or digital marketing. Designed in


10:01:36
collaboration with leading universities and top corporations and delivered by industry experts. Choose any of our programs and set yourself on the path to career success. Click the link in the description to know more.

