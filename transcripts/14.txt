
00:00:04
AI is no longer a thing of the future. It's happening right now and it's changing the way we work, create, and innovate. So whether you're just starting out or already familiar with AI, this Gen AI full course by Simply Learn will help you master the most exciting advancements in artificial intelligence. We'll start with the basics, what is generative AI and then dive into AI careers, the road map to get started, and the top AI technologies shaping the industry. You'll explore


00:00:29
powerful AI models like deepc carbon search GPT and open AI Sora while also understanding key concepts like deep learning transformers GS LSTMs and reinforcement learning. Want to know how businesses are using AI? We'll cover lchain LLMs and AI tools for job interviews along with deep dive into Google quantum AI and Agentic AI. Plus, if you're preparing for an AI job, we have got you covered with deep learning interview questions and LLM benchmarking insights. And by the end of this course,


00:00:56
you won't just know AI, you will know how to use it, build with it, and stay ahead in the AI revolution. But before we commence, if you're interested in mastering the future of technology, the professional certificate course in generative AI and machine learning is a perfect opportunity for you. Offering collaboration with the ENICT Academy IT Kur, this 11-month live and interactive program provides hands-on experience in cutting edge areas like generative AI, machine learning, and tools such as


00:01:22
chat, GPT, DALI 2, and hugging face. Hurry up and enroll now. You can find the course link in the description box and pin comments. Meet Emma, a graphic designer working on a new project. One day, her colleague mentions a tool that helps create designs, images, and text using AI. Intrigued, Emma wonders how AI can create something from scratch. Her curiosity grows and she decides to dive deeper into this new technology called generative AI. Generative AI refers to a type of artificial intelligence designed


00:01:55
to create new content such as text, images, music, and videos. Unlike traditional AI, which analyzes or categorizes data, generative AI produces original content based on patterns learned from vast data sets. Essentially, it generates new unique material. These models are often trained on large amounts of data and use sophisticated algorithms to mimic human creativity. Tools like chat GPT or doll can create art, write essays, or simulate conversations by generating outputs based on user prompts.


00:02:31
Generative AI has a wide range of applications. Content creation tools like GPT4 generate text, blog posts, stories, and essays from simple prompts. Art and design AI models such as Dolli generates unique images and designs based on text descriptions, transforming creativity in art, music and audio. AI can compose music or replicate voices, offering new possibilities for musicians and audio engineers. Healthcare generative AI simulates disease progression or creates synthetic medical data, helping doctors gain faster


00:03:08
insights for research. Let's take image generation as an example to explain how generative AI works. Data collection and learning. AI models like Dolly are trained on large data sets of images paired with text descriptions. These data sets teach the model to recognize different objects, colors, styles, and how to associate text with corresponding images. The more data the AI learns from, the better it can generate accurate and diverse images based on user prompts. neural networks and transformers. When Emma inputs a prompt,


00:03:43
like a cat wearing sunglasses, the transformer model processes the text, recognizing words like cat and sunglasses and links them to images it learned from during training. Transformers help the AI decide how to combine these elements into a coherent image. Tokens and context. The text input, such as a cat wearing sunglasses, is split into smaller parts called tokens. The AI processes each token and understands their relationship. For instance, it knows the sunglasses should be placed on the cat, creating a


00:04:16
contextually accurate image. Feedback mechanism. Generative AI models improve through feedback. After generating an image, users provide feedback on the accuracy or quality of the output. If Emma's generated image shows the sunglasses floating beside the cat, she can mark it as incorrect. The model uses this feedback to improve future image generations. Reinforcement learning. Reinforcement learning further enhances the AI's ability. The model is rewarded when it generates accurate images and


00:04:49
corrected when it makes mistakes. For example, when Emma describes a sunset and the AI produces a vibrant sunset image, it receives positive reinforcement. Over time, this method refineses the model's ability to generate better images. Data science and AI models. Data scientists curate the training data and define the parameters that help the AI generate accurate images. The more varied the data set, the more versatile the AI becomes in generating diverse types of content. Advanced models use billions of


00:05:20
parameters which are settings that guide the AI in processing data and generating outputs. Generating original content. Once trained, the model can generate original images. For example, Emma might describe a futuristic cityscape, and the AI would produce a unique image based on what it learned. The generated image isn't just a copy of past data, but an entirely new creation, showcasing the AI's ability to combine learn patterns and creativity. Now, let's have a quick fun quiz on what we have learned so far.


00:05:53
What does generative AI primarily do? A. Analyze data. B. Generate new content. C store data. Make sure to let us know your answer in the comment section below. The term generative AI has emerged seemingly out of nowhere in recent months with a notable surge in interest according to Google trends even within the past year. The spike in curiosity can be attributed to the introduction of generative models such as Dalu B and CHP. However, what does generative AI entail? As a part of our introductory series on generative AI,


00:06:29
this video will provide a comprehensive overview of a subject starting from the basics. The explanation will cater to all levels of familiarity ensuring that viewers gain a better understanding of how this technology operates and its growing integration to our daily lives. Generative AI is after all a tool that is based on artificial intelligence. A professional who elites to switch careers with AI by learning from the experts. What is generative AI? Generative AI is a form of artificial intelligence possesses the capability of


00:06:57
to generate a wide range of content including text, visual, audio and synthetic data. The recent excitement surrounding generative AI stems from the user-friendly interfaces that allow users to effortlessly create highquality text, graphics and video within a seconds. Now moving forward, let's see how does generative AI works. Generative AI begin a prompt which can take form of text, image, video, design, audio, musical notes or any input that AI system can process. Various AI algorithm that generate new content in response to


00:07:32
the given prompt. This content can range from essay and problem solution to realistic created using images or audio of a person. In the early stages of generative AI, utilizing the technology involved submitting data through an API or a complex process, developers need to acquaint themsel with a specialized tool and writing application using programming language like Python. Some of the recent and fully operational generative AIS are Google part DL, OpenAI, ChatgPT, Microsoft Bing and many more. So now let's discuss Chat GPT, DAL


00:08:05
and B which are the most popular generative AI interfaces. So first is Dale 2 which was developed using openAI GPT implementation in 2021 exemplify a multimodel AI application. It has been trained on a vast data set of images and their corresponding textual description. Dallay is capable of establishing connection between various media forms such as vision, text, audio. It specifically links the meaning of words to visual elements. OpenAI introduced an enhanced version called Dal 2 in 2022 which empowers user to generate imagery


00:08:41
in multiple styles based on their prompts. And the next one is Chat GPT. In November 2022, ChatgPT an AI powered chatbot built on OpenAI GPD 3.5 implementation gained immense popularity worldwide. OpenAI enabled user to interact with and fine-tune the chatbots text response through a chat interface with interactive feedback. Unlike earlier version of GPT that was solely accessible via an API, chat GPT brought a more interactive experience. On March 14, 2023, OpenAI released GPD 4. Chat GPT integrates the conversational


00:09:17
history with a user making a genuine dialogue. Microsoft impressed by the success of new chat GP interface announced a substantial investment in OpenAI and integrated a version of GPT into its Bing search engine. And the next one is Bard. Google Bard. Google was also an earlier fortuner in advancing transformer AI techniques for language processing, protein analysis and other content types. It made some of these models open source for researchers but were not made available through a public interface. In response to


00:09:50
Microsoft integration of GPT into Bing, Google hardly launched a public facing chatbot named Googleb. Bar's debut was met by an error when the language model incorrectly claimed that the web telescope was the first to discover a planet in a foreign solar system. As a consequences, Google's stock price suffered a significant decline. Meanwhile, Microsoft implementation of chat GPT and GPT powered system also faced criticism for producing inaccurate result and displaying erectic behavior in their early irritation. So moving


00:10:22
forward, let's see what are the use cases of generative AI. Generative AI has broad applicability and can be employed across a wide range of use cases to generate diverse form of content. Recent advancement like GPT have made this technology more accessible and customizable for various application. Some notable use cases for generative AI are as follows. Chatbot implementation. Generative AI can be utilized to develop chatbots for customer service and technical support enhancing interaction with users and


00:10:50
providing efficient assistance. The second one is language dubbing enhancement. In the dam in the realm of movies and educational content, generative AI can contribute to improving dubbing in different languages, ensuring accurate and high quality translation. And the third one is content writing. Generative AI can assist in writing email response, dating profiles, rum√©s and term papers, offering valuable support and generating customized content tailored to specific requirement. And the fourth one is art


00:11:20
generation. Leveraging generative AI artists can create photorealistic artwork in various styles enabling the exploration of new artistic expression and enhancing creativity. The fifth one is product demonstration videos. Generative AI can enhance to enhance product demonstration video making them more engaging, visually appealing and effective in showcasing product features and benefits. So generative AI versatility allow it to employ it in many other application making it a valuable tool for content creation and


00:11:50
enhancing user experience across diverse domains. So after seeing use cases of generative AI let's see what are the benefits of generative AI. So generative AI offers extensive application across various business domains simplifying the integration and comprehension of existing content while also enabling the automated creation of a new content. Developers are actively exploring ways to leverage generative AI in order to enhance the optimize existing workflows and even to reshape workflows entirely


00:12:18
to harness the potential of technology fully. Implementing generative AI can bring numerous benefits including automated content creation. Generative AI can automate the manual process of writing content saving time and effort by generating text or other form of content. The next one is efficient email response. Responding to emails can be made more efficient with generative AI reducing the effort required and improving response time. And the third one is enhanced technical support. Generative AI can improve responses to


00:12:51
specific technical queries providing accurate and helpful information to users or customers. And the fourth one is realistic person generation. By leveraging generative AI, it becomes possible to create realistic representation of people enabling applications like virtual characters or avatars. And the fifth one is coherent information summarization. Generative AI can summarize complex information into a coherent narrative. Distilling key points and making it easier to understand and communicate complex


00:13:23
concept. The implementation of generative AI offers a range of potential benefits. streamingly process and enhancing content creation in various areas of business operation. So after seeing advantages of generative AI, let's move forward and see what are the limitations of generative AI. Early implementation of generative AI serve as vivid examples highlighting the numerous limitation associated with this technology. Several challenges arise from the specific approaches employed to implement various use case. For


00:13:51
instance, while a summary of a complex topic may more reader friendly than explanation incorporating multiple supporting sources, the ease of readability comes at the expense of transparent identifying the information sources. So the first one is when implementing or utilizing a generative AI application, it is important to consider the following limitation. I repeat, the first one is lack of source identification. Generative AI does not always provide clear identification of content source making it difficult to


00:14:20
trace and verify origin of the information. The second one is assessment of bias. Assessing the bias of original sources use generative AI can be challenging as it may be difficult to determine the underlying perspective or agenda of the data utilized in the training process. The third one is difficulty in identifying inaccurate information. Generative AI can generate realistic content making identifying inaccuracy or falsehoods within the generated output harder. And the fourth one is adaptability to a new


00:14:54
circumstances. Understanding how to fine-tune generative AI for a new circumstances or specific context can be complex requiring careful consideration and expertise to achieve desired result. And the fifth one is glossing over bias, prejudice and hatred. Generative AI results may amplify or perpetuate biases, prejudices or hateful content present in the training data requiring vigilance scrutiny to prevent such issues. So awareness of these limitation is crucial when the implementing of utilizing generative AI as it helps


00:15:27
users and developers critically evaluate and mitigate potential risk and challenges associated with the technology. So future of generative AI. Furthermore, advances in AI development platforms will contribute to the accelerated progress of research and development in the realm of generative AI. The development will encompass various domains such as text, images, videos, 3D contact, drugs, supply chains, logistic and business processes. While the current standalone tools are impressive, the true transformative


00:15:56
impact genative AI will realize while these capabilities are seemingly integrated into into the existing tools with regular use. Do you know artificial intelligence is transforming industries across the globe creating a wealth of career opportunities for those ready to embrace the future. Take Elon Musk for example. He is known for his work with Tesla and SpaceX and he co-founded OpenAI an organization dedicated to ensuring that AI benefits all the humanity. Musk transitions into AI underscores the massive potential of


00:16:31
this field. Not just the tech enthusiast but for anyone willing to innovate and adapt. Imagine this. In the tech city of Hyderabad, India, Arjun sits at his desk, eyes focused on his computer screen. Just two years ago, he was a new computer science graduate working as a junior software developer at a small startup. His salary was modest and his career prospects seemed limited. But everything changed when he discovered the booming field of artificial intelligence. Arjun spent his free time learning Python, exploring statistics


00:17:06
and experimenting with AI models. Fast forward 18 months, his hard work paid off. He landed a job as an AI engineer at a major tech company in Bengaluru, tripling his salary from six lakh to 18 lakhs per year. More importantly, Arjun found himself at the forefront of technology, working on projects that are shaping the future. Arjun's story is just one example of how AI transforms careers in India. Across the country, professionals are seizing new opportunities in AI as companies invest heavily in this revolutionary field. But


00:17:41
entering AI isn't easy. It requires dedication, continuous learning and adaptability. In this guide, we will explore AI career paths, the skills you need, and what it is like to work in this dynamic field. So let's talk about is AI is a good career or not. You have probably heard a lot about artificial intelligence or AI. It's everywhere and it's shaking up industries all over the world. But here's the big question. Is AI a good career choice? Yes, absolutely it is. Take Elon Musk for example. We


00:18:14
all know him as the guy behind Tesla and SpaceX. But did you know he also co-founded OpenAI? Even Alon's diving into AI, and that just shows how massive this field is becoming. And guess what? AI isn't just for tech geniuses. There's room for everyone. Let's talk about numbers. AI jobs are growing like crazy, up to 32% in recent years. And the pay is pretty sweet with roles offering over $100,000 a year. So whether you're into engineering, research, or even the ethical side of the things, AI has


00:18:50
something for you. Plus, the skills you pick up in AI can be used in all sorts of industries, making it a super flexible career choice. Now, AI is a big field and there are tons of different jobs you can go for. Let's break down some of the key roles. First up, we have machine learning engineers. These folks are like the backbone of AI. They build models that can analyze huge amounts of data in real time. If you've got a background in data science or software engineering, this could be your thing.


00:19:21
The average salary is around $131,000 in the US. Then there's data scientist, the detectives of the AI world. They dig into data to find patterns that help businesses make smart decisions. If you're good with programming and stats, this is a great option. and you can make about $105,000 a year. Next, we've got business intelligence developers. They are the ones to process and analyze data to spot trends that guide business strategies. If you enjoy working with data and have a background in computer science, this


00:19:55
role might be for you. The average salary here is around $87,000 per year. Then we've got research scientist. These are the ones pushing AI to new heights by asking innovative questions and exploring new possibilities. It's a bit more academic, often needing advanced degrees, but it's super rewarding with salaries around $100,000. Next up, we have big data engineers and architects. These are the folks who make sure all the different parts of business's technology talk to each other smoothly. They work with


00:20:31
tools like Hadoop and Spark and they need strong programming and data visualization skills. And get this, the average salary is one of the highest in AI around $151,000 a year. Then we have AI software engineer. These engineers build a software that powers AI application. They need to be really good at coding and have a solid understanding of both software engineering and AI. If you enjoy developing software and want to be a part of the AI revolution, this could be your role. The average salary is


00:21:05
around $108,000. Now, if you're more into designing systems, you might want to look at becoming a software architect. These guys design and maintain entire AI system, making sure everything is scalable and efficient. With expertise in AI and cloud platforms, software architects can earn hefty salary about $150,000 a year. Let's not forget about the data analyst. They have been around for a while, but their role has evolved big time with AI. Now they prepare data for machine learning models and create


00:21:37
super insightful reports. If you're skilled in SQL, Python, and data visualization tools like Tableau, this could be a great fit for you. The average salary is around $65,000, but it can go much higher in tech companies. Another exciting roles is robotics engineer. These engineers design and maintain AI powered robots from factory robots to robots that help in healthcare. They usually need advanced degrees in engineering and strong skills in AI, machine learning and IoT, Internet of Things. The average


00:22:12
salary of robotics engineer is around $87,000. With experience, it can go up to even more. Last but not the least, we have got NLP engineers. NLP stands for natural language processing. And these engineers specialize in teaching machines to understand human language. Think voice assistants like Siri or Alexa. To get into this role, you'll need a background in computational linguistics and programming skills. The average salary of an NLP engineer is around $78,000, and it can go even higher as


00:22:45
you gain more experience. So, you can see the world of AI is full of exciting opportunities. Whether you're into coding, designing systems, working with data, or even building robots, there's a role for you in this fastest growing field. So, what skills do you actually need to learn to land an entry-level AI position? First off, you need to have a good understanding of AI and machine learning concepts. You'll need programming skills like Python, Java, R. And knowing your way around tools like


00:23:14
TensorFlow, and PyTorch will help you give an edge, too. And do not forget about SQL, pandas and big technologies like Hadoop and Spark which are super valuable. Plus, experience with AWS and Google Cloud is often required. So, which industries are hiring AI professionals? AI professionals are in high demand across a wide range of industries. Here are some of the top sectors that hire AI talent. Technology companies like Microsoft, Apple, Google, and Facebook are leading the charge in AI innovation. Consulting firms like


00:23:49
PWC, KPMG and Accenture are looking for AI experts to help businesses transform. Then we have healthcare. Organizations are using AI to revolutionize patient with treatment. Then we have got retail. Giants like Walmart and Amazon leverage AI to improve customer experiences. Then we have got media. Companies like Warner and Bloomberg are using AI to analyze and predict trends in this media industry. AI is not just the future, it's the present. With right skills and determination, you can carve out a


00:24:22
rewarding career in this exciting field. Whether you're drawn to a technical challenges or strategic possibilities, there's a role in AI that's perfect for you. So, start building your skills, stay curious, and get ready to be a part of the air revolution. Imagine a world where creativity knows no bounds, where machines can conjure a art, music, and literature with the flick of a digital switch. This isn't the stuff of science fiction. It's the reality of generative AI, a cuttingedge technology that's


00:24:50
reshaping our digital landscape. Picture this. According to a recent report by Salesforce, generative air tools are already in the hands of 27% of millennials, 28% of Gen X, and a staggering 29% of Gen Z. These aren't just numbers. They are a testament to the growing influence of generative AI in our daily lives. And as organizations race to harness its power, the demand for skilled generative AI experts is skyrocketing. But what exactly is generative AI? It's more than just lines of code. It's a gateway to infinite


00:25:23
possibilities. With generative AI, machines can create anything from images to text to music. All by learning from existing data sets. It's the technology behind deep fakes, virtual influencers, and even the next big hit song. So why should you care about generative AI in 2024? Because it's not just the future, it's the present. It's the key to unlocking new realms of creativity, innovation, and opportunity. And in this video, we are going to show you how to become a master of generative AI in


00:25:53
2024. So, buckle up for the world of generative AI because the future is here and it's more exciting than ever before. So, welcome to the road map of becoming a generative AI expert in 2024. So learning generative AI in 2024 is crucial for several compelling reasons as we will outline in this diagram. So the number one reason is technological advancement. Generative AI represents a significant leap in the evolution of technology particularly in its ability to generate complex outputs like video,


00:26:22
audio, text and images. This innovation is set to expand exponentially marking a new age of technological innovation. And the next reason is wide ranging applications. The surge in interest and development in generative AI is fueled by advancements in machine learning models, artificial intelligence and platforms like chat GP and B. These tools have broad applications across various sectors making knowledge in this area highly valuable. Now going to the next reason that is solving complex problems. Generative AI has the


00:26:53
potential to simplify problem solving processes significantly. Its capabilities in creating realistic models can be applied to innovate and enhance solutions across industries. And the next reason is impact on major fields. The integration of artificial intelligence into major fields is undeniable. And generative AI plays a substantial role in this transformation. It not only presents a threat to certain jobs but also opens up a plethora of new opportunities in the tech industry and beyond. And the next is dynamic and


00:27:23
unexplored field. The field of generative AI is filled with challenges and unexplored territories offering an exciting frontier for those interested in shaping the future of technology. It calls for creativity, problem solving skills, and a willingness to delve into the unknown. So, learning generative AI in 2024 positions individuals at the forefront of technological innovation, equipping them with the skills and knowledge to contribute to significant advancements and explore new possibilities in the digital world. And


00:27:52
now we'll move to the major skills required to learn generative AI in 2024. So to effectively learn and excel in generative AI in 2024, individuals need to possess a specific set of skills that are foundational to understanding and applying this technology. So let's see what those skills are. So let's start with the skills and the number one skill is deep learning and fundamentals. A solid understanding of deep learning concepts is crucial. This includes familiarity with neural networks, back


00:28:19
propagation and the various types of deep learning models and architectures. And the next is machine learning concepts. Proficiency in machine learning is repeat proficiency in machine learning is necessary encompassing a broad range of algorithms, their applications and an understanding of how they can be used within generative AI frameworks. And then comes the Python programming. Python programming remains the dominant programming language in AI and machine learning. Mastery of Python includes its


00:28:47
syntax, data structure, libraries such as TensorFlow and PyTorch and frameworks is essential. And the next skill is generative models knowledge. Specific knowledge of generative models such as generative adversial networks, GN and variational autoenccoders, repeat and variational autoenccoders, VAEs, is required. Understanding how these models function and are applied to key to innovating within the generative AI space. And the next skill is image and text processing. Skills in processing and manipulating image and text data are


00:29:18
necessary as many generative AI applications involve creating or modifying such content. And the next on the list is data processing and data augmentation. The ability to pre-process and augment data efficiently can significantly improve the performance of generative models. Skills in data cleaning, augmentation techniques, and feature engineering are vital. And then comes ethical considerations. With the power of generative AI comes the responsibility to use it ethically. Understanding the ethical implications


00:29:46
of generative AI including issues of bias, fairness, and piracy is crucial. The next is communication. Given the interdisciplinary nature of generative AI projects, effective communication skills are essential for collaborating with teams, explaining complex concepts in simple terms and engaging with stakeholders. So developing these skills will prepare individuals for the dynamic and evolving field of generative AI enabling them to contribute meaningfully to advancements in technology and address the challenges that come with


00:30:14
it. So let's move to the road map to learn generative AI in 2024. And the road map is as follows. So the first step is understanding the basics of machine learning. Then comes mastering programming language that is mainly the Python. Then is the learning data science and related technologies. And then we have hands-on realtime projects. And then learning mathematical and statistics fundamentals. And then on the list is developer skills. And then we have the important thing that is keep learning and exploring. So starting with


00:30:44
the number one point that is understanding the basics of machine learning. So let's start by wrapping your head around the core machine learning algorithms. It's like getting to know the tools in your toolkit. Each has its unique use. Make sure to understand the differences between supervised, unsupervised, and reinforcement learning. Think of them as different paths to solving a puzzle. Some are straightforward. Repeat. Some are straightforward while others need you to figure out the rules as you go.


00:31:10
Get comfortable with handling data. After all, data is the fuel for your machine learning engine. Learn how to clean, spit, and pre-process it to get your models running smoothly. Learn how to evaluate your models with metrics like accuracy and precision. It's like checking the health of your model to ensure it's fit for the real world. And the next step for your road map would be master Python programming. So focus on getting a strong grip on Python syntax and structure. Python is the language of


00:31:36
choice in AI. So this is the learning repeat. So this is like learning the alphabet before writing stories. Dive into libraries essential for AI such as pandas for data manipulation and scikitlearn for machine learning. Think of these libraries as your shortcuts to build powerful models. Practice writing efficient code. It's not just about getting the right answer. It's about getting there faster and cleaner. Engage with the Python community. It's a treasure trove of knowledge and a great


00:32:02
way to stay updated on the latest trends and packages. And then comes the next step that is explore data science and related technologies. Sharpen your skills in data visualization. Visuals can reveal patterns and insights in data that numbers alone might not show. Master feature engineering to transform row data into a format that machines can better understand and predict for. Get a handle on building machine learning pipelines. These are like assembly lines that take your raw data and eye on


00:32:27
emerging technologies and frameworks in data science that complement generative AI. Staying updated will give you an edge in your projects. So now moving to the next step that is engage in hands-on realtime projects. So choose projects that spark on your interest and challenge you. This is where you get to apply what you have learned and see your knowledge comes to life. Work with different generative AI models. Each project is a chance to deepen your understanding and refine your skills. Don't just build, evaluate, and iterate


00:32:55
on your projects. Every iteration is a step closer to mastery. Document and present your work clearly. Sharing your journey not only helps others learn but also solidifies your own understanding. Now moving to the next step for your road map is solidify your math and statistics fundamentals. Dive deep into linear algebra and calculus. These are the building blocks for understanding how AI models learn and make predictions. Understand probability and statistics. This is crucial for modeling uncertainty and making informed


00:33:24
predictions. Learn about optimization techniques. These are the strategies your models use to improve over time like a person learning from the mistakes to get better. Now moving to the next step that is develop essential developer skills. Get comfortable with AI development tools. These tools can make your work faster, more efficient, and more collaborative. Focus on debugging and testing. A model that works flawlessly in theory might face unexpected challenges in the real world. Embrace ethical AI development. It's


00:33:52
important to ensure your AI solutions are fair, accountable, and transparent. And then comes the keep learning. The field of AI is always evolving and staying curious is the key. So this was for the development of essential developer skills. Now coming to the next step that is commit to continuous learning and exploration. So participate in AI communities. These are great spaces for learning from others experiences and sharing your own. Make reading research papers, blogs and books a habit. There are windows to the latest


00:34:19
advancements and theories in AI. Attend workshops and conferences. These events can inspire you and expose you to the new ideas and technologies. Seek mentorship or collaborate on projects. Learning from others can accelerate your growth and open new paths. So this was the road map for learning generative AI in 2024. The world is becoming increasingly competitive requiring business owners or individual to find new ways to stay ahead. Modern customers or individuals have higher expectations demanding personalized experience,


00:34:50
meaningful relationships and fast responses. Artificial intelligence is a gamecher here. AI helps promote goods and services or make your life easy with minimal effort and maximum result allowing everyone to make faster better informed decisions. However, with so many AI tools available, it can be challenging to identify the best ones for your needs and productivity boost. So, here are top 10 AI tools in 2024 that can transform your business or boost your productivity. On the number 10, we have TOM. Tom is a tool that can


00:35:23
help you share your thoughts and ideas quickly and effectively. Unlike other methods such as making a slide deck or building a web page, toms let you create engaging and detailed presentation in just a minute. You can enter any topic or idea and the AI will help you to put together a presentation that look great and gets your message across. It's like getting the ideas out of your head and into the world all without sacrificing quality. With tome you can be sure that your presentation will be the both fast


00:35:52
and effective. And ninth on the list is Zapier. Zapier is a popular web automation tool that connects different apps allowing user to automate repetitive task without coding knowledge. With Zapier you can combine the power of various AI tools to supercharge your productivity. Zapier supports more than 3,000 apps including popular platform like Gmail, Slack and Google Sheets. This versatility makes it a valuable tool for individual teams and businesses looking to streamline their operation and improve productivity. And


00:36:23
also with 7,000 plus integration and services offering, Zapier empower businesses everywhere to create processes and systems that let computer do what they are best at doing and let humans do what they are best at doing. After covering Zapia, number eight on the list is Gravity Write. Gravity Write is an AI powered writing tool that transform content creation. It generates high quality SEO optimized content in over 30 languages catering to diverse need like blog post, social media updates, ad copies and emails. These


00:36:56
tools ensure 100% original plagism free content safeguarding your brand's integrity. Its AI capabilities also include text to email generation enhancing visual content for marketing purposes. The tool offers both free and paid plans making it versatile for freelancer, small business owner and marketing teams. On the seventh number we have audio box. Audio box is advanced AI tool developed by Meta designed to transform audio production. It allow user to create custom voices, sound effect and audio stories with simple


00:37:29
text prompts using natural language processing. Audio box generate high quality audio clips that can be used for various purposes such as text to speech, voice mimicking and sound effect creation. Additionally, audio box offer interactive storytelling demos enabling user to generate dynamic narratives between different AI bosses. This tool is particularly useful for content creator, marketers and anyone needing quick highquality audio production without extensive manual effort. And next on number six, we have AOL. A cool


00:38:00
is advanced AI power tool tailored for e-commerce and marketing professionals. It offers comprehensive suit of feature designed to streamline content creation and enhance personalization. With a cool user can generate customized text, images, voice and videos, making it an invaluable assert for creating engaging product videos and marketing materials. Key feature of a cool include face swapping, realistic avatars, video transition and talking photos. These tools allow businesses to create dynamic


00:38:29
and personalized content that can captivate audience on social media and other platform. A cool's user-friendly interface and intelligent design make it easy for user to produce highquality content quickly and efficiently. On number five, we have 11 Labs. 11 Labs is a leading AI tools for text to speech and voice cloning. Known for its high quality natural sounding speech generation, the platform includes features like voice lab for rating or cloning voices with customizable options such as gender, age, and accent. Hey


00:39:01
there, did you know that AI voices can whisper or do pretty much anything? Ladies and gentlemen, hold on to your hats because this is one bizarre site. We have reports of an enormous fluffy pink monster strutting its stuff through downtown. Fluffy bird in downtown. Weird. Um, let's switch the setting to something more calming. Imagine diving into a fast-paced video game. Your heartbeat sinking with the story line. I got to go. The aliens are closing in. That wasn't calming at all. Explore all those voices yourself on the


00:39:36
11 Labs platform. Professional voice cloning supports multiple language and needs around 30 minutes of voice samples for precise replication. The extensive voice library offers a variety of profiles suitable for podcast, video narration and more. With various pricing plans ranging from free to enterprise level, 11 labs skors and large businesses alike. Standing out for its user-friendly interface and superior voice output quality. At number four, we have Go enhance. Go enhance AI is an advanced


00:40:08
multimedia tool designed to revise video and image editing. It leverages powerful AI algorithm to enhance and upscale images transforming them into high resolution masterpiece with extreme detail. The platform standout feature video to video allow user to convert standard video into various animated result such as pixel art and anime giving a fresh and creative touch to otherwise ordinary footage. This AI tool is ideal for social media content creator, marketer, educator, and anyone looking to bring their creative vision


00:40:42
to life. Whether you need to create eye-catching marketing materials or professional grade videos, Go enhance AI provides the resources to do so efficiently. At number three, we have Ptory. Ptory AI power tool designed to streamline video creation by transforming various content types into engaging visual media. It excels in converting textbased content like articles and script into compelling videos making it ideal for content marketers and educators. Users can also upload their own images and videos to


00:41:14
craft personalized content. The platform featured AI generated voiceovers which add a professional touch without the need for expensive voice talent. Victoria AI offers a range of customizable templates simplifying the video production process even for those with no design skills. Additionally, its unique textbased video editing capability allow user to repurpose existing content easily, creating highlights or short clips from the longer videos. At number two, we have Nvidia broadcast. It's a powerful tool


00:41:44
that can enhance your video conferencing experience. Whether you are using Zoom or Teams, it can address common challenges like background noise, poor lightning, or lowquality audio video. With this software, you can improve audio quality by removing unwanted noise such as keyboard clicks or fan sound. It also offers virtual background option and blurring effect without needing a green screen. So you can seamlessly integrate it with other application like OBS, Zoom, Discord or Microsoft Teams. Think of it as having a professional


00:42:15
studio at home. Plus, it's a free for Nvidia RTX graphic card user. Visit the website to learn more and start using it today. After covering all the tools, at number one we have Taplio. Taplio is an AI powered tool designed to enhance your LinkedIn presence and personal branding. It leverages artificial intelligence to create engaging content, schedule post and provide insight into your LinkedIn performance. Tableau's main feature include AI powered content inspiration, a library of viral post and a robust


00:42:47
post composer for scheduleuling and managing LinkedIn content efficiently. Tableau also offers easy to understand LinkedIn analytics to help user make informed decision based on their performance data. A free Chrome extension provides a quick overview of performance metrics directly on linkedin.com making it a convenient tool for daily users. There you have it, top 10 AI tools that are set to transform your life in 2024. Whether you are developer, content creator, or someone looking to boost their productivity,


00:43:17
these tools are worth keeping an eye on. The future is here and it's powered by AI. Something big is happening in the world of AI and it's shaking up the industry. No, it's not another Chat GPT update or a new release from OpenAI. This time it's a brand new player on the scene. Deepseek R1, an AI model that's already outperforming the best known models in ways that have experts seriously been talking. But why is this causing such a stir? Well, it's because Deepseek R1 doesn't just answer


00:43:49
questions like other AIS. It thinks. And I mean really thinks. It breaks down complex problems step by step, showing you exactly how it arrives at the solution. You just don't get an answer. You get the entire process from start to finish. That too in real time. This is the kind of AI that doesn't just solve problems. It solves the way we do. Now, here's a twist. Deepseek Arban isn't from one of the usual AI giants like OpenAI or Antropic. It was developed by Highfly Capital Management, a hedge fund


00:44:23
that's quietly been making waves in the AI world. In fact, their new Deep Sea Carbon Light preview is already making headlines for its stunning performance on task like mathematical reasoning, logical inference, and real-time decision-m areas where even the best of the OpenAI's models often fall short. But why is this model such a gamecher? It's because in 2024, the world is demanding AIs that can think logically and reason through problems. Deepseek's ability to show its work is what sets it


00:44:56
apart. While other AIs give you answers, Deepseek lets you watch its thought process unfold. Think of it as the AI that doesn't just solve problems, but teaches you how to think like it does. And if you think that's all just a hype, let me tell you, Forbes isn't the only one buzzing about it. Even experts across the industry are pointing out that DeepS is outperforming model from heavyweights like OpenAI Antropic on test that measures complex reasoning. It's already winning in some major AI


00:45:28
benchmarks and it's only going to get better. So why is Deepseek R1 such a big deal right now? And what does it mean for the future of AI in 2024 and beyond? Well, in this video, we will explore Deepseek R1 starting with its unique capabilities and how it stands out from the other AI model. We'll also test its math, reasoning and problem solving skills comparing it with models like OpenAI. Then we will examine the real world application in fields like finance and tech. So let's dive in and see how


00:45:58
it's shaping the future of AI. So at first let us understand what is exactly deepseek our life preview. As you can see this tweet post. So Deepseek our life preview is an AI tool which is similar to CHD and is created by a Chinese company which is known as deepseek. The company announced this new model on X on November 20 and shared a few details. So I'll show you the documentation page over here. So this is a documentation page of deepseek API and deepse review is meant to be really good


00:46:32
at solving problem reasoning in maths coding and logic. It shows you how to think step by step so that you can understand how it comes up with answers and which help people people trust more. So you can try out me on the website and the website name I'm going to show you. So you just have to simply go to Google and type over here chart.deepseek.com. As soon as you enter this, you'll be redirected to deepsek page. So simply I'll just log with the Google account. And here you can see this is the interface of our deepseek


00:47:12
hour light model. And uh here we will enable this uh deep think uh preview which is a new feature over here. And now let's start with our test and start testing whether deep sync this model is really that effective or worth the hyper low. So first we'll be doing the strawberry test. Okay. So let's see what exactly is a strawberry test. So let's start with a very simple question which is how many times does the letter R occur in the word strawberry? So this is our question over here and let us just


00:47:46
check what would be the answer. So I didn't expect such a long reasoning process for what seems like a straightforward task. I thought that after counting the letter R and identify the position of the word it would have stopped there. But what's interesting to me is that it didn't stop. It double checked the counting a couple of times and even considered things like how people might pronounce or spell the word differently which I think is a bit of especially the pronunciation part but


00:48:15
then that's how it does show how careful and thoughtful it is. So it just explained us every step by step and what process we can go through it. So let's move on to our second question which would be a mathematical reasoning question. So our next question would be uh if a triangle so if a triangle has sides of 3.5 what would be its area? So let's just check so as you can see here the DC car model is getting our answers and uh it performs the checks I predicted although in a different order


00:48:53
but then it's using all the steps and all the formulas and what would be the best to go with this question. So both the explanation and the output are particularly clear and easy to follow which makes me think that this would be fantastic model to embed in math students assistant. For example, this is a particular use case. Maybe the thought process could be shown first and the student could interact with acknowledging whether they understood it or not. So that's a pretty uh fast answer. Let's move on to our third


00:49:27
question. So now let's ask deep model a geometrical type of question which is a bit tough and let's see what answer would it generate for us and uh how it is it taking the logic building and the process. So as you can see it is taking all the formulas and also it is considering the other probabilities as well. So we'll just wait a few seconds. So as you can see it has written data messed up in the denominator. So it is even checking what mistakes hasn't generated which is really good. It is


00:50:02
just like as if somebody is solving this mathematical problem but then in the form of AI. So that's really interesting. So now as you can see it has given us the final answer and also stepby-step approach which is well known geometric formulas and applied them directly which makes the reasoning very easy to follow. So the output is very clear and easy to follow and across all these example it's been impressive to see how it consistently doublech checkck calculations using different methods.


00:50:36
The thought process is always detailed logical and very easy to understand. So for somebody who wants to actually learn or go through these mathematical problem this model will give a very detailed explanation. So I would highly suggest using this. So let's now move on to our next question which which would be a coding test. So the first question would be uh implement a function in Python that finds the longest palomic substring in a given particular string. The function should have a time complexity


00:51:09
better than O N raised to the power 3. They go off N to the^ 3. So let us just see what and what is the time complexity will it generate the answer for us the code. Let's just check it out. So as you can see it has given us the solution code here and also the explanation part and whatever the function it is using. It has briefly mentioned the explanation about it and I think that the model did a very great job solving the problem and finding the longest parenthes which would be slow it used


00:51:52
the very clever expand around sentence technique and this method handled both oddlength parales and even length parent zones as well which was really clear and effective. So that was for the coding part. So now let's move on to a logical reasoning type of question and let's see what answer does he generate and u and we will also compare it with the open model here as well. So let's quickly check it out. A question here is a man has to cross a river with a wolf, a goat and a cabbage. This goat can only carry


00:52:28
himself and one other thing. If left alone, the w the wolf would eat the goat and the goat would eat the cabbage. How can he get everything across safely? Let's quickly check out the answer. So, as you can see, uh it's thinking of giving the answer for us. Also, for the deep thing messages, you can use 50 messages per day. So we have already um used five texts and we have 45 messages left for today. So as you can see it has given a clear detailed explanation and it solved the problem really very well. I think it carefully


00:53:08
went through the rules and checked different possibilities. It understands that some pairs like the wool and the goat or the goat the cabbage can't be left alone. It also review the constraints at the starting. It also reviews the constraints at the start. Starting from this, it looks at what would happen if the man takes each item across the river first and works out it creates any problem. What I find really nice is about how the model adjusts its plan when something doesn't work. For


00:53:39
example, when it tries taking the wool first, it realizes talking to the wool first, but realizes at the same time that it causes trouble and then rethinks the step. This trial and error method feels very similar to how we as human might solve the puzzle ourselves. So, in the end, the model comes up with a very right solution and explains it in a very clear and step-by-step way. So as you can see I've given charity to this question and this is the answer it has generated but then it wasn't that


00:54:11
detailed as compared to deepseat model. So here I would rate deepse model more because it gave me a very clear explanation with all the details step you can clearly see the difference out here. So for anyone who wants to understand from depth can this deepse model would be a great help. So let's quickly have a look on the beat hour live preview benchmarks. So we have tested the model in these conditions. But how does it stack up against other AIs on benchmarks like a IM math and competitive programming languages? So


00:54:48
you have this on the screen. We have compared it with a IM math GPQ diamond code like codebench and also Zebra logic. So according to the benchmarks, Bitseek our life review lowers the competition out of the water when it comes to math and logic problems with the pass rate of 52.5% on the AIM 2024 challenge. It is really ahead of OpenAI's preview and far surpasses models like GPD4 on complex math task. So with this we have come to the last part of the section and here we will be comparing that who really wins.


00:55:27
At first you have seen that we have compared with the open AI model. So let's just quickly test some questions and just check it out that with open AI which one performs much more better. So now we will compare deepseek versus openi model with some command prompts to see who really wins in different categories. So here I have this deepseek model. So my first command for this test is a math problem and it says solve for x the quadratic equation 3 x ^ 2 + 5x - 2. So let's quickly see this what answer


00:56:04
is it giving and we will be comparing it with the openi model as well. So as you can see it is giving us all the available solutions using different methods as well and see the first method the second method quadratic formula it's basically giving you the full overview of all the methods which you find easier and you can use that and here is our final answer. So let's compare it with the open AI open model as well. So here I have this question and I'll just check the answer. It's taking a bit of time


00:56:40
but let's wait for a minute. So as you can see it has only used one formula which is the quadratic formula and here we have the answer for this thing. But what I found is it provided a correct solution but then it lack whereas our deepse model it provided much more detailed explanation. So I think deepseek wins for this maths question as well. For the second comparison I have a coding problem. So let's check for both. First we will be checking for deepseat model. And the coding problem is write a


00:57:17
Python function to check if a given string is talent or not. And I'm waiting for the answer here. So as you can see deepse light review model has provided us a correct solution along with a detailed breakdown of why each part of the code works as it is does. So let's check for the oven model as well. So now as you can see for the coding task uh the oven model has given us the solution but then it provides a working solution with clear code but it doesn't explain the logic behind it. Whereas the


00:57:50
deepseek model had provided the logic and also the solution with each detailed breakdown. And I think for this also the winner would be deepse carbon net review because it provides better and clearer explanation. Now let's move on to the third test which would be the logical reasoning. So here is my question the logical reasoning which is if a man has five apples and gives three away how many apples does he left with? So let's check for the deepseat model. So now you can see it has given us the answer and


00:58:20
also the correct answer but it has explained each and every logic to go through this question and why have we used this particular logic as well. So that's a pretty uh breakdown detailed step-by-step explanation. Let's check for the open AI open model as well. So here I have my question. Let's check for the answer. See it has given us a straightforward answer which is approx to apples but then it hasn't mentioned the logic behind it same as it was seen in the coding task. I think um since


00:58:55
both of these models have provided the accurate correct answer but here also the winner would be deepse because it has provided with a lot of explanation and transparency as well. So, this is the wrap-up of the video today. And if you're looking for an AI that excels in reasoning, problem solving, and transparency, it's definitely a contender. While OpenAI and other models are fantastic, deep brings something unique to the table. Step-by-step logic, clear explanation with an edge when it


00:59:25
comes to math and coding challenges. So, what is deep learning? Deep learning is a subset of machine learning, which itself is a branch of artificial intelligence. Unlike traditional machine learning models which require manual feature extraction, deep learning models automatically discovers representation from raw data. So this is made possible through neural networks particularly deep neural networks which consist of multiple layers of interconnected nodes. So these neural network are inspired by


00:59:53
the structure and the function of human brain. Each layer in the network transform the input data into more abstract and composite representation. For instance, in image recognition, the initial layer might detect simple features like edges and textures while the deeper layer recognizes more complex structure like shapes and objects. So one of the key advantage of deep learning is its ability to handle large amount of unstructured data such as images, audios and text making it extremely powerful for various


01:00:23
application. So stay tuned as we delve deeper into how these neural networks are trained, the types of deep learning models and some exciting applications that are shaping our future types of deep learning. Deep learning AI can be applied supervised, unsupervised and reinforcement machine learning using various methods for each. The first one supervised machine learning. In supervised learning, the neural network learns to make prediction or classify that data using label data sets. Both input features and target variables are


01:00:53
provided and the network learns by minimizing the error between its prediction and the actual targets. A process called back propagation. CNN and RNN are the common deep learning algorithms used for tasks like image classification, sentiment analysis and language translation. The second one unsupervised machine learning. In unsupervised machine learning, the neural network discovers patterns or cluster in unlabelled data sets without target variables. It identifies hidden pattern or relationship within the data.


01:01:23
Algorithms like autoenccoders and generative models are used for tasks such as clustering, dimensionality reduction and anomaly detection. The third one, reinforcement machine learning. In this, an agent learns to make decision in an environment to maximize a reward signal. The agent takes action, observes the records and learns policies to maximize cumulative rewards over time. Deep reinforcement learning algorithms like deep Q networks and deep deterministic polygradient are used for tasks such as robotics and


01:01:55
gameplay. Moving forward, let's see what are the artificial neural networks. Artificial neural networks an inspired by the structure and the function of human neurons consist of interconnected layers of artificial neurals or units. The input layer receives data from the external resources and it passes to one or more hidden layers. Each neuron in these layers computes a weighted sum of inputs and transfers the result to the next layer. During training, the weight of these connection are adjusted to


01:02:26
optimize the network's performance. A fully connected artificial neural network includes an input layer or more hidden layers and an output layer. Each neuron in a hidden layer receives input from the previous layer and sends its output to the next layer. So this process continues until the final output layer produce the network response. So moving forward let's see types of neural networks. So deep learning models can automatically learn feature from data making them ideal to task like image


01:02:56
recognition, speech recognition and natural language processing. So the most common architecture and deep learnings are the first one feed forward neural network FN. So these are the simplest type of neural network where information flows linearly from the input to the output. They are widely used for tasks such as image classification, speech recognition and natural language processing NLP. The second one convolutional neural network designed specifically for image and video recognition. CNN's automatically learn


01:03:27
feature from images making them ideal for image classification, object detection and image segmentation. The third one, recurrent neural networks, RNNs are specialized for processing sequential data, time series, and natural language. They maintain an internal state to capture information from previous input, making them suitable for task such as a speech recognition, NLP, and language transition. So now let's move forward and see some deep learning application. The first one is autonomous vehicle.


01:03:57
Deep learning is changing the development of self-driving car. Algorithms like CNN's process data from sensors and cameras to detect object, recognize traffic signs and make driving decision in real time, enhancing safety and efficiency on the road. The second one is healthcare diagnostic. Deep learning models are being used to analyze medical images such as X-rays, MRIs and CT scans with high accuracy. They help in early detection and diagnosis of diseases like cancer, improving treatment outcomes and saving


01:04:27
lives. The third one is NLP. Recent advancement in NLP powered by deep learning models like transformers, chat GPD have led to more sophisticated and humanlike text generation, translation and sentiment analysis. So application include virtual assistant, chat bots and automated customer service. The fourth one defake technology. So deep learning techniques are used to create highly realistic synthetic media known as defects. While this technology has entertainment and creative application, it also raises ethical concern regarding


01:05:01
misinformation and digital manipulation. The fifth one, predictive maintenance in industries like manufacturing and aviation. Deep learning models predict equipment failures before they occur by analyzing sensor data. The proactive approach reduces downtime, lowers maintenance cost, and improves operational efficiency. So now let's move forward and see some advantages and disadvantages of deep learning. So first one is high computational requirements. So deep learning requires significant data and computational resources for


01:05:32
training. Whereas advantage is high accuracy achieves a state-of-the-art performance in tasks like image recognition and natural language processing. Whereas deep learning needs large label data sets often require extensive label data set for training which can be costly and time consuming together. So second advantage of deep learning is automated feature engineering automatically discovers and learn relevant features from data without manual intervention. The third disadvantage is overfitting. So deep learning can


01:06:03
overfit to training data leading to poor performance on new unseen data. Whereas the third deep learning advantage is scalability. So deep learning can handle large complex data set and learn from massive amount of data. So in conclusion, deep learning is a transformative leap in AI mimicking human neural networks. It has changed healthcare, finance, autonomous vehicles and NLP. On July 25th, Open AI introduced search GBT, a new search tool changing how we find information online. Unlike traditional search engines which


01:06:38
require you to type in specific keywords, Sergeibility lets you ask question in natural everyday language just like having a conversation. So this is a big shift from how we were used to searching the web. Instead of thinking in keywords and hoping to find the right result, you can ask now search exactly what you want to know and it will understand the context and give you direct answers. It designed to make searching easier and more intuitive without going through links and pages. But with this new way of searching, so


01:07:11
there are some important questions to consider. Can Serge GPT compete with Google, the search giant we all know? What makes SPD different from AIO views? Another recent search tool. And how does it compare to chat GPT open AI popular conversational AI? So in this video we are going to explore these questions and more. We will look at what makes RGBT special, how it compares to other tools and why it might change the way we search for information. Whether you are new into tech or just curious, this


01:07:43
video will break it down in simple words. Stick around to learn more about search. So what is SGPD? Serge GPT is a new search engine prototype developed by OpenAI designed to enhance the way we search for information using AI. Unlike a typical chatbot like chat GPT, search isn't just about having a conversation. It's focused on improving the search experience with some key features. The first one is direct answer. Instead of simply showing you a list of links, Sergey delivers direct answer to your


01:08:15
question. For example, if you ask what is the best wireless noise cancellation headphone in 2024, Sergeity will summarize the top choices highlighting their pros and cons based on expert reviews and user opinions. So this approach is different from the traditional search engines that typically provide a list of links leading to various articles or videos. The second one is relevant sources. Search responses come with clear citations and links to the original sources ensuring transparency and accuracy. So this way you can easily


01:08:46
verify the information and del deeper into the topic if you want. The third one conversational search allows you to have a back and forth dialogue with the search engine. You can ask follow-up questions or refine your original query based on the responsive you receive making your search experience more interactive and personalized. Now let's jump into the next topic which is search GPT versus Google. So search GPT is being talked about a major competitor to Google in the future. So let's break


01:09:16
down how they differ in their approach to search. The first one is conversational versus keywordbased search. Search GPT uses a conversational interface allowing user to ask question in natural language and refine their queries through follow-up questions. So this creates a more interactive search experience. On the other hand, Google relies on keyword based search where user enter specific terms to find relevant web pages. The second thing is direct answer versus list of links. So one of the search GP's standout feature


01:09:49
is its ability to provide direct answers to the question. It summarizes information from the various sources and clearly sites them so you don't have to click through multiple links. Google typically present a list of links leaving user to sift through the result to find the information they need. The third one AI powered understanding versus keyword matching. Search GPS uses AI to understand the intent behind your question offering more relevant result even if your query isn't perfectly


01:10:17
worded. Google's primary method is keyword matching which can sometimes lead to less accurate result especially for complex queries. The fourth one dynamic context versus isolated searches. So search maintains content across multiple interaction allowing for more personalized responses. Whereas Google treats each search as a separate query without remembering previous interaction. And the last one realtime information versus index web pages. Sergeibities aim to provide the latest information using real-time data from


01:10:49
the web. Whereas Google vast index is comprehensive but may include outdated or less relevant information. So now let's jump into the next topic which is SRGBD versus AI overviews. So SRGBD and AI overviews both use AI but they approach search and information delivery differently. It's also worth noting that both tools are still being developed, so their features and capabilities may evolve and even overlap as they grow. So here are the differences. The first one is source attribution. Search GBT


01:11:19
provides clear and direct citation linked to the original sources making it easy for user to verify the information whereas AI overviews include links. The citation may not always be clear or directly associated with specific claims. The second one is transparency control. Serge GBT promises greater transparency by offering publishers control over how their content is used, including the option to opt out of AI training. AI overviews offers less transparency regarding the selection of content and the summarization process


01:11:52
used. The next one is scope and depth. SRGBD strives to deliver detailed and comprehensive answers pulling from a broad range of sources including potential multimedia content and in AI overviews offers a concise summary of key points often with links for further exploration but with a more limited scope. Now let's jump into the next part. SGPD versus Chat GBD. Search GPD and CH GPD both developed by OpenAI share some core features but serve different purposes. So here are some differences. The first one is primary


01:12:25
purpose. Search GPT designed for search providing direct answer and sources from the web. Whereas SGPT focus on conversational AI generating text responses. The second one is information sources. SGPT relies on realtime information from the web whereas GPD knowledge based on the training data which might not be correct. The third one is response format. Serge Gibbdity prioritize concise answers with citation and source links. So whereas serity is more flexible generating longer text summarizes creative content code and


01:12:58
etc. The next feature is use cases search idle for factf finding research and task requiring upto-date information whereas GP is suitable for creative writing, brainstorming, drafting emails and other open-ended task. So now question arises when will ser be released? Sergeyd is currently in a limited prototype phase, meaning it's not yet widely available. OpenAI is testing with a select group to gather feedback and improve the tool. So if you are interested in trying Serge GPD, so you can join the weight list on its web


01:13:33
page, but you will need a chat GPD account. A full public release by the end of 2024 is unlikely as OpenAI hasn't set a timeline. It's more probable that surgdy features will gradually added to the chat GPD in 2024 or in 2025 with a potential standalone release later based on testing and the feedback. So with this we have come to end of this video. If you have any question or doubt please feel free to ask in the comment section below. Our team of experts will help you as soon as possible. Did you know that


01:14:02
within just a few lines of code you can transform an AI model into something far more powerful? something that responds to questions, connects to live data, pull insights from databases, and even interacts with other app in real time. That's what Langchain allows you to do. And it's quickly becoming the go-to framework for AI developers. Think about this. You're about to create something amazing. An AI that can think, learn, and grow in ways we once only dreamed of. And here's the best part. You don't


01:14:33
need to be an AI expert to make that happen. Langchain is like a toolkit that connects the most advanced large language models like OpenAI's GPT to realtime data, allowing you to build AI applications that are smart, flexible, and highly interactive. Langchain is more than just a way to make AI development easier. It's a framework that allows different language models to work together seamlessly. So whether you want to understand user questions with one LLM, create humanlike responses with


01:15:04
another, or pull in data from an API or a database, Langchain makes all possible. The framework takes care of heavy lifting, connecting models, managing data flows, and even customizing how your AI interacts with external sources. Now the question is why is Langchain so popular? It has become one of the most fastest growing opensource project because it's solving a huge problem for developers. The challenge of integrating generative AI analyms with external data and complex workflows. As AI becomes more central to


01:15:36
our lives in 2024, Langchain is helping developers create smarter, more powerful application. So whether it's just for chatbots, content creation, or advanced data analysis. In this tutorial, I'll show you exactly how to get started with Langchain. From setting up your environment to building the first AI powered app, I'll walk you through it. So, Langchain makes it possible to train models on our own custom data, opening up more possibilities for building specialized intelligent application. By


01:16:06
the end of this video, you will be ready to start building with Langchain. And trust me, once you see how easy it is, you'll wonder why you didn't start using it sooner. Let's start with a simple question. Why should we use lang chain? Imagine you're working with large language models like GPT4 or hugging face models and you want to take their capabilities further like integrating them with your own data sources or allowing them to take action based on information they retrieve. This is where


01:16:34
Langchain comes in. Langchain is like an open source framework that allows you to build intelligent applications by connecting large language models with external data sources. It can turn static AI interactions into dynamic data aware workflows. One of the best parts is you don't have to manually code everything from scratch. Lang chain abstracts away much more complexity of working with LMS allowing developers to focus on building functional applications instead of wrangling API calls and managing data pipelines. So,


01:17:05
Langchain is set to play even bigger role in AI development because it enables you to harness true power of generative AI by connecting it with real-time data and external tools. So, now we have understood what Lchain is. Let us now understand how to install lang. So, let's start with the installation of langu. We'll just simply go to the website and we'll just simply go to the website docs part and we'll just read through this documentation. So here it has explained what langchain is


01:17:35
and what are the framework consisting of. So we also have this tutorials on how do we install lang chain. Okay. So for installing you can just simply click on this quick start and uh see here it has uh written how do we set up on Jupyter notebook. So this is the command if you want to uh install latching we will use the pi pip command. So just simply you can copy this command pip install lang and you have to open your command prompt or the terminal in your computer and here you have to simply copy paste the command. So as you can


01:18:13
see it has it will uh load all the packages which is required for installing lchain. So you can see here requirement already satisfied. This is because I had already installed my uh lang chain before. So uh we have understood how to install this lang chain by using this command and you can also install the lm chain. We'll understand it later. Now let me just show you what else you need to install. First we have understood this lang chain. Then we have the pine cone client. So we'll just simply search here


01:18:45
pine cone client and uh it will redirect us to this page. So pine cone client is actually a vector store for storing and retrieving embeddings which we will use in the later steps. So pine cone is also used to actually uh you know create secret API keys. So here you can also create the API keys. You can also read the documentation part. So uh so we'll understand how to create API keys using open AI. But first uh let us install pine cone client in our system. So we'll again go to command from this terminal


01:19:24
and we'll just copy paste pip install pine cone client. So you can see here it will download and install all the packages required. So it has been installed. Now the third thing we're talking about is open EI client. So we'll use open AI models for our language large language task. So u so we'll just simply search here open AI. Okay. And so it has redirected us to this page open AAI platform. And uh okay before starting this uh so this is the platform here where you can create export an API key


01:20:06
uh in open AI. Okay. So you can see here overview quick start concept everything is there. And uh to create an API key, we'll just simply click here. And uh here you have to select this option create a new secret key. Suppose I give my secret key name. Anything you can give. Suppose uh I give test test 1 2 3. Okay. And permissions is all. And we create the secret key. Now uh you need to uh actually save your key. We will just copy this key because it will be required later while uh debugging the


01:20:44
code. So we will just copy paste this secret key. We will require it later. And then done. So these are the keys I have created. So actually charge GPT and other LLM models like open AI and hugging face uses lchain to integrate with other APIs to create your own custom LLM models or chat bots. So suppose here we have logged in our chart GBT here and uh if I search here who won the uh WTC World Cup in 2023. So here it has shown the answer. For example, uh if I search who won the cricket match world


01:21:32
[Music] cup. So here as you can see the charge has given answer as as my last knowledge update for men's welcome has not taken place yet. This is happening because the uh this charg older version has not been trained on the latest uh upcoming news or whatever the new technology is. So by using lang you can integrate with other APIs and you can create your own customized LM models or chat bots which help you to train your own custom data using various tool and APIs. So uh before we move on, I have already showed


01:22:13
you how to create the secret uh API key and how you have to store that the API key address. So first we have already understood how do we install lang chain here. So by using the pip command we do it and uh so you also need to install python uh 3.8 or later installed in your system. So I already have Python installed and to check that you can just simply I already have it installed in my system. So to check that I'll simply just type here Python minus minus version and click enter. So as you can


01:22:49
see it has shown me the Python version which is installed in my computer. This so the second step is already uh we have discussed which is open AI API key here. So second step we have already discussed how do we create our open AI API key. So we have to sign up in our open AI then go to the API key section and then create a new secret key and these are the keys I have created and you can just uh keep it later later you'll use that. So now we'll come to the third step which is create a project directory and


01:23:21
setup. So what we do is uh we have Jupiter installed in our system. So we'll just go to command prompt here and type uh J U py Jupiter notebook. So it will redirect us to the Jupyter notebook installed in our system. So if not uh we can just simply go from here. It is loading right now. We have to wait. Now you just need to click on this uh new and Python 3 II kernel because Python has been installed in my system. So we will use Python as our kernel here. And here you can just give the prompts the command. Before


01:24:02
that you have to create a Python file which we can also create this Python file in Visual Studio Code. Just simply go to Visual Studio Code here and just simply click on file new file and I'll just type here Python. P1 and you can uh you have to first create store the API key. For this we use the command open AI API key equal to and just give your uh secret key. Okay. So you can just simply uh copy paste here your secret key and just store this. So this ensures that your API key is stored securely and it can be


01:24:55
used whenever needed. Now the step four is to initialize your project and install the required libraries. So you need to add some additional libraries like streamlet to make a user interface. So let's uh add that too in our project folder. You can either create u a text file named requirements.ext and then uh install all these. We have already installed the OpenAI lang chain. We just need to install streamllet. So here you can just give the command pip install streamllet. So as you can see this I have already uh installed


01:25:31
streamllet before. Same wise you can also install openai if it is not installed in your system using the command terminal the windows powershell. So uh we have understood this how what all packages and what all we need to install. Now the next step is to build your first lang chain app. To create a simple app which uses a input query and the app will generate a response using openis GPT model. So you have to create a python file name uh main.pi here. So so as you can see I've already uh imported this main.pi and this is my


01:26:07
code here. import streamllet as SD from lang and the constants I have created and then I have initialized the open AI with our API key. So you have you have to just type this prompt here. I'm using VS code here. You can also do it in your Jupyter notebook and then to create the streamllet app you have to give a title lang demo with open AI. So this is the title I have provided and then the text input pro for prompt the prompt is uh st.ext text input and enter a prompt. You can just type enter a prompt or


01:26:39
whatever you wish to and then display the response. So if prompt response is lm.tpredict prompt so you can use the predict method for llm. So here what the app will so after creating and debugging this in the terminal. So your app will initialize open using your API key and the user inputs a prompt through the streamllet interface. lang chain processes the input and sends it to the open GBD model and the AI generates a response which is then displayed in the app. So now you can use all these prompts to run on your app. So to do


01:27:12
this you can just uh to see your app in action you can just go to the terminal and run the following command which is streamlit run main.pi. So you can just simply go to uh the terminal here and just simply type the command uh simply type the command which is stream streamllet run main py. So by giving this prompt a new tab in your browser will open displaying the app and you can also type any question into the input box. So last now we have understood all these steps. So this was a quite basic tutorial on


01:27:46
how to install Langchain and then you know integrate it with the app. You can also customize and expand. So Langchain's flexibility allows you to integrate other APIs also external data sources or even add memory to your AI application. So whether you're building a simple chatboard or more complex AI system, the possibilities are endless. So by following all these steps, you will have a fully functioning app running in your system in no time. Today we will take you through a hands-off lab


01:28:14
demo of how we can use GN generative adversarial network for the image classification and for amazing videos like this subscribe to our YouTube channel and press that bell icon to stay updated. So in today's session we will discuss what GN is and moving ahead we will cover types of models in G and in the end we will do a hands-off lab demo of celebrated face image using GN. So now let's see what is GN. So generative adversarial networks were introduced in 2014 by INJ Goodfellow and co-authors


01:28:48
JN's perform unsupervised learning task in machine learning. GNS can be used to generate new example that possibly could have been drawn from the original data set. So this is an image of GM. There is a database that has a real 100 rupee node. The generator neural network generates fake 100 rupee node. So the discriminator network will help to identify the real and the fake node or the real and the fake images you can say. So moving ahead let's see what is generator. So a generator is a GN neural


01:29:17
network that creates fake data to be trained on the discriminator. It learns to generate plausible data. So the generator examples or instances become negative training examples of the discriminator. So as you can see here the random input generate a new fake image. The main aim of the generator is to make the discriminator classify its output as real. So the part of GN that trains the generator includes the noisy input actor or generator network which transform the random input into an instance or the discriminator network


01:29:47
which classify the generator data. So after seeing what is generator let's see what is a discriminator. So the discriminator is a neural network that identifies the real data from the fake data created by the generator. So the discriminator training data comes from two sources. The real data instance such as real pictures of birds, human currency, nodes and anything are used by the discriminator as a positive sample during training. The second one is the fake data instance created by the generator are used as a


01:30:16
negative examples during the training process. So discriminator decide from the real images and the fake images generated by generator and discriminator decide which is fake and which are real. So now let's move on to the programming part and see how we can use GN using celebrity face image data set. So here we will start with GN generative adversarial networks. Okay. So first I will rename it GN. Okay. So here we will import uh some libraries like import OS. So we will do from pytorch machine learning deep learning library


01:30:55
which work for like neural networks. So here I will write from torch dot utlis dot data import data loader. Okay. So what is this torch dot list data? So this is an abstract class representing a data set and you here you can custom data set that inherit data set and override the data set. Okay and this import data loader. So data loader is a client application for the bulk import or export of the data and we can use it for to insert update delete or export like records and when importing data data loader reads extract


01:31:42
and loads data from the CSV files like comma separated values or from a database connection you can say and when exporting data it output a CSV file okay then moving forward like do vision then dot transform as t. Okay. So transform are like very common image transformation available in the to vision. So transformation module they can be changed together using compost. So most transform classes have function equivalent functional transform give fine grain control over the transformations. And one more like


01:32:34
from dot vision vision dot transforms sorry data sets import image folder. Okay. Invalid syntax. Why it is invalid? Let I will tell you it's not. It's import. Okay. Yes. So now what I will do? Okay. torch utilities. Yeah, now it's working fine. So now we will import uh the data set. So we are here we are using celebrity face image. Okay. So I will provide you the data set in the description box below. Don't worry. Okay. So you can download from data set directly from there. So this is my path to data


01:34:07
set 375 texttop face image data Okay, now let's run it. Okay, now let's run it. Okay, now I guess it's fine. Yeah. So here what I will do, I will set the image size and all. So image size was to 64. Then patch size equals to 256. Then batch size equals to 256. then stats equals to 0.5 comma 0.5 and again 0.5 okay 0.5 comma 0.5 comma 0.5 Okay so here we have set the image size and the patch size and the stat values used. So now what we will do we will train the data set. So here I will


01:35:44
write train train ds equals to image folder data sorry data directory transform t dot t dot compose Suppose here I will add D dot uh the G size then image size. Okay. Then again teeth dot center crop. Center crop here I will write image size I will be small. Then here I will write t dot to tensor comma t dot normalize stats. Okay. Let me do like this. Here I can write train DL equals to data loader then train DS P size then shuffle equals to true comma num workers equals to two number of workers then here I will write pin


01:38:03
memory okay true let me run it okay the system cannot find the path specified C user okay so there is an path error Okay. So, let me copy my path. Let's see. Now, let me run. Yeah. So, it's working fine. So let me import torch from torch vision dot utils import migr. Okay. Then import mattplot lib plot lib dot pip plot s plt [Music] then plot l. So this vision utils import make grid is used to uh make a grid. Okay, grid you know small small boxes and this mat plot li you already know is used for the making charts


01:39:46
different types of chart line chart bar chart pie chart okay so let me run this give some spaces so here I will write now make a function non img tensors. Then return img tensors stats 1 0 plus stats 0. Okay. So let me run this. Now what we will do? We will make again a new function for show images and show badges. Okay. For that I will write df show image. Okay. Then images, n max equals to 64. 64 will be there. Then figure comma axis equals to plt dot subplots figure size 10. Okay. Then axis dot


01:41:35
set x dot set vertx. Okay. Then ex dot im show. This is image show. Then make grid the non with the n function images dot bit detect and max comma end row number of rows will be 8 then dot permute 1 comma 2 comma 0. Okay. Then df show badge dla n max = to 64. Then for images in DL show images then images, max and max then trick. Okay. So now let's see some batches. So I will write show batch train. It's loading. It's loading. Okay. Some error. Okay. Image. Okay, I'm show not okay the spelling


01:44:15
mistake. So as you can see here this maybe Robert Downey Jr. This is Robert Downey Jr. uh this is also Robert Junior and different celebrities here. So we have to do GN in this we will generate the fake images and we'll generate the new images then discriminator will set the images which are real or fake. Okay. So now let's use GPU like let's see uh GPU is available or not. Okay. So here I will write dev get default device then if docq dot is available then return dot device Q. Okay.


01:45:30
Else return to dot device to CPU. Then DF to device data comma device like for from this we will move tensor to chosen device like okay if is instance and see instance data comma Uh list comma double return to device x comma device for x in data. return data dot to device command non blocking equals to true. Okay, t will be capital here. Then I will write class device data loader. So here what we we will do we will wrap a data loader to move data to a device. So for that df init function to self comma dl comma


01:47:33
device then here I will write self dot dl = to dl then self dot device Okay. So here I will write for the iteration. So here I have to give two underscore. Here I will write again self. So it yield a batch to data after moving it to a device. So for for bin self dot then yield to device then B comma self dot device. Okay. And the last one is df for length. We'll write self then it will return the number of batches. So return length of self dot dl. Okay. Invalid syntax. Okay. Not do not do not. Okay. So here I will


01:49:41
write device. And I will write device get fault device. Okay. Then train DL equals to device device data loader and train DL common device. Okay. So uh as we already know what is GN and what is discriminator and you know generator. So let's uh take again GN overview. So a generative address network GN has two parts. So the generator learns to generate possible data. The generator instant become negative training examples for the for producing impossible results. So So you have data. So what discriminator


01:51:07
will do? Discriminator will you know decide from the generated data and the real data which are fake and which are real. Okay. This will generator will do. Discriminator sorry. Okay. So discriminator like takes an image as an input and tries to classify it as real or generated. In this sense it's like any other neural network. So I will use here CNN which outputs is a single neural network for every image. So okay so I hope you know again like what discriminator is what generator is and what is like real data it is this


01:51:45
okay and we will generate the data okay fake data and what discriminator will do discrim will check whether the data is fake or real okay so here I will write import torch dot n as n. Here I will write discriminator equals to n dot sequential Okay. So, these are some So these are some layer okay flattened layer, convoluted layer. Okay, leaky uh radio layer. So here I am setting you know discriminator like 3 into 64 64 okay so here 64x 128 128 by 256. So these are the sizes sizes of the images. Okay.


01:53:27
So here discriminator equals to device discriminator device. Okay. this. Okay. What's wrong? The spelling is wrong maybe. Okay. So, it's saying discriminator is not defined. Okay. Let me debug. Okay. and nothing else the spelling was wrong. So sorry for that. So let me do for the better visuals. Okay. So I know I hope you know the generator what generator network is. So here what I will do I will set the size latent size equals to 128. Okay. So here we have set the discriminator. Now what we will do? We


01:54:50
will set the generator. Okay. The sizes like 3 into 64, 64 or 32, 128 and so on for all the layers. So here I'm setting for the generator the same I will write here. generator to device generator command device again the generator is okay I'm copying this one [Music] Okay, generator is defined here. Okay, that's working fine. So here so now what I will do I will do the discriminator training. Okay. So for that I have to write DF train discriminator real images. opt. Okay. Now we will clear the


01:56:38
discriminator gradients. So opt d dot zero grid. Okay. Here we will pass real images through discriminator. Okay. So these are the for the real images because by we have to show the all the real and the fake images then we'll shuffle then and we'll find out which is real and which is not. Okay. So and now we will generate the fake images using latent. Okay. So for this lat equals to torch dotrandom input and the best size we are giving the latent size we are giving. Okay, fake images is equal to


01:57:36
generator. So now what we will do? We will pass the fake images through discriminator as we did for the real images. Okay. So now we will update discriminator weights. For that I have to write loss equals to real loss then plus fake loss. Okay. Then loss dot backward opt dot step return loss dot item comma L score fake score. Okay. I done. Okay. Bracket is missing. Okay. Lost backward 36. Okay. So here what we did we did we pass the real images to discriminator then generate fake images and the same time


01:59:17
we pass the fake images through discriminator and at the end the loss equals to real loss and the fake loss we update the discriminator weights. Okay. So now so this was the discriminator training. Now what we will do? We will do the generator training. Okay. So for that I have to write df. For that I have to write df train generator then opt g do dot zero tra. So what we are doing here we are clearing the generator gradients. Before that we did for the same the discriminator one. Okay. So now we will generate the fake


02:00:24
images. Okay. What generator do? Generator only uh generate the fake images. Okay. So from this prediction from this prediction what we we are doing we are just make trying to fool the discriminator. Okay. So so here we will update the generator rates. So I will write loss dot backward. Then I will write op_g dot step. Then I will write return loss dot item. Okay, let's run it. So here I will write from dot utes import save image and here I will write sample directory equals to generated generated okay and os dot make


02:01:54
directory simple [Music] directory comma exist okay equals to true. Okay. So now what we will do we will uh save the sample data. Okay. So we I have to create uh to save samples uh one function. Okay. So here what I'm doing we are I'm making the fake images generating the fake images and saving it. Okay. So moving forward but what but I will fix the I will fix uh the latent latent equals to dot random input then 64 latent size comma 1 comma 1 comma then device. Then again save samples to zero comma


02:03:16
fixed latent. Okay. Save samples is not defined. It's defined here. Yeah. So see this is the generated images. This is a fake image. Okay. Now what I will do? I will do the full training loop. For that I have to write from TQDM dot notebook import DM then import to do.n NN dot functional SF let me give the spaces. So now what we will do we will train this uh we will do the full training loop till the 400 epoch. So it will take a very long time. So first I will write the definition. Okay, I will define one uh the


02:04:41
function. Okay, and then I will get back to you. So yes, so what I did uh so this I have set the losses and the scores. Okay. And uh these are the optimizers. Some optimizers opt you can say optimizer we are creating. And here I'm training the uh discriminator. And here I'm training the generator. Okay. for the loss and here the record of the loss in the you know scores will the save and this is for the log of losses and the scores last batch and for this this is for the generated image. Okay, it will save the generated


02:05:26
image. Okay, we have already created here you can see for the sample image for the saving. Okay. Now what I will do? I will write percent time then LR equals to 0.005 then epox= to 400 epox means it will take a huge step. So history equals to fit box comma l. Okay, fit is not defined. Okay, I have to run it again. Okay, but it's coming like this. Okay. Something item object has zero grid. Okay. Have to check. So as you can see it started running. So this box will run till 400. So it will take a long time, very long


02:06:50
time. So I will get back to you after that. So as you can see this is a 1 by 400. So it will run three till 339. Okay. So it will take a very very long time. So it will define the loss of generated, the loss of discriminator and the real score and the fake score and at the same time it's saving the generated images. Okay. So it will take a long time and then I will get back to you. So as you can see here the GN are done till like 400. Okay, till all the 400. Okay. So now let's do some losses. comma


02:07:46
losses of discriminator and the real score and the fake scores equals to history. So here I will touch dot save the generator dot state let's go directory path comma g dot ph path Okay, then I will write to dot save discriminator dot state directory path, d.t tth. Okay. Some spelling mistake is there. Yeah. So I write from ipython dot display import image. Okay. So here I will write image like what the generator generated the image dot slash generated slash images then 0001.p PNG. Okay, let's see. So this is this is


02:10:19
the first image which generated by the generator. Okay, so same we we have 400 epox. So let's see. So here I will check the 100 image. So as you can see 100 image is more bit clear. So what if I will check for the like 300 300 image one it's more bit clear. Okay, now let's check the 400 image. I hope see it is clear. So it is these are the fake images which are generated by the generator to fool the discriminator to confuse the discriminator. Okay. So now we will plot a graph. we will protoraph for the epoch


02:11:23
and the loss in the for the discriminator and the generator. So for that right so as you can see this is a discriminator okay blue one and there is a generator generator so loss for the generator is the more and the loss for discriminator is less which is very good and now let's see the real and fake images okay so these are the real images score and these are the fake images welcome to This video tutorial by simply learn. In this video, we will learn about an important popular deep learning


02:12:03
neural network called generative adversarial networks. Yan Leon, one of the pioneers in the field of machine learning and deep learning, described it as the most interesting idea in the last 10 years in machine learning. In this video, you will learn about what are generative adversarial networks and look in brief at generator and discriminator. Then we'll understand how GANs work and the different types of GANs. Finally, we'll look at some of the applications of GANs. So let's begin. So what are


02:12:34
generative adversarial networks? Generative adversarial networks or GANs introduced in 2014 by Ian J. Goodfellow and co-authors became very popular in the field of machine learning. GAN is an unsupervised learning task in machine learning. It consists of two models that automatically discover and learn the patterns in input data. The two models called generator and discriminator compete with each other to analyze, capture, and copy the variations within a data set. GANs can be used to generate new examples that possibly could have


02:13:06
been drawn from the original data set. In the image below, you can see that there is a database that has real 100 rupee nodes. The generator which is basically a neural network generates fake 100 rupees nodes. The discriminator network will identify if the nodes are real or fake. Let us now understand in brief about what is a generator. A generator in GANs is a neural network that creates fake data to be trained on the discriminator. It learns to generate plausible data. The generated instances


02:13:38
become negative training examples for the discriminator. It takes a fixed length random vector carrying noise as input and generates a sample. Now the main aim of the generator is to make the discriminator classify its output as real. The portion of the GAN that trains a generator includes a noisy input vector. The generator network which transforms the random input into a data instance. A discriminator network which classifies the generator data. And a generator loss which penalizes the generator for failing to do the


02:14:09
discriminator. The back propagation method is used to adjust each weight in the right direction by calculating the weights impact on the output. The back propagation method is used to obtain gradients and these gradients can help change the generator weights. Now let us understand in brief what a discriminator is. A discriminator is a neural network model that identifies real data from the fake data generated by the generator. The discriminator's training data comes from two sources. The real data instances such as real


02:14:40
pictures of birds, humans, currency notes, etc. are used by the discriminator as positive samples during the training. The fake data instances created by the generator are used as negative examples during the training process. While training the discriminator, it connects with two loss functions. During discriminator training, the discriminator ignores the generator loss and just uses the discriminator loss. In the process of training the discriminator, the discriminator classifies both real data


02:15:09
and fake data from the generator. The discriminator loss penalizes the discriminator for mclassifying a real data instance as fake or a fake data instance as real. Now moving ahead, let's understand how GANs work. Now GANs consists of two networks. A generator which is represented as G of X and a discriminator which is represented as D of X. They both play an adversarial game where the generator tries to fool the discriminator by generating data similar to those in the training set. The discriminator tries not to be fooled by


02:15:41
identifying fake data from the real data. They both work simultaneously to learn and train complex data like audio, video or image files. Now you are aware that GANs consists of two networks a generator G of X and discriminator D of X. Now the generator network takes a sample and generates a fake sample of data. The generator is trained to increase the probability of the discriminator network to make mistakes. On the other hand, the discriminator network decides whether the data is generated or taken from the real sample


02:16:11
using a binary classification problem with the help of a sigmoid function that gives the output in the range zero and one. Here is an example of a generative adversarial network trying to identify if the 100 rupee nodes are real or fake. So first a noise vector or the input vector is fed to the generator network. The generator creates fake 100 rupee nodes. The real images of 100 rupee nodes stored in a database are passed to the discriminator along with the fake nodes. The discriminator then identifies


02:16:41
the nodes and classifies them as real or fake. We train the model, calculate the loss function at the end of the discriminator network and back propagate the loss into both discriminator and generator. Now the mathematical equation of training again can be represented as you can see here. Now this is the equation and these are the parameters here. G represents generator. D represents the discriminator. Now P data of X is the probability distribution of real data. P of zed is the distribution of generator. X is the sample of


02:17:14
probability data of X. Zed is the sample size from P of zed. D of X is the discriminator network and G of zed is the generator network. Now the discriminator focuses to maximize the objective function such that D of X is close to one and Z of zed is close to zero. It simply means that the discriminator should identify all the images from the training set as real that is one and all the generated images as fake that is zero. The generator wants to minimize the objective function such that D of Z of Zed is one. This


02:17:50
means that the generator tries to generate images that are classified as real that is one by the discriminator network. Next, let's see the steps for training a neural network. So, we have to first define the problem and collect the data. Then, we'll choose the architecture of GAN. Now, depending on your problem, choose how your GAN should look like. Then we need to train the discriminator on real data. That will help us predict them as real for n number of times. Next, you need to generate fake inputs for the generator.


02:18:22
After that, you need to train the discriminator on fake data. To predict the generator data is fake. Finally, train the generator on the output of discriminator. With the discriminator predictions available, train the generator to fool the discriminator. Let us now look at the different types of GANs. So first we have vanilla GANs. Now vanilla GANs have minmax optimization formula that we saw earlier where the discriminator is a binary classifier and is using sigmoid cross entropy loss during optimization. In vanilla GANs the


02:18:56
generator and the discriminator are simple multi-layer perceptrons. The algorithm tries to optimize the mathematical equation using stochastic gradient descent. Up next we have deep convolutional GANs or DC GANs. Now DC GANs support convolutional neural networks instead of vanilla neural networks at both discriminator and generator. They are more stable and generate higher quality images. The generator is a set of convolutional layers with fractional strided convolutions or transpose convolutions.


02:19:26
So it unsamples the input image at every convolutional layer. The discriminator is a set of convolutional layers with strided convolutions. So it downsamples the input image at every convolutional layer. Moving ahead, the third type you have is conditional GANs or C GANs. Vanilla GANs can be extended into conditional models by using an extra label information to generate better results. In C GAN, an additional parameter called Y is added to the generator for generating the corresponding data. Labels are fed as


02:19:55
input to the discriminator to help distinguish the real data from fake data generated. Finally, we have super resolution GANs. Now, SR GANs use deep neural networks along with adversarial neural network to produce higher resolution images. Super resolution GANs generate a photorealistic highresolution image when given a low resolution image. Let's look at some of the important applications of GANs. So, with the help of DC GANs, you can train images of cartoon characters for generating faces of anime characters


02:20:27
and Pokemon characters as well. Next, GANs can be used on the images of humans to generate realistic faces. The faces that you see on your screens have been generated using GANs and do not exist in reality. Third application we have is GANs can be used to build realistic images from textual descriptions of objects like birds, humans, and other animals. We input a sentence and generate multiple images fitting the description. Here is an example of a text to image translation using GANs for a bird with a black head, yellow body,


02:21:00
and a short beak. The final application we have is creating 3D objects. So, GANs can generate 3D models using 2D pictures of objects from multiple perspectives. GANs are very popular in the gaming industry. GANs can help automate the task of creating 3D characters and backgrounds to give them a realistic feel. Welcome to our video about Transformers in AI. And no, we don't mean the robot toys from the movies. We are diving into something even cooler in the world of computers and AI. Have you


02:21:32
ever wondered how your phone knows what word you might type next? Or how Google Translate works so well? That's where transformers come in. They are like super smart computer brains that can understand and create humanlike text. Here's a fun example. I asked the transformer to tell me a joke and it said, "Why did the computer go to the art school?" Because it wanted to improve its draw speed. Okay, that's a bit cheesy, but it shows how these computer programs can come up with new


02:22:00
ideas on their own. Transformers are changing how we use technology every day. They help us with things like translating languages, summarizing long articles, writing emails and stories, and even playing games like chess. In this video, we will explore how transformers work, why they are so special, and what cool things they might do in the future. So, let's talk about what exactly are transformers. Transformers are an artificial intelligence model used to process and generate natural languages. They can


02:22:32
read and understand huge amount of text and then use that knowledge to answer questions, translate languages, summarize information, and even create stories or write code. The magic behind transformers is their ability to focus on different text parts with attention mechanisms. This means that they can understand context better than older models, making their outputs more accurate and natural sounding. The basic structure of a transformer includes two main parts, the encoder and the decoder. Think of the encoder as a translator


02:23:03
that understands and processes the input and the decoder as the one that takes the processed information and turns it into the output. For example, if we are translating a sentence from English to French, the encoder reads the English sentence and converts it into a form that AI can understand. The decoder then takes this form and generates the French sentence. A great example of a transformer in action is chat GBD. Chat GBD uses transformers to understand and generate humanlike text. When you ask a


02:23:32
question, it processes your input with its encoder and generates a response with its decoder. This lets it have conversations, write essays, and even tell jokes. For instance, if you ask Chad GPT, what's the weather like today? It uses its transformer model to understand your question and responded with it's sunny with a chance of rain in the afternoon. This ability to understand and generate text makes transformers incredibly powerful. So, let's talk about how transformers work. Transformers are especially good at


02:24:03
sequencetosequence learning task like translating a sentence from one language to another. Here's how they work. First, there's the attention mechanism. This allows the transformer to focus on different parts of input data. For example, if it's translating the sentence, the cat sat on the mat, it can pay attention to each word's context to understand the meaning better. So it knows CAT is related to SAT and MAT helping it produce an accurate translation in another language. Transformers also use something called


02:24:33
positional encoding. Since they process all words at once, they need a way to understand the order of the words. Positional encoding adds information about the position of each word to the input helping the transformers understand the sequence. Another key feature is like parallel processing. Unlike older models like recurrent neural network which is RNNS, the process takes words by word, transformers can process the entire sentence at once. This makes them much faster and more efficient. Let's compare


02:25:05
transformers with recurrent neural networks. But first, let's understand what are RNNs. So RNS is a type of neural network designed to handle sequential data. They process data one step at a time maintaining a memory of previous steps. This makes them good for task where order matters like speech recognition or time series prediction. However, our illness have a problem called the vanishing gradient which means that they can forget information from earlier in the sequence. Imagine trying to understand the sentence Alice


02:25:36
went to the park and then to the store and RNN might struggle to remember Alice by the time it gets to the store. But a transformer can easily keep track of Alice throughout the sentence. So why are transformers better? Unlike RNS, transformers process the entire sentence at once, keeping the context intact. This solves the vanishing gradient problem and makes transformers faster and more accurate to task like language translation and text generation. So let's talk about the applications of


02:26:07
transformers. At first we have language translation. They are used by services like Google translate to convert text from one language to another. For example, translating hello, how are you to Spanish as hola. Then we have document summarization. They can take long articles and summarize them into shorter, more concise versions. For instance, summarizing a 10page report into a few key points, making it easier to understand the main ideas without reading the whole document. Then we have content generation. Transformers can


02:26:38
write articles, stories, and even code. They can create new content based on what they have learned. For example, you could ask a transformer to write a short story about a space adventure, and then it would come up with a unique narrative. Then we have game playing. Transformers can learn and play complex games like chess, making strategic decisions just like a human player. They analyze the entire board and make moves considering all possible outcomes. Let's talk about image processing. They are


02:27:07
used in task like image classification and object detection helping computers understand visual data. For example, identifying objects in a photo like recognizing a cat, tree or a car. Now let's understand the training process. The training transformers involves two main steps. Semi-supervised learning. They can learn from both label data where the answer is known and unlabelled data where the answer is not provided. This makes them very versatile. For example, a transformer could be trained


02:27:37
on a mix of articles with and without summaries to learn how to summarize text effectively. Pre-training and fine-tuning, transformers are pre-trained on a large data set to learn general patterns. Then they are fine-tuned with specific task making them highly versatile. For instance, a transformer might be pre-trained on a large collection of books to understand language and then fine-tuned to generate marketing copy for a specific brand. The future potential of transformers is huge. Researchers are continuously


02:28:07
improving them, making them even more powerful. We can expect more advanced applications in areas like healthcare, finance, and more sophisticated AI systems that interact with humans in more natural ways. Imagine having an AI that can provide personalized medical advice or one that can help you write a novel. In conclusion, we can say that transformers are a revolutionary architecture in AI. They offer speed, efficiency, and versatility, changing how we interact with the technology. The future looks bright for transformers,


02:28:38
and we can't wait to see what they'll do next. So now, what is RNN? RNN are a type of neural network that are designed to process sequential data. They can analyze data with temporal dimensions such as time series, speech and text. RN can do this by using a hidden state passed from one time stamp to the next. The next hidden state is updated at each other time step based on the input and the previous hidden state. RNN are able to capture short-term dependencies in sequential data, but they struggle with


02:29:11
capturing long-term dependencies. Why the LSTMs are made? So moving forward let's discuss types of LSTM gates. So LSTM models have three types of gates. The input gate, the forget gate and the output gate. So let's first discuss the input gate. The input gate controls the flow of information into the memory cell deciding what to store. The input gate determines which values from the input should be updated in the memory cell. It uses a sigmoid activation function to scale the values between 0ero and one


02:29:47
and then applies pointwise multiplication to decide what information to store. Next is forget gate controls the flow of information out of the memory cell deciding what to discard. The forget gate decides what information should be discarded from the memory cell. It also uses a sigmoid activation function to scale the values between zero and one followed by pointwise multiplication to determine what information to forget. The last one is output gate controls the flow of information out of the LSTM deciding


02:30:18
what to use for the output. The output gate determines the output of the LSTM unit. It uses a sigmoid activation function to scale the values from 0 to one then applies f multiplication to produce the output of the LSTM unit. So these gates implemented using sigmoid function are trained using back propagation. They open and close based on the input and the previous hidden state allowing the LSTM to selectively retain or discard information effectively capturing long-term dependencies. So now let's discuss


02:30:49
application of LSTM. LSTM models are highly effective and used in various application including video analysis analyzing video frames to identify action object and scenes. The second is language simulation task like language modeling, machine translation and text summarization. The third one is time series prediction. So predicting future values in a time series. The fourth is voice recognition task such as speech to text transcription and command recognition. The last one is sentiment analysis classifying text sentiment as


02:31:23
positive negative on Europe. So there are many more examples of LSTM. So now let's move forward and understand LSTM model and how it works with example. Let's consider the task of predicting the next word in a sentence. This is a common application of LSTM networks in natural language processing. So I will break it down step by step using the analogy of remembering a story and deciding what comes next based on the context. So imagine you are reading a story. As you read, you need to remember


02:31:50
what has happened so far to predict what might happen next. So let's illustrate with a simple example sentence. The cat sat on the dash. So you want to predict the next word which could be mat or roof or something else. An LSTM network helps this make prediction by remembering important parts of the story and forgetting irrelevant details. So now let's dive into step-by-step process. So stepby-step acclamation using LSTM. The first one is reading the story input sequence. As you read each word in the


02:32:21
word sentence, you process it and store relevant information. For example, the you understand its determiner. Cat, you know it's a noun and the subject of the sentence. Said indicates the action performed by the subject. On preposition indicating the relationship between the cat and the next noun. So this sequence diagram showing the words being read and processed. So second comes forget gate. As you move through the sentence, you might decide that some details are no longer important. For instance, you


02:32:52
might decide that knowing the is less important now that you have kept and said. So the word forget gates help discard this less important information. So this sequence diagram you can see on the screen showing how relevant information is discarded. So the third one input gate when you read on you need to decide how relevant this information is. So this sequence diagram in the screen is showing how new information is integrated with the last one. Okay. The fourth one is the cell state memory part. So this is like your memory of the


02:33:29
story so far. It carries information about the subject cat and the action set on. So it updates the new information as you read each word. Okay. The cat sat on the retaining the important context. So this sequence diagram showing how the memory is updated with the new information. So the last one is output gate. When you need to predict the next word, the output gate helps you decide based on the current memory cell state. So it uses the context the cat set on the. So the predict the next word might


02:34:02
be met because cat and matt are often associated with the context. So it can predict anything. The cat sat on the table or on the sofa anything but Matt why I'm saying Matt because cat and matt are often associated in the same context. So this diagram is showing the prediction of the next word based on the current memory. So there are many applications where you can use LSTM in predicting time series or next word in the sentence. So by the using of LSTM gates, input gate, forget gate and output gate and updating the cell state,


02:34:38
the network can predict the next word in a sequence by maintaining relevant context and discarding unnecessary information. This step-by-step process allow LSTM network to effectively handle sequence and make accurate prediction based on the context. LLMs, if you ever wondered how machine learning can now understand and generate humanlike text, you are in the right place. From chatboards like chat jeepy to AI assistant that powers search engines, LLMs are transforming how we interact with technology. One of the most


02:35:08
exciting advancement in this space is Google's Gemini or OpenAI charging large language model designed to push the boundaries of what AI can achieve. In this video, we will explore what LLMs are, how they work, and why models like Geminy are critical for the future of AI. Google Gemini is part of a new wave of AI models that are smarter, faster, and more efficient. It is designed to understand context better, offer more accurate responses, and integrate deeply into service like Google search and


02:35:39
Google Assistant, providing more humanlike interactions. So, we will break down the science behind LLMs, including their massive training data set, transformer architecture, and how models like Gemini use deep learning innovation to change industries. Plus, we will compare Google Gemini to other popular LMS such as OpenAI Chat GB models, showing how each of these technologies is used to power chat bots, virtual assistants, and other AIdriven application. By end of this video, you will have a clear understanding of how


02:36:07
large language models like Gemini work, their key features, and what they mean for their future AI. Don't forget to like, subscribe, and hit the bell icon to never miss any update from Simply Learn. So, what are the large language models? Large language models like CH GPD4 generative pre-trained transformer 4 O and Google Gemini are sophisticated AI system designed to comprehend and generate humanlike text. These models are built using deep learning techniques and are trained on vast data set


02:36:35
collected from the internet. They leverage self attention mechanism to analyze relationship between words or tokens allowing them to capture context and produce coherent relevant responses. LLMs have significant application including powering virtual assistant, chatboards, content creation, language translation and supporting research and decision making. Their ability to generate fluent and contextually appropriate text has advanced natural language processing and improved human computer interaction. So now let's see


02:37:02
what are large language model used for. Large language models are utilized in scenarios with limited or no domain specific data available for training. These scenarios include both few short and zero short training approaches which rely on the model's strong inductive bias and its capability to derive meaningful representation from a small amount of data or even no data at all. So now let's see how are large language model trained. Large language models typically undergo pre-training on a


02:37:29
board all encompassing data set that shares statical similarities with the data set specific to the target task. The objective of pre-training is to enable the model require highlevel feature that can later be applied during the fine-tuning phase for a specific task. So there are some training processes of LLM which involves several steps. The first one is text prep-processing. The textual data is transformed into a numerical representation that the LLM model can effectively process. This conversion may


02:37:57
be involve techniques like tokenization, encoding and creating input sequences. The second one is random parameter initialization. The model's parameter are initialized randomly before the training process begins. The third one is input numerical data. The numerical representation of the text data is fed into the model of processing. The model's architecture typically based on transformers allows it to capture the conceptual relationship between the words or tokens in the next. The fourth


02:38:21
one is loss function calculation. A loss function calculation measure the discrepancy between the model's prediction and the actual next word or token in a syntax. The LLM model aims to minimize this loss during training. The fifth one is parameter optimization. The model's parameter are registered through optimization technique. This involves calculating gradient and updating the parameters accordingly gradually improving the model's performance. The last one is iterative training. The


02:38:45
training process is repeated over multiple iteration or epochs until the model's output achieve a satisfactory level of accuracy on that given task or data set. By following this training process, large language model learn to capture linguistic patterns, understand context and generate coherent responses enabling them to excel at various language related task. The next topic is how do large language models work. So large language models leverage deep neural network to generate output based


02:39:12
on patterns learned from the training data. Typically a large language model adopts a transformer architecture which enables the model to identify relationship between words in a sentence irrespective of their position in the sequence. In contrast to RNNs that rely on recurrence to capture token relationship transformer neural network employ self attention as their primary mechanism. Self attention calculates attention scores that determine the importance of each token with respect to the other token in the text sequence


02:39:40
facilitating the modeling of intricate relationship within the data. Next let's see application of large language models. Large language models have a wide range of application across various domains. So here are some notable application. The first one is natural language processing NLP. Large language models are used to improve natural language understanding tasks such as sentiment analysis, named entity recognition, text classification and language modeling. The second one is chatbot and virtual assistant. Large


02:40:07
language models power conversational agents, chatbots and virtual assistant providing more interactive and humanlike user interaction. The third one is machine translation. Large language models have been used for automatic language translation enabling text translation between different languages with improved accuracy. The fourth one is sentiment analysis. LLMs can analyze and classify the sentiment or emotion expressed in a piece of text which is valuable for market research, brand monitoring and social media analysis.


02:40:36
The fifth one is content recommendation. These models can be employed to provide personalized content recommendations enhancing user experience and engagement on platforms such as news website or the streaming services. So these application highlight the potential impact of large language models in various domains to improving language understanding, automation and interaction between humans and computers. We've looked at a lot of examples of machine learning. So let's see if we can give a little bit


02:41:00
more of a concrete definition. What is machine learning? Machine learning is the science of making computers learn and act like humans by feeding data and information without being explicitly programmed. We see here we have a nice little diagram where we have our ordinary system your computer nowadays you can even run a lot of the stuff on a cell phone because cell phones have advanced so much. And then with artificial intelligence and machine learning it now takes the data and it learns from what happened before and


02:41:30
then it predicts what's going to come next. And then really the biggest part right now in machine learning that's going on is it improves on that. How do we find a new solution? So we go from descriptive where it's learning about stuff and understanding how it fits together to predicting what it's going to do to postcripting coming up with a new solution. And when we're working on machine learning there's a number of different diagrams that people have posted for what steps to go through. A


02:41:57
lot of it might be very domain specific. So, if you're working on photo identification versus language versus medical or physics, some of these are switched around a little bit or new things are put in. They're very specific to the domain. This is kind of a very general diagram. First, you want to define your objective. Very important to know what it is you're wanting to predict. Then, you're going to be collecting the data. So, once you've defined an objective, you need to collect the data that matches. You spend


02:42:24
a lot of time in data science collecting data and the next step preparing the data. You got to make sure that your data is clean going in. There's the old saying, bad data in, bad answer out or bad data out. And then once you've gone through and we've cleaned all this stuff coming in, then you're going to select the algorithm. Which algorithm are you going to use? You're going to train that algorithm. In this case, I think we're going to be working with SVM, the support vector machine. Then you have to


02:42:52
test the model. Does this model work? Is this a valid model for what we're doing? And then once you've tested it, you want to run your prediction. You want to run your prediction or your choice or whatever output it's going to come up with. And then once everything is set and you've done lots of testing, then you want to go ahead and deploy the model. And remember I said domain specific. This is very general as far as the scope of doing something. A lot of models you get halfway through and you


02:43:17
realize that your data is missing something and you have to go collect new data because you've run a test in here someplace along the line. You're saying hey I'm not really getting the answers I need. So there's a lot of things that are domain specific that become part of this model. This is a very general model but it's a very good model to start with. And we do have some basic divisions of what machine learning does that's important to know. For instance, do you want to predict a category? Well,


02:43:41
if you're categorizing thing, that's classification. For instance, whether the stock price will increase or decrease. So, in other words, I'm looking for a yes no answer. Is it going up or is it going down? And in that case, we'd actually say, is it going up? True. If it's not going up, it's false, meaning it's going down. This way, it's a yes, no. 01. Do you want to predict a quantity? That's regression. So, remember, we just did classification. Now, we're looking at regression. These


02:44:07
are the two major divisions in what data is doing. For instance, predicting the age of a person based on the height, weight, health, and other factors. So based on these different factors, you might guess how old a person is. And then there are a lot of domain specific things like do you want to detect an anomaly? That's anomaly detection. This is actually very popular right now. For instance, you want to detect money withdrawal anomalies. You want to know when someone's making a withdrawal that


02:44:33
might not be their own account. We've actually brought this up because this is really big right now. If you're predicting the stock whether to buy stock or not, you want to be able to know if what's going on in the stock market is an anomaly, use a different prediction model because something else is going on. You got to pull out new information in there, or is this just the norm? I'm going to get my normal return on my money invested. So being able to detect anomalies is very big in


02:44:57
data science these days. Another question that comes up which is on what we call untrained data is do you want to discover structure in unexplored data and that's called clustering. For instance, finding groups of customers with similar behavior given a large database of customer data containing their demographics and past buying records. And in this case, we might notice that anybody who's wearing certain set of shoes goes shopping at certain stores or whatever it is. They're going to make certain purchases.


02:45:27
By having that information, it helps us to market or group people together. So then we can now explore that group and find out what it is we want to market to them if you're in the marketing world. And that might also work in just about any arena. You might want to group people together whether they're uh based on their different areas and investments and financial background, whether you're going to give them a loan or not. before you even start looking at whether they're a valid customer for the bank,


02:45:54
you might want to look at all these different areas and group them together based on unknown data. So, you're not you don't know what the data is going to tell you, but you want to cluster people together that come together. Let's take a quick detour for quiz time. Oh, my favorite. So, we're going to have a couple questions here under quiz time and um we'll be posting the answers in the part two of this tutorial. So, let's go ahead and take a look at these quiz times questions and hopefully you'll get


02:46:20
them all right and it'll get you thinking about how to process data and what's going on. Can you tell what's happening in the following cases? Of course, you're sitting there with your cup of coffee and you have your checkbox and your pen trying to figure out what's your next step in your data science analysis. So, the first one is grouping documents into different categories based on the topic and content of each document. Very big these days. you know, you have legal documents, you have uh


02:46:48
maybe it's a sports group documents, maybe you're analyzing newspaper postings, but certainly having that automated is a huge thing in today's world. B, identifying handwritten digits in images correctly. So, we want to know whether uh they're writing an A or capital A, B, C, what are they writing out in their hand digit, their handwriting. C behavior of a website indicating that the site is not working as designed. D, predicting salary of an individual based on his or her years of


02:47:21
experience with HR hiring uh setup there. So stay tuned for part two. We'll go ahead and answer these questions when we get to the part two of this tutorial or you can just simply write at the bottom and send a note to SimplyLearn and they'll follow up with you on it. Back to our regular content. And these last few bring us into the next topic which is another way of dividing our types of machine learning and that is with supervised unsupervised and reinforcement learning. Supervised learning is a method used to


02:47:52
enable machines to classify predict objects, problems or situations based on labeled data fed to the machine. And in here you see we have a jungle of data with circles, triangles and squares. And we label them. We have what's a circle, what's a triangle, what's a square. And we have our model training and it trains it. So we know the answer. Very important when you're doing supervised learning, you already know the answer to a lot of your information coming in. So you have a huge group of data coming in


02:48:19
and then you have new data coming in. So we've trained our model. The model now knows the difference between a circle, a square, a triangle. And now that we've trained it, we can send in in this case a square and a circle goes in and it predicts that the top one's a square and the next one's a circle. And you can see that this is uh being able to predict whether someone's going to default on a loan because I was talking about banks earlier. Supervised learning on stock market whether you're going to make


02:48:45
money or not. That's always important. And if you are looking to make a fortune in the stock market, keep in mind it is very difficult to get all the data correct on the stock market. It is very uh it fluctuates in ways you really hard to predict. So it's quite a roller coaster ride. If you're running machine learning on the stock market, you start realizing you really have to dig for new data. So we have supervised learning and if you have supervised we need unsupervised learning. In unsupervised


02:49:12
learning machine learning model finds the hidden pattern in an unlabeled data. So in this case, instead of telling it what the circle is and what a triangle is and what a square is, it goes in there, looks at them and says for whatever reason, it groups them together. Maybe it'll group it by the number of corners and it notices that a number of them all have three corners, a number of them all have four corners, and a number of them all have no corners. And it's able to filter those through and group them together. We


02:49:38
talked about that earlier with looking at a group of people who are out shopping. We want to group them together to find out what they have in common. And of course, once you understand what people have in common, maybe you have one of them who's a customer at your store, or you have five of them are customer at your store, and they have a lot in common with five others who are not customers at your store. How do you market to those five who aren't customers at your store yet? They fit the demographics of who's going to shop


02:50:02
there, and you'd like them to shop at your store, not the one next door. Of course, this is a simplified version. You can see very easily the difference between a triangle and a circle, which is might not be so easy in marketing. Reinforcement learning. Reinforcement learning is an important type of machine learning where an agent learns how to behave in an environment by performing actions and seeing the result. And we have here where the in this case a baby. It's actually great that they used an


02:50:26
infant for this slide because the reinforcement learning is very much in its infant stages. But it's also probably the biggest machine learning demand out there right now or in the future. It's going to be coming up over the next few years is reinforcement learning and how to make that work for us. And you can see here where we have our action. In the action in this one, it goes into the fire. Hopefully, the baby didn't it's just a little candle, not a giant fire pit like it looks like


02:50:51
here. When the baby comes out and the new state is the baby is sad and crying because they got burned on the fire. And then maybe they take another action. The baby's called the agent because it's the one taking the actions. And in this case, they didn't go into the fire. They went a different direction and now the baby's happy and laughing and playing. Reinforcement learning is very easy to understand because that's how as humans that's one of the ways we learn. We learn whether it is you know you burn


02:51:15
yourself on the stove, don't do that anymore. Don't touch the stove. In the big picture, being able to have machine learning program or an AI be able to do this is huge because now we're starting to learn how to learn. That's a big jump in the world of computer and machine learning. And we're going to go back and just kind of go back over supervised versus unsupervised learning. Understanding this is huge because this is going to come up in any project you're working on. We have in supervised


02:51:43
learning, we have labeled data. We have direct feedback. So someone's already gone in there and said, "Yes, that's a triangle. No, that's not a triangle." And then you predict an outcome. So you have a nice prediction. This is this this new set of data is coming in and we know what it's going to be. And then with unsupervised training, it's not labeled. So we really don't know what it is. There's no feedback. So, we're not telling it whether it's right or wrong.


02:52:07
We're not telling it whether it's a triangle or a square. We're not telling it to go left or right. All we do is we're finding hidden structure in the data, grouping the data together to find out what connects to each other. And then you can use these together. So, imagine you have an image and you're not sure what you're looking for. So, you go in and you have the unstructured data. Find all these things that are connected together and then somebody looks at those and labels them. Now you can take


02:52:34
that label data and program something to predict what's in the picture. So you can see how they go back and forth and you can start connecting all these different tools together to make a bigger picture. There are many interesting machine learning algorithms. Let's have a look at a few of them. Hopefully this gave you a little flavor of what's out there and these are some of the most important ones that are currently being used. We'll take a look at linear regression, decision tree and


02:52:59
the support vector machine. Let's start with a closer look at linear regression. Linear regression is perhaps one of the most well-known and well understood algorithms in statistics and machine learning. Linear regression is a linear model. For example, a model that assumes a linear relationship between the input variables x and the single output variable y. And you'll see this if you remember from your algebra classes, y = mx + c. Imagine we are predicting distance traveled y from speed x. Our


02:53:29
linear regression model representation for this problem would be y = m * x + c or distance equals m * speed + c where m is the coefficient and c is the y intercept. And we're going to look at two different variations of this. First, we're going to start with time is constant. And you can see we have a bicyclist. He's got a safety gear on. Thank goodness. Speed equals 10 meters/s. And so over a certain amount of time, his distance equals 36 kilometers. We have a second bicyclist who's going twice the speed or 20 m/s.


02:54:04
And you can guess if he's going twice the speed and time is a constant, then he's going to go twice the distance. And that's easy to compute. 36 * 2, you get 72 km. And so if you had the question of how fast somebody is going three times that speed or 30 m/s is, you can easily compute the distance in our head. We can do that without needing a computer. But we want to do this for more complicated data. So, it's kind of nice to compare the two, but let's just take a look at that and what that looks like in a


02:54:32
graph. So, in a linear regression model, we have our distance to the speed and we have our m equals the ve slope of the line. And we'll notice that the line has a plus slope. And as the speed increases, distance also increases. Hence, the variables have a positive relationship. And so, your speed of the person, which equals y= mx plus c, distance traveled in a fixed interval of time. And we could very easily compute either following the line or just knowing it's three times 10 meters/s that this is roughly 102 kilometers


02:55:03
distance that this third bicyclist has traveled. One of the key definitions on here is positive relationship. So the slope of the line is positive. As distance increase, so does speed increase. Let's take a look at our second example where we put distance is a constant. So we have speed equals 10 m/s. They have a certain distance to go and it takes him 100 seconds to travel that distance. And we have our second bicyclist who's still doing 20 meters per second. Since he's going twice the


02:55:32
speed, we can guess he'll cover the distance in about half the time, 50 seconds. And of course, you could probably guess on the third one 100 / 30 since he's going three times the speed. You can easily guess that this is 33.33 seconds time. We put that into a linear regression model or a graph. If the distance is assumed to be constant, let's see the relationship between speed and time. And as time goes up, the amount of speed to go that same distance goes down. So now your m equals a minus


02:56:01
v slope of the line. As the speed increases, time decreases. Hence, the variable has a negative relationship. Again, there's our definition. Positive relationship and negative relationship dependent on the slope of the line. And with a simple formula like this um and even a significant amount of data let's uh see with the mathematical implementation of linear regression and we'll take this data. So suppose we have this data set where we have xyx= 1 2 3 4 5 standard series and the y value is 3


02:56:33
22 43. When we take that and we go ahead and plot these points on a graph, you can see there's kind of a nice scattering and you could probably eyeball a line through the middle of it. But we're going to calculate that exact line for linear regression. And the first thing we do is we come up here and we have the mean of XI. And remember mean is basically the average. So we added 5 + 4 + 3 + 2 + 1 and divide by five. And that simply comes out as three. And then we'll do the same for y.


02:57:02
We'll go ahead and add up all those numbers and divide by five. And we end up with a mean value of y of i equals 2.8 where the x i references it's an average or means value and the yi also equals a means value of y. And when we plot that, you'll see that we can put in the y= 2.8 and the x= 3 in there on our graph. We kind of gave it a little different color so you could sort it out with the dashed lines on it. And it's important to note that when we do the linear regression, the linear regression


02:57:31
model should go through that dot. Now, let's find our regression equation to find the best fit line. Remember, we go ahead and take our y= mx plus c. So, we're looking for m and c. So, to find this equation for our data, we need to find our slope of m and our coefficient of c. And we have y = mx + c where m equals the sum of x - x average * y - y average or y means and x means over the sum of x - x means squared. That's how we get the slope of the value of the line. And we can easily do that by


02:58:06
creating some columns here. We have xy. Computers are really good about iterating through data. And so we can easily compute this and fill in a graph of data. And in our graph you can easily see that if we have our x value of 1 and if you remember the x i or the means value is 3 1 - 3 equals a -2 and 2 - 3 = a -1 so on and so forth and we can easily fill in the column of x - x i y - yi and then from those we can compute x - x i^ 2 and x - x i * y - yi and you can guess it that the next step is to go


02:58:46
ahead and sum the different columns for the answers we need. So we get a total of 10 for our x - x i^ 2 and a total of 2 for x - x i * y - yi. And we plug those in, we get 2/10, which equals2. So now we know the slope of our line equals2. So we can calculate the value of c. That'd be the next step is we need to know where it crosses the y ais. And if you remember, I mentioned earlier that the linear regression line has to pass through the means value, the one that we showed earlier. We can just flip


02:59:19
back up there to that graph. And you can see right here, there's our means value, which is 3 x= 3 and y= 2.8. And since we know that value, we can simply plug that into our formula. y =2x + c. So we plug that in, we get 2.8 8 =2 * 3 + C. And you can just solve for C. So now we know that our coefficient equals 2.2. And once we have all that, we can go ahead and plot our regression line. Y =2 * X + 2.2. And then from this equation, we can compute new values. So let's predict the values of Y using X= 1 2 3 4 5 and plot


03:00:01
the points. Remember the 1 2 3 4 5 was our original x values. So now we're going to see what y thinks they are, not what they actually are. And we plug those in, we get y of designated with y of p. You can see that x= 1 = 2.4, x= 2 = 2.6, and so on and so on. So we have our y predicted values of what we think it's going to be when we plug those numbers in. And when we plot the predicted values along with the actual values, we can see the difference. And this is one of the things that's very


03:00:31
important with linear regression in any of these models is to understand the error. And so we can calculate the error on all of our different values. And you can see over here we plotted um x and y and y predict. And we draw a little line so you can sort of see what the error looks like there between the different points. So our goal is to reduce this error. We want to minimize that error value on our linear regression model. Minimizing the distance. There are lots of ways to minimize the distance between


03:00:57
the line and the data points like sum of squared errors, sum of absolute errors, root mean square error, etc. We keep moving this line through the data points to make sure the best fit line has the least squared distance between the data points and the regression line. So to recap with a very simple linear regression model, we first figure out the formula of our line through the middle and then we slowly adjust the line to minimize the error. Keep in mind this is a very simple formula. The math


03:01:25
gets even though the math is very much the same, it gets much more complex as we add in different dimensions. So this is only two dimensions y = mx + c. But you can take that out to x, z, y, jq, all the different features in there. And they can plot a linear regression model on all of those using the different formulas to minimize the error. Let's go ahead and take a look at decision trees. A very different way to solve problems in the linear regression model. Decision tree is a treeshaped algorithm used to


03:01:55
determine a course of action. Each branch of a tree represents a possible decision, occurrence, or reaction. We have data which tells us if it is a good day to play golf. And if we were to open this data up in a general spreadsheet, you can see we have the outlook whether it's rainy, overcast, sunny, temperature, hot, mild, cool, humidity, windy, and did I like to play golf that day? Yes or no? So, we're taking a census and certainly I wouldn't want a computer telling me when I should go


03:02:25
play golf or not. But you could imagine if you got up in the night before you're trying to plan your day and it comes up and says tomorrow would be a good day for golf for you in the morning and not a good day in the afternoon or something like that. This becomes very beneficial and we see this in a lot of applications coming out now where it gives you suggestions and lets you know what what would uh fit the match for you for the next day or the next purchase or the next uh whatever you know next mail out


03:02:50
in this case is tomorrow a good day for playing golf based on the weather coming in. And so we come up and let's uh determine if you should play golf when the day is sunny and windy. So we found out the forecast tomorrow is going to be sunny and windy. And suppose we draw our tree like this. We're going to have our humidity. And then we have our normal, which is uh if it's if you have a normal humidity, you're going to go play golf. And if the humidity is really high, then we look at the outlook. And if the


03:03:17
outlook is sunny, overcast, or rainy, it's going to change what you choose to do. So if you know that it's a very high humidity and it's sunny, you're probably not going to play golf cuz you're going to be out there miserable, fighting off the mosquitoes that are out joining you to play golf with you. Maybe if it's rainy, you probably don't want to play in the rain. But if it's slightly overcast and you get just the right shadow, that's a good day to play golf


03:03:40
and be outside out on the green. Now, in this example, you can probably make your own tree pretty easily cuz it's a very simple set of data going in. But the question is, how do you know what to split? Where do you split your data? What if this is much more complicated data where it's not something that you would particularly understand like studying cancer? They take about 36 measurements of the cancerous cells and then each one of those measurements represents how bulbous it is, how extended it is, how sharp the edges are,


03:04:10
something that as a human we would have no understanding of. So how do we decide how to split that data up? And is that the right decision tree? But so that's a question that's going to come up. Is this the right decision tree? For that we should calculate entropy and information gain. Two important vocabulary words there are the entropy and the information gain. Entropy. Entropy is a measure of randomness or impurity in the data set. Entropy should be low. So we want the chaos to be as


03:04:38
low as possible. We don't want to look at it and be confused by the images or what's going on there with mixed data. And the information gain, it is a measure of decrease in entropy after the data set is split. Also known as entropy reduction. information gain should be high. So we want our information that we get out of the split to be as high as possible. Let's take a look at entropy from the mathematical side. In this case, we're going to denote entropy as I of P of and N where P is the probability


03:05:11
that you're going to play a game of golf and N is the probability where you're not going to play the game of golf. Now, you don't really have to memorize these formulas. There's a few of them out there depending on what you're working with. But it's important to note that this is where this formula is coming from. So when you see it, you're not lost when you're running your programming, unless you're building your own decision tree code in the back. And we simply have a log 2 of p over p plus


03:05:34
n minus n over p plus n * the log 2 of n of p plus n. But let's break that down and see what actually looks like when we're computing that from the computer script side. Entropy of a target class of the data set is the whole entropy. So we have entropy play golf. And when we look at this, if we go back to the data, you can simply count how many yeses and no in our complete data set for playing golf days. In our complete set, we find we have five days we did play golf and nine days we did not play golf. And so


03:06:06
our I equals, if you add those together, 9 + 5 is 14. And so our I equals 5 over 14 and 9 over 14. That's our PNN values that we plug into that formula. And you can go 5 over 14=.36. 9 over 14=64. And when you do the whole equation, you get the -.36 log<unk>^2 of.36 minus.64 log<unk> of 64. And we get a set value. We get 94. So we now have a full entropy value for the whole set of data that we're working with. And we want to make that entropy go down. And just like we calculated the entropy out for the whole


03:06:47
set, we can also calculate entropy for playing golf and the outlook. Is it going to be overcast or rainy or sunny? And so we look at the entropy. We have P of sunny times E of three of two. And that just comes out how many sunny days yes and how many sunny days no over the total which is five. Don't forget to put the we'll divide that five out later on. uh equals p overcast = 4 comma 0 plus rainy = 2a 3 and then when you do the whole setup we have 5 over4 remember I said there was a total of five 5 over 14


03:07:22
* the i of 3 of 2 + 4 over 14 * the 4 0 and 514 over i of 23 and so we can now compute the entropy of just the part that has to do with the forecast and we get 693 similar We can calculate the entropy of other predictors like temperature, humidity, and wind. And so we look at the gain outlook. How much are we going to gain from this entropy play golf minus entropy play golf outlook? And we can take the original 0.94 for the whole set minus the entropy of just the rainy day and temperature. And we end up with a gain


03:08:00
of.247. So this is our information gain. Remember we define entropy and we define information gain. The higher the information gain, the lower the entropy, the better. The information gain of the other three attributes can be calculated in the same way. So we have our gain for temperature equals 0.029. We have our gain for humidity equals.152. And our gain for a windy day equals 0048. And if you do a quick comparison, you'll see the 247 is the greatest gain of information. So that's the split we want. Now let's build the


03:08:34
decision tree. So, we have the outlook. Is it going to be sunny, overcast, or rainy? That's our first split because that gives us the most information gain. And we can continue to go down the tree using the different information gains with the largest information. We can continue down the nodes of the tree where we choose the attribute with the largest information gain as the root node and then continue to split each subnode with the largest information gain that we can compute. And although


03:08:59
it's a little bit of a tongue twister to say all that, you can see it's a very easy to view visual model. We have our outlook. We split it three different directions. If the outlook is overcast, we're going to play. And then we can split those further down if we want. So if the over outlook is sunny, but then it's also windy. If it's uh windy, we're not going to play. If it's uh not windy, we'll play. So, we can easily build a nice decision tree to guess what we would like to do tomorrow and give us a


03:09:26
nice recommendation for the day. So, we want to know if it's a good day to play golf when it's sunny and windy. Remember the original question that came out, tomorrow's weather report is sunny and windy. You can see by going down the tree, we go outlook sunny, outlook windy. We're not going to play golf tomorrow. So, our little smartwatch pops up and says, I'm sorry, tomorrow's not a good day for golf. It's going to be sunny and windy. And if you're a huge golf fan, you might go, "Uhoh, it's not


03:09:52
a good day to play golf." We can go in and watch a golf game at home. So, we'll sit in front of the TV instead of being out playing golf in the wind. Now that we looked at our decision tree, let's look at the third one of our algorithms we're investigating. Support vector machine. Support vector machine is a widely used classification algorithm. The idea of support vector machine is simple. The algorithm creates a separation line which divides the classes in the best possible manner. For


03:10:16
example, dog or cat disease or no disease. Suppose we have a labeled sample data which tells height and weight of males and females. A new data point arrives and we want to know whether it's going to be a male or a female. So we start by drawing a line. We draw decision lines. But if we consider decision line one, then we will classify the individual as a male. And if we consider decision line two, then it'll be a female. So you can see this person kind of lies in the middle of the two groups. So it's a little confusing


03:10:46
trying to figure out which line they should be under. We need to know which line divides the classes correctly. But how the goal is to choose a hyper plane and that is one of the key words they use when we talk about support vector machines. Choose a hyper plane with the greatest possible margin between the decision line and the nearest point within the training set. So you can see here we have our support vector. We have the two nearest points to it and we draw a line between those two points. And the


03:11:14
distance margin is the distance between the hyper plane and the nearest data point from either set. So we actually have a value and it should be equal distant between the two points that we're comparing it to. When we draw the hyperplanes, we observe that line one has a maximum distance. So we observe that line one has a maximum distance margin. So we'll classify the new data point correctly. And our result on this one is going to be that the new data point is MEL. One of the reasons we call


03:11:42
it a hyper plane versus a line is that a lot of times we're not looking at just weight and height. We might be looking at 36 different features or dimensions. And so when we cut it with a hyper plane, it's more of a three-dimensional cut in the data. Multi-dimensional that cuts the data a certain way. And each plane continues to cut it down until we get the best fit or match. Let's understand this with the help of an example. Problem statement. You always start with a problem statement when


03:12:10
you're going to put some code together. We're going to do some coding now. Classifying muffin and cupcake recipes using support vector machines. So the cupcake versus the muffin. Let's have a look at our data set. And we have the different recipes here. We have a muffin recipe that has so much flour. I'm not sure what measurement 55 is in, but it has 55, maybe it's ounces, but it has a certain amount of flour, certain amount of milk, sugar, butter, egg, baking powder, vanilla, and salt. And so based


03:12:40
on these measurements, we want to guess whether we're making a muffin or a cupcake. And you can see in this one, we don't have just two features. We don't just have height and weight as we did before between the male and female. In here, we have a number of features. In fact, in this we're looking at eight different features to guess whether it's a muffin or a cupcake. What's the difference between a muffin and a cupcake? Turns out muffins have more flour. Well, cupcakes have more butter


03:13:07
and sugar. So, basically the cupcakes a little bit more of a dessert where the muffin's a little bit more of a fancy bread. But how do we do that in Python? How do we code that to go through recipes and figure out what the recipe is? And I really just want to say cupcakes versus muffins like some big professional wrestling thing. Before we start in our cupcakes versus muffins, we are going to be working in Python. There's many versions of Python, many different editors. That is one of the


03:13:35
strengths and weaknesses of Python is it just has so much stuff attached to it. It's one of the more popular data science programming packages you can use. In this case, we're going to go ahead and use Anaconda and Jupyter Notebook. The Anaconda Navigator has all kinds of fun tools. Once you're into the Anaconda Navigator, you can change environments. I actually have a number of environments on here. We'll be using Python 36 environment. So, this is in Python version 36. Although, it doesn't


03:14:06
matter too much which version you use. I usually try to stay with the 3x because they're current unless you have a project that's very specifically in version 2x. 27 I think is usually what most people use in the version two. And then once we're in our um Jupyter notebook editor, I can go up and create a new file and we'll just jump in here. In this case, we're doing SPM muffin versus cupcake. And then let's start with our packages for data analysis. And we almost always use a couple


03:14:38
there's a few very standard packages we use. We use import oops import numpy that's for number python they usually denoted as np that's very comma that's very common and then we're going to import pandas as pd and numpy deals with number arrays. There's a lot of cool things you can do with the numpy uh setup as far as multiplying all the values in an array in a numpy array, data array. Pandas, I can't remember if we're using it actually in this data set. I think we do as an import. It


03:15:15
makes a nice data frame. And the difference between a data frame and a numpy array is that a data frame is more like your Excel spreadsheet. You have columns, you have indexes. So you have different ways of referencing it easily viewing it. And there's additional features you can run on a data frame. And pandas kind of sits on numpy. So they you need them both in there. And then finally, we're working with the support vector machine. So from sklearn, we're going to use the sklearn model.


03:15:45
Import SVM support vector machine. And then as a data scientist, you should always try to visualize your data. Some data obviously is too complicated or doesn't make any sense to the human, but if it's possible, it's good to take a second look at it so that you can actually see what you're doing. Now, for that, we're going to use two packages. We're going to import mapplot library.pipplot as plt. Again, very common. And we're going to import seabor as sns. And we'll go ahead and set the


03:16:18
font scale in the SNS. Right in our import line, that's what this U semicolon followed by a line of data. We're going to set the SNS. And these are great because the the seabour sits on top of map plot library just like pandas sits on numpy. So it adds a lot more features and uses and control. We're obviously not going to get into mattplot library and seabour. It' be its own tutorial. We're really just focusing on the SVM, the support vector machine from sklearn. And since we're in Jupiter


03:16:48
notebook, uh we have to add a special line in here for our mattplot library. And that's your percentage sign or amber sign mattplot library in line. Now, if you're doing this in just a straight code project, a lot of times I use like Notepad++ and I'll run it from there. You don't have to have that line in there because it'll just pop up as its own window on your computer depending on how your computer's set up because we're running this in the Jupyter notebook as a browser setup. This tells it to


03:17:20
display all of our graphics right below on the page. So that's what that line is for. I remember the first time I ran this, I didn't know that and I had to go look that up years ago. It's quite a headache. So map plot library inline is just because we're running this on the web setup and we can go ahead and run this. Just make sure all our modules are in. They're all imported, which is great. If you don't have them import, you'll need to go ahead and pip. Use the pip or however you do it. There's a lot


03:17:46
of other install packages out there, although pip is the most common. And you have to make sure these are all installed on your Python setup. The next step, of course, is we got to look at the data. You can't run a model for predicting data if you don't have actual data. So, to do that, let me go ahead and open this up and take a look. And we have our uh cupcakes versus muffins. and it's a CSV file or CSV meaning that it's commaepparated variable and it's going to open it up in


03:18:13
a nice uh spreadsheet for me and you can see up here we have the type we have muffin muffin muffin cupcake cupcake cupcake and then it's broken up into flour milk sugar egg baking powder vanilla and salt so we can do is we can go ahead and look at this data also in our Python let us create a variable recipes equals We're going to use our pandas module read CSV. Remember it was a commaepparated variable and the file name happened to be cupcakes versus muffins. Oops, I got double brackets


03:18:53
there. Do it this way. There we go. Cupcakes versus muffins. because the program I loaded or the the place I saved this particular Python program is in the same folder, we can get by with just the file name. But remember, if you're storing it in a different location, you have to also put down the full path on there. And then because we're in pandas, we're going to go ahead and you can actually in line you can do this, but let me do the full print. You can just type in recipes.head head in the Jupyter


03:19:29
notebook. But if you're running in code in a different script, you'd need to go ahead and type out the whole print recipes. And Pandanda's noses is that's going to do the first five lines of data. And if we flip back on over to the spreadsheet where we opened up our CSV file, uh you can see where it starts on line two. This one calls it zero. And then 2 3 4 5 6 is going to match. Go and close that out because we don't need that anymore. And it always starts at zero. And these are it automatically indexes


03:20:00
it since we didn't tell it to use an index in here. So that's the index number for the left hand side. And it automatically took the top row as labels. So pandas using it to read a CSV is just really slick and fast. One of the reasons we love our pandas, not just because they're cute and cuddly teddy bears. And let's go ahead and plot our data. And I'm not going to plot all of it. I'm just going to plot the uh sugar and flour. Now, obviously, you can see where they get really complicated if we


03:20:36
have tons of different features. And so, you'll break them up and maybe look at just two of them at a time to see how they connect. And to plot them, we're going to go ahead and use Seabor. So, that's our SNS. And the command for that is SNS.LM plot. And then the two different variables I'm going to plot is flour and sugar. Data equals recipes. The hue equals type. And this is a lot of fun because it knows that this is pandas coming in. So this is one of the powerful things about pandas mixed with


03:21:10
seabor and doing graphing. And then we're going to use a pallet set one. There's a lot of different sets in there. You can go look them up for seabor. We do a regular a fit regular equals false. So, we're not really trying to fit anything. And it's a scatter KWS. A lot of these settings you can look up in Seabor. Half of these you could probably leave off when you run them. Somebody played with this and found out that these were the best settings for doing a Seabor plot. And let's go ahead and run that. And because


03:21:41
it does it in line, it just puts it right on the page. And you can see right here that just based on sugar and flour alone, there's a definite split. And we use these models because you can actually look at it and say, "Hey, if I drew a line right between the middle of the blue dots and the red dots, we'd be able to do an SVM and and a hyper plane right there in the middle. Then the next step is to format or pre-process our data. And we're going to break that up into two parts. We need a type label. And


03:22:26
remember, we're going to decide whether it's a muffin or a cupcake. Well, a computer doesn't know muffin or cupcake. It knows zero and one. So, what we're going to do is we're going to create a type label. And from this we'll create a numpy array np where and this is where we can do some logic. We take our recipes from our panda and wherever type equals muffin it's going to be zero. And then if it doesn't equal muffin which is cupcakes it's going to be one. So we


03:22:55
create our type label. This is the answer. So when we're doing our training model remember we have to have a a training data. This is what we're going to train it with. Is that it's zero or one? it's a muffin or it's not. And then we're going to create our recipe features. And if you remember correctly from right up here, the first column is type. So we really don't need the type column because that's our muffin or cupcake. And in pandas, we can easily sort that out. We take our value


03:23:29
recipes columns. That's a pandas function built into pandas. values converting them to values. So it's just the column titles going across the top and we don't want the first one. So what we do is since it always starts at zero, we want one colon till the end. And then we want to go ahead and make this a list. And this converts it to a list of strings. And then we can go ahead and just take a look and see what we're looking at for the features. Make sure it looks right. Go ahead and run


03:24:07
that. And I forgot the S on recipes. So, we'll go ahead and add the S in there and then run that. And we can see we have flour, milk, sugar, butter, egg, baking powder, vanilla, and salt. And that matches what we have up here, right? Where we printed out everything but the type. So, we have our features and we have our label. Now, the recipe features is just the titles of the columns. We actually need the ingredients. And at this point, we have a couple options. One, we could run it over all the


03:24:43
ingredients. And when you're doing this, usually you do. But for our example, we want to limit it so you can easily see what's going on because if we did all the ingredients, we have, you know, that's what, um, seven, eight different hyperplanes that would be built into it. We only want to look at one so you can see what the SVM is doing. And so we'll take our recipes and we'll do just flour and sugar. Again, you can replace that with your recipe features and do all of them, but we're


03:25:12
going to do just flour and sugar. And we're going to convert that to values. We don't need to make a list out of it because it's not string values. These are actual values on there. And we can go ahead and just print ingredients. And you can see what that looks like. Uh, and so we have just the nanoflower and sugar, just the two sets of plots. And just for fun, let's go ahead and take this over here and take our recipe features. And so if we decided to use all the recipe features, you'll see that


03:25:47
it makes a nice column of different data. So it just strips out all the labels and everything. We just have just the values. But because we want to be able to view this easily in a plot later on, we'll go ahead and take that and just do flour and sugar. And we'll run that. And you'll see it's just the two columns. So the next step is to go ahead and fit our model. We'll go ahead and just call it model. And it's a SVM. We're using a package called SVC. In this case, we're going to go ahead


03:26:21
and set the kernel equals linear. So, it's using a specific setup on there. And if we go to the reference on their website for the SVM, you'll see that there's about there's eight of them here. Three of them are for regression. Three are for classification. The SVC, support vector classification, is probably one of the most commonly used. And then there's also one for detecting outliers and another one that has to do with something a little bit more specific on the model. But SBC and SVR are the two


03:26:52
most commonly used standing for support vector classifier and support vector regression. Remember regression is an actual value, a float value or whatever you're trying to work on. And SBC is a classifier. So it's a yes, no, true, false. But for this we want to know 01 muffin cupcake. We go ahead and create our model. And once we have our model created, we're going to do model.fit. And this is very common, especially in the sklearn. All their models are followed with the fit command. And what we put into the fit,


03:27:26
what we're training with it is we're putting in the ingredients, which in this case we limited to just flour and sugar, and the type label. Is it a muffin or cupcake? Now, in more complicated data science series, you'd want to split into, we won't get into that today, where you split it into training data and test data. And they even do something where they split it into thirds, where a third is used for where you switch between which one's training and test. There's all kinds of things go into that. It


03:27:55
gets very complicated when you get to the higher end. Not overly complicated, just an extra step, which we're not going to do today because this is a very simple set of data. And let's go ahead and run this. And now we have our model fit. And uh I got an error here. So let me fix that real quick. It's capital SBC. It turns out I did it lowercase. Support vector classifier. There we go. Let's go ahead and run that. And you'll see it comes up with all this information that it prints out automatically. These are


03:28:28
the defaults of the model. You notice that we changed the kernel to linear. And there's our kernel linear on the printout. And there's other different settings you can mess with. We're going to just leave that alone for right now. For this, we don't really need to mess with any of those. So, next we're going to dig a little bit into our newly trained model. And we're going to do this so we can show you on a graph. And let's go ahead and get the separating. and we're going to say uh


03:29:01
we're going to use a W for our variable on here and we're going to do model.coreefficient_0. So what the heck is that? Again, we're digging into the model. So we've already got a prediction and a train. This is a math behind it that we're looking at right now. And so the w is going to represent two different coefficients. And if you remember, we had y = mx + c. So these coefficients are connected to that but in two-dimensional it's a plane. We don't want to spend too much


03:29:38
time on this because you can get lost in the confusion of the math. So if you're a math wiz this is great. You can go through here and you'll see that we have a= minus w of 0 over w of 1. Remember there's two different values there. And that's basically the slope that we're generating. And then we're going to build an xx. What is xx? We're going to set it up to a numpy array. There's our np line space. So we're creating a line of values between 30 and 60. So it


03:30:12
just creates a set of numbers for x. And then if you remember correctly, we have our formula y equals the slope * x plus the intercept. Well, to make this work, we can do this as y equals the slope times each value in that array. That's the neat thing about numpy. So, when I do a * xx, which is a whole numpy array of values, it multiplies a across all of them. And then it takes those same values and we subtract the model intercept. That's your uh we had mx plus c. So, that'd be the c from the formula y mx plus c.


03:30:54
And that's where all these numbers come from. A little bit confusing because it's digging out of these different arrays. And then what we want to do is we're going to take this and we're going to go ahead and plot it. So plot the parallels to separating hyper plane that pass through the support vectors. And so we're going to create B equals a model support vectors. Pulling our support vectors out there. Here's our y, which we now know is a set of data. And we have uh we're going to create y down = a


03:31:24
* xx + b1 - a * b 0. And then model support vector b is going to be set that to a new value the minus1 setup. And y up = a * xx + b1 - a * b 0. And we can go ahead and just run this to load these variables up. If you wanted to know understand a little bit more of what's going on, you can see if we print y, let me just run that. You can see it's an array. This is a line. It's going to have in this case between 30 and 60. So there's going to be 30 variables in here. And the same thing


03:32:03
with y up, y down. And we'll we'll plot those in just a minute on a graph so you can see what those look like. Just go ahead and delete that out of here and run that. So, it loads up the variables. Nice clean slate. I'm just going to copy this from before. Remember this? Our SNS, our Seabor plot, LM plot, flower, sugar. And I'll just go and run that real quick so you can see what remember what that looks like. It's just a straight graph on there. And then one of the neat things is because


03:32:33
Seabour sits on top of piplot, we can do the piplot for the line going through. And that is simply plt.plot And that's our xx and y are two corresponding values x y. And then somebody played with this to figure out that the line width equals 2 and the color black would look nice. So let's go ahead and run this whole thing with the pi plot on there. And you can see when we do this, it's just doing flour and sugar on here. Corresponding line between the sugar and the flour and the muffin


03:33:09
versus cupcake. Um, and then we generated the U support vectors, the y down and y up. So let's take a look and see what that looks like. So we'll do our plot. And again, this is all against xx or our x value, but this time we have y down. And let's do something a little fun with this. We can put in a k dash dash. That just tells it to make it a dotted line. And if we're going to do the down one, we also want to do the up one. So here's our y up. And when we run that, it adds both


03:33:55
sets of line. And so here's our support. And this is what you expect. You expect these two lines to go through the nearest data point. So the dash lines go through the nearest muffin and the nearest cupcake when it's plotting it. And then your SVM goes right down the middle. So it gives it a nice split in our data. And you can see how easy it is to see based just on sugar and flour which one's a muffin or a cupcake. Let's go ahead and create a function to predict muffin or cupcake.


03:34:31
I've got my uh recipes I pulled off the um internet and I want to see the difference between a muffin or a cupcake. And so we need a function to push that through. And uh we create a function with deaf. And let's call it muffin or cupcake. And remember, we're just doing flour and sugar today. We're not doing all the ingredients. And that actually is a pretty good split. You really don't need all the ingredients to know it's flour and sugar. And let's go ahead and do an if


03:34:58
else statement. So if model predict is of flower and sugar equals zero. So we take our model and we do run a predict. It's very common in sklearn where you have a predict. You put the data in and it's going to return a value. In this case if it equals zero then print you're looking at a muffin recipe. Else if it's not zero that means it's one and you're looking at a cupcake recipe. That's pretty straightforward for function or def for definition. Deaf is how you do that in Python. And of


03:35:34
course, if you're going to create a function, you should run something in it. And so, let's run a cupcake. And we're going to send it values 50 and 20. A muffin or a cupcake. I don't know what it is. And let's run this and just see what it gives us. It says, "Oh, it's a muffin. You're looking at a muffin recipe." So, it very easily predicts whether we're looking at a muffin or a cupcake recipe. Let's plot this. There we go. plot this on the graph so we can


03:35:58
see what that actually looks like. And I'm just going to copy and paste it from below where we plotting all the points in there. So, this is nothing different than we did before. If I run it, you'll see it has all the points and the lines on there. And what we want to do is we want to add another point and we'll do pltot. And if you remember correctly, we did for our test we did 50 and 20. And then somebody went in here and decided we'll do yo for yellow or it's kind of a orangeish yellow color is going to come


03:36:31
up. Marker size nine. Those are settings you can play with. Somebody else played with them to come up with the right setup so it looks good. And you can see there it is graphed. Um clearly a muffin in this case in cupcakes versus muffins. The muffin has won. And if you'd like to do your own muffin cupcake contender series, you certainly can send a note down below and the team at SimplyLearn will send you over the data they use for the muffin and cupcake. And that's true of any of the data. We


03:37:04
didn't actually run a plot on it earlier. We had men versus women. You can also request that information to run it on your data setup. So you can test that out. So to go back over our setup, we went ahead for our support vector machine code. We did a predict 40 parts flour, 20 parts sugar. I think it was different than the one we did whether it's a muffin or a cupcake. Hence, we have built a classifier using SVM which is able to classify if a recipe is of a cupcake or a muffin. Which wraps up our


03:37:35
cupcake versus muffin. What's in it for you? We're going to cover clustering. What is clustering? K means clustering which is one of the most common used clustering tools out there including a flowchart to understand K means clustering and how it functions and then we'll do an actual Python live demo on clustering of cars based on brands. Then we're going to cover logistic regression. What is logistic regression? Logistic regression curve and sigmoid function. And then we'll do another


03:38:04
Python code demo to classify a tumor as malignant or benign based on features. And let's start with clustering. Suppose we have a pile of books of different genres. Now we divide them into different groups like fiction, horror, education, and as we can see from this young lady, she definitely is into heavy horror. You can just tell by those eyes and the maple Canadian leaf on her shirt. But we have fiction, horror, and education. And we want to go ahead and divide our books up. Well, organizing


03:38:33
objects into groups based on similarity is clustering. And in this case, as we're looking at the books, we're talking about clustering things with known categories. But you can also use it to explore data. So you might not know the categories. You just know that you need to divide it up in some way to conquer the data and to organize it better. But in this case, uh we're going to be looking at clustering in specific categories. And let's just take a deeper look at that. We're going to use K means


03:39:00
clustering. K means clustering is probably the most commonly used clustering tool in the machine learning library. K means clustering is an example of unsupervised learning. If you remember from our previous thing, it is used when you have unlabeled data. So we don't know the answer yet. We have a bunch of data that we want to cluster to different groups. Define clusters in the data based on feature similarity. So we've introduced a couple terms here. We've already talked about unsupervised


03:39:28
learning and unlabeled data. So we don't know the answer yet. We're just going to group stuff together and see if we can find an unanswer connect. We've also introduced feature similarity. Features being different features of the data. Now, with books, we can easily see fiction and horror and history books. But a lot of times with data, some of that information isn't so easy to see right when we first look at it. And so, K means is one of those tools where we can start finding things that connect that


03:39:57
match with each other. Suppose we have these data points and want to assign them into a cluster. Now when I look at these data points, I would probably group them into two clusters just by looking at them. I'd say two of these group of data kind of come together. But in K means we pick K clusters and assign random centrids to clusters where the K clusters represents two different clusters. We pick K clusters and say random centroidids to the clusters. Then we compute distance from objects to the


03:40:25
centrids. Now we form new clusters based on minimum distances and calculate the centrids. So we figure out what the best distance is for the centrid. Then we move the centrid and recalculate those distances. Repeat previous two steps iteratively till the cluster centroid stop changing their positions and become static. Repeat previous two steps iteratively till the cluster centroid stop changing and the positions become static. Once the clusters become static, then K means clustering algorithm is


03:40:54
said to be converged. And there's another term we see throughout machine learning is converged. That means whatever math we're using to figure out the answer has come to a solution or it's converged on an answer. Shall we see the flowchart to understand make a little bit more sense by putting it into a nice easy step by step? So we start, we choose K. We'll look at the elbow method in just a moment. We assign random centrids to clusters and sometimes you pick the centrids because


03:41:21
you might look at the data in a in a graph and say ah these are probably the central points. Then we compute the distance from the objects to the centrids. We take that and we form new clusters based on minimum distance and calculate their centrids. Then we compute the distance from objects to the new centrids. And then we go back and repeat those last two steps. We calculate the distances. So as we're doing it, it brings into the new centrid and then we move the centrid around and we figure out what the best which


03:41:50
objects are closest to each centrid. So the objects can switch from one centroid to the other as the centroidids are moved around and we continue that until it is converged. Let's see an example of this. Suppose we have this data set of seven individuals and their score on two topics A and B. Uh so here's our subject in this case referring to the person taking the uh test and then we have subject A where we see what they've scored on their first subject and we have subject B and we can see what they


03:42:19
score on the second subject. Now let's take two farthest apart points as initial cluster centroidids. Now remember we talked about selecting them randomly or we can also just put them in different points and pick the furthest one apart so they move together. Either one works okay depending on what kind of data you're working on and what you know about it. So we took the two furthest points one and one and five and seven. And now let's take the two farthest apart points as initial cluster


03:42:46
centrids. Each point is then assigned to the closest cluster with respect to the distance from the centrids. So we take each one of these points in there. We measure that distance. And you can see that if we measured each of those distances and you use the the Pythagorean theorem for a triangle in this case because you know the x and the y and you can figure out the diagonal line from that or you can just take a ruler and put it on your monitor. That'd be kind of silly but it would work if you're just eyeballing it. You can see


03:43:13
how they naturally come together in certain areas. Now we again calculate the centroidids of each cluster. So cluster one and then cluster two and we look at each individual dot. There's one, two, three. We're in one cluster. Uh the centrid then moves over. It becomes 1.8 comma 2.3. So remember it was at 1 and one. Well, the very center of the data we're looking at would put it at the one point roughly 22, but 1.8 and 2.3. And the second one, if we wanted to make the overall mean vector,


03:43:43
the average vector of all the different distances to that centrid, we come up with 4, 1, and 54. So we've now moved the centrids. We compare each individual's distance to its own cluster mean and to that of the opposite cluster and we find build a nice chart on here that the as we move that centrid around we now have a new different kind of clustering of groups and using uklidian distance between the points and the mean we get the same formula you see new formulas coming up. So we have our


03:44:11
individual dots distance to the mean centrid of the cluster and distance to the mean centrid of the cluster. Only individual three is nearer to the mean of the opposite cluster cluster two than its own cluster one. And you can see here in the diagram where we've kind of circled that one in the middle. So when we've moved the clust the centroidids of the clusters over one of the points shifted to the other cluster because it's closer to that group of individuals. Thus, individual three is


03:44:37
relocated to cluster two, resulting in a new partition. And we regenerate all those numbers of how close they are to the different clusters. For the new clusters, we will find the actual cluster centroidids. So now we move the centrids over. And you can see that we've now formed two very distinct clusters on here. On comparing the distance of each individual's distance to its own cluster mean and to that of the opposite cluster, we find that the data points are stable. Hence, we have our final clusters. Now if you remember


03:45:05
I brought up a concept earlier K mean on the K means algorithm choosing the right value of K will help in less number of iterations and to find the appropriate number of clusters in a data set we use the elbow method and within sum of squares WSS is defined as the sum of the squared distance between each member of the cluster and its centrid and so you see we've done here is we have the number of clusters and as you do the same K means algorithm over the different clusters and you calculate


03:45:37
what that centrid looks like and you find the optimal you can actually find the optimal number of clusters using the elbow the graph is called as the elbow method and on this we guessed at two just by looking at the data but as you can see the slope you actually just look for right there where the elbow is in the slope and you have a clear answer that we want two different to start with k means equals two a lot of times people end up computing k means equals 2 3 four five until they find the value which


03:46:05
fits on the elbow joint. Sometimes you can just look at the data and if you're really good with that specific domain remember domain I mentioned that last time you'll know that that where to pick those numbers and where to start guessing at what that K value is. So let's take this and we're going to use a use case using K means clustering to cluster cars into brands using parameters such as horsepower, cubic inches, make, year, etc. So, we're going to use the data set cars data having


03:46:32
information about three brands of cars, Toyota, Honda, and Nissan. We'll go back to my favorite tool, the Anaconda Navigator with the Jupiter notebook. And let's go ahead and flip over to our Jupyter notebook. And in our Jupyter notebook, I'm going to go ahead and just paste the uh basic code that we usually start a lot of these off with. We're not going to go too much into this code because we've already discussed numpy. We've already discussed mapplot library and pandas. Numpy being the number


03:47:01
array. Pandas being the pandas data frame and mattplot for the graphing. And don't forget uh since if you're using the Jupyter notebook, you do need the mattplot library in line so that it plots everything on the screen. If you're using a different Python editor, then you probably don't need that because it'll have a popup window on your computer. And we'll go ahead and run this just to load our libraries and our setup into here. The next step is of course to look at our data which I've


03:47:27
already opened up in a spreadsheet. And you can see here we have the miles per gallon, cylinders, cubic inches, horsepower, weight pounds, how you know how heavy it is, time it takes to get to 60. My card is probably on this one at about 80 or 90, what year it is. So this is you can actually see this is kind of older cars and then the brand Toyota, Honda, Nissan. So the different cars are coming from all the way from 1971 if we scroll down to uh the 80s. We have between the 70s and 80s a number of cars


03:47:58
that they've put out. And let's uh when we come back here, we're going to do importing the data. So we'll go ahead and do data set equals. And we'll use pandas to read this in. And it's uh from a CSV file. Remember, you can always post this in the comments and request the data files for these. Either in the comments here on the YouTube video or go to simplylearn.com and request that the car CSV, I put it in the same folder as the code that I've stored. So, my Python code is stored in the same folder. So, I


03:48:27
don't have to put the full path. If you store them in different folders, you do have to change this. And double check your name variables. And we'll go ahead and run this. And uh we've chosen data set arbitrarily because, you know, it's a data set we're importing. And we've now imported our car CSV into the data set. As you know, you have to prep the data. So, we're going to create the X data. This is the one that we're going to try to figure out what's going on


03:48:50
with. And then there is a number of ways to do this, but we'll do it in a simple loop so you can actually see what's going on. So, we'll do for i and x.c columns. So, we're going to go through each of the columns. And a lot of times it's important I I'll make lists of the columns and do this because I might remove certain columns or there might be columns that I want to be processed differently. But for this we can go ahead and take x of i and we want to go fill na and that's a pandas command. But


03:49:21
the question is what are we going to fill the missing data with? We definitely don't want to just put in a number that doesn't actually mean something. And so one of the tricks you can do with this is we can take x of i. And in addition to that, we want to go ahead and turn this into an integer because a lot of these are integers. So we'll go ahead and keep it integers. And me add the bracket here. And a lot of editors will do this. They'll think that you're closing one bracket. Make sure


03:49:46
you get that second bracket in there if it's a double bracket. That's always something that happens regularly. So once we have our integer of x of yi, this is going to fill in any missing data with the average. And I was so busy closing one set of brackets, I forgot that the mean is also has brackets in there for the pandas. So we can see here we're going to fill in all the data with the average value for that column. So if there's missing data is in the average of the data it does have. Then once


03:50:11
we've done that, we'll go ahead and loop through it again and just check and see to make sure everything is filled in correctly. And we'll print and then we take x is null. And this returns a set of the null value or the how many lines are null. And we'll just sum that up to see what that looks like. And so when I run this and so with the X, what we want to do is we want to remove the last column because that had the models. That's what we're trying to see if we can cluster


03:50:38
these things and figure out the models. There is so many different ways to sort the X out. For one, we could take the X and we could go data set, our variable we're using, and use the eyelocation, one of the features that's in pandas, and we could take that and then take all the rows and all but the last column of the data set. And at this time, we could do values. We just convert it to values. So, that's one way to do this. And if I let me just put this down here and print X, it's a capital X we chose. and I run


03:51:13
this, you can see it's just the values. We could also take out the values and it's not going to return anything because there's no values connected to it. What I like to do with this is instead of doing the location which does integers more common is to come in here and we have our data set and we're going to do data set dot or data set columns. And remember that lists all the columns. So if I come in here, let me just mark that as red and I print data set.c columns. You can see that I have my


03:51:51
index here. I have my MPG cylinders everything including the brand which we don't want. So the way to get rid of the brand would be to do data columns of everything but the last one minus one. So now if I print this, you'll see the brand disappears. And so I can actually just take data set columns minus one and I'll put it right in here for the columns we're going to look at. And let's unmark this. And unmark this. And now if I do an x.ad I now have a new data frame. And you can see right here we have all the


03:52:30
different columns except for the brand at the end of the year. And it turns out when you start playing with the data set, you're going to get an error later on and it'll say cannot convert string to float value. And that's because it for some reason these things the way they recorded them must have been recorded as strings. So we have a neat feature in here on pandas to convert. And it is simply convert objects. And for this, we're going to do convert. Oops. Convert underscore numeric numeric equals true.


03:53:07
And yes, I did have to go look that up. I don't have it memorized. The convert numeric in there. If I'm working with a lot of these things, I remember them, but um depending on where I'm at, what I'm doing, I usually have to look it up. And we run that. Oops, I must have missed something in here. Let me double check my spelling. And when I double check my spilling, you'll see I missed the first underscore in the convert objects. And when I run this, it now has everything converted into a numeric


03:53:32
value because that's what we're going to be working with is numeric values down here. And the next part is that we need to go through the data and eliminate null values. Most people when they're doing small amounts, working with small data pools, discover afterwards that they have a null value and they have to go back and do this. So, you know, be aware whenever we're formatting this data, things are going to pop up and sometimes you go backwards to fix it. And that's fine. That's just part of


03:54:00
exploring the data and understanding what you have. And I should have done this earlier, but let me go ahead and increase the size of my window one notch. There we go. Easier to see. So, we'll do 4 I in working with X dot columns. will page through all the columns and we want to take x of i and we're going to change that we're going to alter it and so with this we want to go ahead and fill in x of i. Pandas has the fill in a and that just fills in any non-existent missing data. And we'll put


03:54:39
my brackets up. And there's a lot of different ways to fill this data. If you have a really large data set, some people just void out that data because if and then look at it later in a separate exploration of data. One of the tricks we can do is we can take our column and we can find the means and the means is in there or quotation marks. So we take the columns, we're going to fill in the non-existing one with the means. The problem is that returns a decimal float. So some of these aren't decimals certainly. Let me


03:55:14
be a little careful of doing this, but for this example, we're just going to fill it in with the integer version of this. Keeps it on par with the other data that isn't a decimal point. And then what we also want to do is we want to double check. A lot of times you do this first part first to double check, then you do the fill, and then you do it again just to make sure you did it right. So, we're going to go through and test for missing data. And one of the re ways you can do that is


03:55:42
simply go in here and take our X of I column. So it's going to go through the X of I column. It says is null. So it's going to return any any place there's a null value. It actually goes through all the rows of each column is null. And then we want to go ahead and sum that. So we take that, we add the sum value. And these are all pandas. So is null is a panda command and so is sum. And if we go through that and we go ahead and run it and we go ahead and take and run that, you'll see that all the columns


03:56:14
have zero null values. So we've now tested and double checked and our data is nice and clean. We have no null values. Everything is now a number value. We turned it into numeric and we've removed the last column in our data. And at this point, we're actually going to start using the elbow method to find the optimal number of clusters. So, we're now actually getting into the sklearn part. Uh, the K means clustering on here. I guess we'll go ahead and zoom it up one more notch so you can see what


03:56:44
I'm typing in here. And then from sklearn going to or sklearn cluster, we're going to import K means. I always forget to capitalize the K and the M when I do this. So those capital K capital M K means and we'll go and create a um array WCSS equals we'll make it an empty array. If you remember from the elbow method from our slide within the sums of squares WSS is defined as the sum of squared distance between each member of the cluster and it centrid. So, we're looking at that


03:57:28
change in differences as far as a squared distance. And we're going to run this over a number of K mean values. In fact, let's go for I in range. We'll do 11 of them. Range zero of 11. And the first thing we're going to do is we're going to create the actual we'll do it all lowercase. And so we're going to create this object from the K means that we just imported. And the variable that we want to put into this is in clusters. We're going to set that equals to I. That's


03:58:09
the most important one because we're looking at how increasing the number of clusters changes our answer. There are a lot of settings to the K means. Our guys in the back did a great job just kind of playing with some of them. The most common ones that you see in a lot of stuff is how you init your K means. So we have K means plus plus. This is just a tool to let the model itself be smart how it picks it centrids to start with its initial centroidids. We only want to iterate no more than 300 times. We have


03:58:42
a max iteration we put in there. We have the infinite the random state equals zero. You really don't need to worry too much about these when you're first learning this. As you start digging in deeper, you start finding that these are shortcuts that will speed up the process as far as a setup. But the big one that we're working with is the inclusters equals I. So, we're going to literally train our K means 11 times. We're going to do this process 11 times. And if you're working with uh big data, you


03:59:12
know, the first thing you do is you run a small sample of the data. So you can test all your stuff on it. And you can already see the problem that if I'm going to iterate through a terabyte of data 11 times and then the K means itself is iterating through the data multiple times. That's a heck of a process. So you got to be a little careful with this. A lot of times though, you can find your elbow using the elbow method, find your optimal number on a sample of data, especially if you're working with larger data


03:59:39
sources. So, we want to go ahead and take our K means and we're just going to fit it. If you're looking at any of the sklearn, very common you fit your model. And if you remember correctly, our variable we're using is the capital X. And once we fit this value, we go back to the array we made, and we want to go and just append that value on the end. And it's not the actual fit we're pinning in there. It's like when it generates it, it generates the value you're looking for is inertia. So K


04:00:08
means inertia will pull that specific value out that we need. And let's get a visual on this. We'll do our plt plot. And what we're plotting here is first the x axis, which is range 0 11. So that will generate a nice little plot there. and the WCSS for our y axis. It's always nice to give our uh plot a title. And let's see, we'll just give it the elbow method for the title. And let's get some labels. So, let's go ahead and do plt x label. And what we'll do, we'll do


04:00:47
number of clusters for that. And plt y label. And for that, we can do oops, there we go. WCSS since that's what we're doing on the plot on there. And finally, we want to go ahead and display our graph, which is simply plt. Oops. Show. There we go. And because we have it set to inline, it'll appear in line. Hopefully, I didn't make a type error on there. And you can see we get a very nice graph. You can see a very nice elbow joint there at uh two. And again, right around three and four. And then


04:01:24
after that, there's not very much. Now, as a data scientist, if I was looking at this, I would do either three or four. And I'd actually try both of them to see what the output look like. And they've already tried this in the back. So, we're just going to use three as a setup on here. And let's go ahead and see what that looks like when we actually use this to show the different kinds of cars. And so, let's go ahead and apply the K means to the cars data set. And basically, we're going to copy the code


04:01:54
that we looped through up above where K means equals K means number of clusters. And we're just going to set the number of clusters to three since that's what we're going to look for. And you can do three and four on this and graph them just to see how they come up differently. It'd be kind of curious to look at that. But for this, we're just going to set it to three. Go ahead and create our own variable Y K means for our answers. And we're going to set that equal to Whoops, that double equal there


04:02:21
to K means, but we're not going to do a fit. We're going to do a fit predict is the setup you want to use. And when you're using untrained models, you'll see um a slightly different because usually you see fit and then you see just the predict, but we want to both fit and predict the K means on this. And that's fit predict. And then our capital X is the data we're working with. And before we plot this data, we're going to do a little pandas trick. We're going to take our x value and we're


04:02:51
going to set x as matrix. So we're converting this into a nice rows and columns kind of setup. And we want the we're going to have columns equals none. So it's just going to be a matrix of data in here. And let's go ahead and run that. A little warning. You'll see this warnings pop up because things are always being updated. So there's like minor changes in the versions and future versions. instead of matrix. Now that it's more common to set it values instead of doing as matrix, but mass


04:03:20
matrix works just fine for right now and you'll want to update that later on. But let's go ahead and dive in and plot this and see what that looks like. And before we dive into plotting this data, I always like to take a look and see what I am plotting. So let's take a look at y k means. I'm just going to print that out down here. And we see we have an array of answers. we have 2 1 0 2 1 2. So it's clustering these different rows of data based on the three different spaces it thinks it's going to


04:03:52
be. And then let's go ahead and print X and see what we have for X. And we'll see that X is an array. It's a matrix. So we have our different values in the array. And what we're going to do, it's very hard to plot all the different values in the array. So, we're only going to be looking at the first two or positions zero and one. And if you were doing a full presentation in front of the board meeting, you might actually do a little different than and dig a little deeper into the different aspects


04:04:24
because this is all the different columns we looked at. But we'll only look at columns one and two for this to make it easy. So, let's go ahead and clear this data out of here and let's bring up our plot. And we're going to do a scatter plot here. So pl scatter and this looks a little complicated. So let's explain what's going on with this. We're going to take the x values and we're only interested in y of k means equals 0, the first cluster. Okay? And then we're going to take value


04:04:56
zero for the x axis. And then we're going to do the same thing here. We're only interested in k means equals zero, but we're going to take the second column. So, we're only looking at the first two columns in our answer or in the data. And then the guys in the back played with this a little bit to make it pretty. And they discovered that it looks good with a size equals 100. That's the size of the dots. We're going to use red for this one. And when they were looking at the data and what came


04:05:26
out, it was definitely the Toyota on this. We're just going to go ahead and label it Toyota. Again, that's something you really have to explore in here as far as playing with those numbers and see what looks good. We'll go ahead and hit enter in there. And I'm just going to paste in the next two lines, which is the next two cars. And this is our Nissa and Honda. And you'll see with our scatter plot, we're now looking at where Y_K means equals 1. And we want the zero column and YK means equals 2. Again,


04:05:56
we're looking at just the first two columns, zero and one. And each of these rows then corresponds to Nissan and Honda. And I'll go ahead and hit enter on there. And uh finally, let's take a look and put the centrids on there. Again, we're going to do a scatter plot. And on the centrids, you can just pull that from our K means the uh model we createdcluster centers. And we're going to just do um all of them in the first number and all of them in the second number which is 01 because you always start with 0


04:06:31
and one. And then they were playing with the size and everything to make it look good. We'll do a size of 300. We're going to make the color yellow. And we'll label them. It's always good to have some good labels. Centroidids. And then we do want to do a title. PLT title. and pop up there. PLT title. So you always make want to make your graphs look pretty. We'll call it clusters of car make. And one of the features of the plot library is you can add a legend. It'll automatically bring in


04:07:06
it since we've already labeled the different aspects of the legend with Toyota, Nissan, and Honda. And finally, we want to go ahead and show so we can actually see it. And remember, it's in line. Uh, so if you're using a different editor that's not the Jupyter notebook, you'll get a popup of this and you should have a nice set of clusters here. So we can look at this and we have a clusters of Honda in green, Toyota and red, Nissan and purple and you can see where they put the


04:07:32
centroidids to separate them. Now when we're looking at this, we can also plot a lot of other different data on here as far because we only looked at the first two columns. This is just column one and two or 01 as as you label them in computer scripting. But you can see here we have a nice clusters of car making and we were able to pull out the data and you can see how just these two columns form very distinct clusters of data. So if you were exploring new data you might take a look and say well what makes these different


04:08:03
almost going in reverse you start looking at the data and pulling apart the columns to find out why is the first group set up the way it is. Maybe you're doing loans and you want to go, well, why is this group not defaulting on their loans and why is the last group defaulting on their loans? And why is the middle group 50% defaulting on their bank loans? And you start finding ways to manipulate the data and pull out the answers you want. So now that you've seen how to use K mean for clustering, let's move on to


04:08:34
the next topic. Now let's look into logistic regression. The logistic regression algorithm is the simplest classification algorithm used for binary or multiclassification problems. And we can see we have our little girl from Canada who's into horror books is back. That's actually really scary when you think about that with those big eyes. In the previous tutorial, we learned about linear regression, dependent and independent variables. So to brush up, y= mx + c. Very basic algebraic function


04:09:05
of uh y and x. The dependent variable is the target class variable we are going to predict. The independent variables X1 all the way up to XN are the features or attributes we're going to use to predict the target class. We know what a linear regression looks like. But using the graph, we cannot divide the outcome into categories. It's really hard to categorize 1.5, 3.6, 9.8. Uh for example, a linear regression graph can tell us that with increase in number of hours studied, the marks of a student


04:09:40
will increase, but it will not tell us whether the student will pass or not. In such cases where we need the output as categorical value, we will use logistic regression. And for that, we're going to use the sigmoid function. So you can see here we have our marks 0 to 100, number of hours studied. That's going to be what they're comparing it to in this example. And we usually form a line that says y = mx + c. And when we use the sigmoid function, we have p = 1 / 1 + e the minus y, it generates a sigmoid


04:10:11
curve. And so you can see right here when you take the ln, which is the natural logarithm. I always thought it should be nl, not ln. That's just the inverse of uh e to the minus y. And so we do this, we get ln of p 1 - p = m * x + c. That's the sigmoid curve function we're looking for. And we can zoom in on the function and you'll see that the function as it deres goes to one or to zero depending on what your x value is. And the probability if it's greater than 0.5, the value is automatically rounded


04:10:47
off to one indicating that the student will pass. So if they're doing a certain amount of studying, they will probably pass. Then you have a threshold value at the 0.5. It automatically puts that right in the middle usually. And your probability if it's less than 0.5, the value run it off to zero indicating the student will fail. So if they're not studying very hard, they're probably going to fail. This, of course, is ignoring the outliers of that one student who's just a natural genius and


04:11:11
doesn't need any studying to memorize everything. That's not me, unfortunately. Have to study hard to learn new stuff. problem statement to classify whether a tumor is malignant or B9. And this is actually one of my favorite data sets to play with because it has so many features and when you look at them, you really are hard to understand. You can't just look at them and know the answer. So it gives you a chance to kind of dive into what data looks like when you aren't able to understand the specific domain of the


04:11:40
data. But I also want you to remind you that in the domain of medicine, if I told you that my probability was really good at classified things that say 90% or 95% and I'm classifying whether you're going to have a malignant or a B9 tumor, I'm guessing that you're going to go get it tested anyways. So you got to remember the domain we're working with. So why would you want to do that if you know you're just going to go get a biopsy? Because you know it's that serious. This is like an all or nothing.


04:12:09
just referencing the domain. It's important. It might help the doctor know where to look just by understanding what kind of tumor it is. So it might help them or aid them on something they missed from before. So let's go ahead and dive into the code and I'll come back to the domain part of it in just a minute. So use case and we're going to do our normal imports here where we're importing numpy, pandas, seabour, the mattplot library and we're going to do mattplot library in line since I'm going


04:12:37
to switch over to anaconda. So, let's go ahead and flip over there and get this started. So, I've opened up a new window in my Anaconda Jupyter Notebook. And by the way, Jupyter Notebook, uh, you don't have to use Anaconda for the Jupyter Notebook. I just love the interface and all the tools that Anaconda brings. So, we got our import numpy aspy number array. We have our pandas pd. We're going to bring in Seabor to help us with our graphs as SNS. So many really nice tools in both Seabour and


04:13:07
Mattplot library. And we'll do our mapplot library.pipplot as plt. And then of course we want to let it know to do it in line. And let's go and just run that. So it's all set up. And we're just going to call our data data. Not creative today. Uh equals pd. And this happens to be in a CSV file. So we'll use a pdread_csv. And I happen to name the file. I renamed it data forp2.csv. You can of course um write in the comments below the YouTube and request for the data set itself or go to


04:13:41
the simply learn website and we'll be happy to supply that for you. And let's just um open up the data before we go any further and let's just see what it looks like in a spreadsheet. So when I pop it open in a local spreadsheet and this is just a CSV file, comma separated variables. We have an ID. So I guess the U categorizes for reference or what ID which test was done. The diagnosis M for malignant, B for B9. So there's two different options on there. And that's what we're going to


04:14:10
try to predict is the M and B and test it. And then we have like the radius mean or average the texture average, perimeter mean, area mean, smoothness. I don't know about you, but unless you're a doctor in the field, most of the stuff, I mean, you can guess what concave means just by the term concave, but I really wouldn't know what that means in the measurements they're taking. So, they have all kinds of stuff like how smooth it is, uh, the symmetry, and these are all float values. You just


04:14:38
page through them real quick, and you'll see there's, I believe, 36, if I remember correctly, in this one. So there's a lot of different values they take and all these measurements they take when they go in there and they take a look at the different growth, the tumorous growth. So back in our data and I put this in the same folder as a code. So I saved this code in that folder. Obviously if you have it in a different location, you want to put the full path in there and we'll just do uh


04:15:06
pandas first five lines of data with the data head. We run that. We can see that we have pretty much what we just looked at. We have an ID. We have a diagnosis. If we go all the way across, you'll see all the different columns coming across displayed nicely for our data. And while we're exploring the data, our uh Seabor, which we referenced as SNS, makes it very easy to go in here and do a joint plot. You'll notice the very similar to because it is sitting on top of the U plot library. So, the joint


04:15:41
plot does a lot of work for us. And we're just going to look at the first two columns that we're interested in, the radius mean and the texture mean. We'll just look at those two columns and data equals data. So that tells it which two columns we're plotting and that we're going to use the data that we pulled in. Let's just run that and it generates a really nice graph on here. And there's all kinds of cool things on this graph to look at. I mean, we have the texture mean and the radius mean


04:16:07
obviously the axes. You can also see and uh one of the cool things on here is you can also see the histogram. They show that for the radius mean where is the most common radius mean come up and where the most common texture is. So we're looking at the tech the on each growth it's average texture and on each radius it's average uh radius on there gets a little confusing because we're talking about the individual objects average. And then we can also look over here and see the the histogram showing


04:16:38
us the median or how common each measurement is. And that's only two columns. So let's dig a little deeper into Seabor. They also have a heat map. And if you're not familiar with heat maps, a heat map just means it's in color. That's all that means. Heat map. I guess the original ones were plotting heat density on something. And so ever since then it's just called a heat map. And we're going to take our data and get our corresponding numbers to put that into the heat map. And that's simply


04:17:08
data.coR for that. That's a pandas expression. Let's remember we're working in a pandas data frame. So that's one of the cool tools in pandas for our data. And let's just pull that information into a heat map and see what that looks like. And you'll see that we're now looking at all the different features. We have our ID. We have our texture. We have our area, our compactness, concave points. And if you look down the middle of this chart diagonal going from the upper left to bottom right, it's all


04:17:37
white. That's because when you compare texture to texture, they're identical. So they're 100% or in this case perfect one in their correspondence. And you'll see that when you look at say area or right below it, it has almost a black on there. when you compare it to texture. So these have almost no corresponding data. They don't really form a linear graph or something that you can look at and say how connected they are. They're very scattered data. This is really just a really nice graph to get a quick look at


04:18:08
your data. Doesn't so much change what you do, but it changes verifying. So when you get an answer or something like that or you start looking at some of these individual pieces, you might go, "Hey, that doesn't match. According to showing our heat map, this should not correlate with each other. And if it is, you're going to have to start asking, well, why? What's going on? What else is coming in there? But it does show some really cool information on here. I mean, we can see from the ID, there's no real


04:18:36
one feature that just says if you go across the top line that lights up. There's no one feature that says, hey, if the area is a certain size, then it's going to be B9 or malignant. It says there's some that sort of add up and that's a big hint in the data that we're trying to ID this whether it's malignant or B9. That's a big hint to us as data scientists to go okay we can't solve this with any one feature. It's going to be something that includes all the features or many of the different


04:19:04
features to come up with a solution for it. And while we're exploring the data let's explore one more area and let's look at data isisnull. We want to check for null values in our data. If you remember from earlier in this tutorial, we did it a little differently where we added stuff up and sum them up. You can actually with pandas do it really quickly. Data.isnull and summit. And it's going to go across all the columns. So when I run this, you're going to see all the columns come up with no null


04:19:37
data. So we've just just to rehash these last few steps. We've done a lot of exploration. We have looked at the first two columns and seen how they plot with the seabour with a joint plot which shows both the histogram and the data plotted on the XY coordinates. And obviously you can do that more in detail with different columns and see how they plot together. And then we took and did the Seabor heat map the SNS heat mapap of the data. And you can see right here where it did a nice job showing us some bright spots where stuff


04:20:14
correlates with each other and forms a very nice combination or points of scattering points. And you can also see areas that don't. And then finally, we went ahead and checked the data. Is the data null value? Do we have any missing data in there? Very important step because it'll crash later on. If you forget to do this step, it will remind you when you get that nice error code that says null values. Okay. So, not a big deal if you miss it, but it it's no fun having to go back when you're when you're in a huge


04:20:44
process and you've missed this step and now you're 10 steps later and you got to go remember where you were pulling the data in. So, we need to go ahead and pull out our X and our Y. So, we just put that down here and we'll set the X equal to. And there's a lot of different options here. Certainly we could do X equals all the columns except for the first two because if you remember the first two is the ID and the diagnosis. So that certainly would be an option. But what we're going to do is we're actually


04:21:13
going to focus on the worst. The worst radius, the worst texture, parameter area, smoothness, compactness, and so on. One of the reasons to start dividing your data up when you're looking at this information is sometimes the data will be the same data coming in. So if I have two measurements coming into my model, it might overweigh them. It might overpower the other measurements because it's measuring it's basically taking that information in twice. That's a little bit past the scope of this


04:21:44
tutorial. I want you to take away from this though is that we are dividing the data up into pieces and our team in the back went ahead and said hey let's just look at the worst. So I'm going to create a an array and you'll see this array radius worst texture worst perimeter worst. We've just taken the worst of the worst and I'm just going to put that in my X. So this X is still a pandas data frame but it's just those columns. And our Y, if you remember correctly, is going to be Oops. Hold on


04:22:13
one second. It's not X. is data. There we go. So, x equals data and then it's a list of the different columns, the worst of the worst. And if we're going to take that, then we have to have our answer for our y for the stuff we know. And if you remember correctly, we're just going to be looking at the diagnosis. That's all we care about is what is it diagnosed? Is it B9 or malignant? And since it's a single column, we can just do diagnosis. Oh, I forgot to put the brackets. There we go.


04:22:42
Okay. So, it's just diagnosis on there. And we can also real quickly do like an X do. If you want to see what that looks like and Y head and run this and you'll see um it only does the last one. I forgot about that. If you don't do print, you can see that the the Y.D is just mm because the first ones are all malignant. And if I run this the X do head, it's just the first five values of radius worst, texture worst, parameter worst, area worst, and so on. I'll go ahead and take


04:23:14
that out. So, moving down to the next step, we've built our two data sets, our answer and then the features we want to look at. In data science, it's very important to test your model. So we do that by splitting the data and from sklearn model selection we're going to import train test split. So we're going to split it into two groups. There are so many ways to do this. I noticed in one of the more modern ways they actually split it into three groups and then you model each group and test it against the other


04:23:51
groups. So you have all kinds and there's reasons for that which is past the scope of this and for this particular example isn't necessary for this. We're just going to split it into two groups. one to train our data and one to test our data. And the sklearn uh selection we have train tests split. You could write your own quick code to do this where you just randomly divide the data up into two groups. But they do it for us nicely and we actually can almost we can actually do it in one statement with


04:24:20
this where we're going to generate four variables. Capital X train capital X test. So we have our training data we're going to use to fit the model and then we need something to test it and then we have our y train. So we're going to train the answer and then we have our test. So this is the stuff we want to see how good it did on our model. And we'll go ahead and take our train test split that we just imported. And we're going to do X and our Y, our two different data that's


04:24:47
going in for our split. And then the guys in the back came up and wanted us to go ahead and use a test size equals.3. That's test size. Random state. It's always nice to kind of switch a random state around, but not that important. What this means is that the test size is we're going to take 30% of the data and we're going to put that into our test variables, our Y test and our X test. And we're going to do 70% into the X train and the Y train. So, we're going to use 70% of the data to


04:25:17
train our model and 30% to test it. Let's go ahead and run that and load those up. So now we have all our stuff split up and all our data ready to go. And now we get to the actual logistics part. We're actually going to do our create our model. So let's go ahead and bring that in from sklearn. We're going to bring in our linear model and we're going to import logistic regression. That's the actual model we're using. And let's we'll call it log model. Oops, there we go. Model. And


04:25:46
let's just set this equal to our logistic regression that we just imported. So now we have a variable log model set to that class for us to use. And with most the uh models in the sklearn, we just need to go ahead and fix it. Fit do a fit on there. And we use our x train that we separated out with our y train. And let's go ahead and run this. So once we've run this, we'll have a model that fits this data, that 70% of our training data. Uh, and of course it prints this out that tells us all the different


04:26:20
variables that you can set on there. There's a lot of different choices you can make, but for Word do, we're just going to let all the defaults set. We don't really need to mess with those on this particular example. And there's nothing in here that really stands out as super important until you start fine-tuning it. But for what we're doing, the basics will work just fine. And then let's we need to go ahead and test out our model. Is it working? So let's create a variable Y predict. And


04:26:46
this is going to be equal to our log model. And we want to do a predict. Again, very standard format for the sklearn library is taking your model and doing a predict on it. And we're going to test y predict against the y test. So we want to know what the model thinks it's going to be. That's what our y predict is. And with that, we want the capital xx test. So we have our train set and our test set. And now we're going to do our y predict. And let's go ahead and run that. And if we uh


04:27:20
print y predict, let me go ahead and run that. You'll see it comes up and it predents a prints a nice array of uh B and M for B9 and malignant for all the different test data we put in there. So, it does pretty good. We're not sure exactly how good it does, but we can see that it actually works and is functional. Was very easy to create. You'll always discover with our data science that as you explore this, you spend a significant amount of time prepping your data and making sure your data coming in is good. Uh there's


04:27:55
a saying, good data in, good answers out. Bad data in, bad answers out. That's only half the thing. That's only half of it. Selecting your models becomes the next part as far as how good your models are. and then of course fine-tuning it depending on what model you're using. So we come in here, we want to know how good this came out. So we have our Y predict here, log model.predict X test. So for deciding how good our model is, we're going to go from the sklearn.metrics, we're going to import


04:28:29
classification report. And that just reports how good our model is doing. And then we're going to feed it the model data. And let's just print this out. and we'll take our uh classification report and we're going to put into there our test our actual data. So this is what we actually know is true and our prediction what our model predicted for that data on the test side. And let's run that and see what that does. So we pull that up. You'll see that we have um a precision for B9 and


04:29:05
malignant B and M. And we have a precision of 93 at 91, a total of 92. So it's kind of the average between these two of 92. There's all kinds of different information on here. Your F1 score, your recall, your support coming through on this. And for this, I'll go ahead and just flip back to our slides that they put together for describing it. And so here we're going to look at the precision using the classification report. And you see this is the same print out I had up above. Some of the


04:29:35
numbers might be different because it does randomly pick out which data we're using. So this model is able to predict the type of tumor with 91% accuracy. So we look back here that's you will see where we have uh B9 and malignant. It actually has 92 coming up here. We're looking about a 92 91% precision. And remember I reminded you about domain. So, when we're talking about the domain of a medical domain with a very catastrophic outcome, you know, at 91 or 92% precision, you're still going to go


04:30:06
in there and have somebody do a biopsy on it. Very different than if you're investing money and there's a 92% chance you're going to earn 10% and 8% chance you're going to lose 8%, you're probably going to bet the money because at that odds, it's pretty good that you'll make some money. And in the long run, you do that enough, you definitely will make money. And also with this domain, I've actually seen them use this to identify different forms of cancer. That's one of


04:30:31
the things that they're starting to use these models for because then it helps a doctor know what to investigate. So that wraps up this section. We're finally we're going to go in there and let's discuss the answer to the quiz asked in machine learning tutorial part one. Can you tell what's happening in the following cases? Grouping documents into different categories based on the topic and content of each document. This is an example of clustering where K means clustering can be used to group the


04:30:58
documents by topics using bag of words approach. So if you gotten in there that you're looking for clustering and hopefully you had at least one or two examples like K means that are used for clustering different things then give yourself a two thumbs up. B identifying handwritten digits in images correctly. This is an example of classification. The traditional approach to solving this would be to extract digit dependent features like curvature of different digits etc. and then use a classifier


04:31:27
like SVM to distinguish between images. Again, if you got the fact that it's a classification example, give yourself a thumb up. And if you're able to go, hey, let's use SVM or another model for this, give yourself those two thumbs up on it. C. Behavior of a website indicating that the site is not working as designed. This is an example of anomaly detection. In this case, the algorithm learns what is normal and what is not normal, usually by observing the logs of the website. Give yourself a thumbs up if


04:31:58
you got that one. And just for a bonus, can you think of another example of anomaly detection? One of the ones I use it for in my own business is detecting anomalies in stock markets. Stock markets are very fickled and they behave very erratic. So finding those erratic areas and then finding ways to track down why they're erratic. Was something released in social media? Was something released you can see where knowing where that anomaly is can help you to figure out what the answer is to it in another


04:32:25
area. D predicting salary of an individual based on his or her years of experience. This is an example of regression. This problem can be mathematically defined as a function between independent years of experience and dependent variables salary of an individual. And if you guess that this was a regression model, give yourself a thumbs up. And if you were able to remember that it was between independent and dependent variables and that terms, give yourself two thumbs up. Summary. So to wrap it up, we went over what is K


04:32:57
means and we went through also the chart of choosing your elbow method and assigning a random centrid to the clusters, computing the distance and then going in there and figuring out what the minimum centroidids is and computing the distance and going through that loop until it gets the perfect centrid. And we looked into the elbow method to choose K based on running our clusters across a number of variables and finding the best location for that. We did a nice example of clustering cars with K means even though we only looked


04:33:26
at the first two columns to make it simple and easy to graph. You can easily extrapolate that and look at all the different columns and see how they all fit together. And we looked at what is logistic regression. We discussed the sigmoid function. What is logistic regression? And then we went into an example of classifying tumors with logistics. I hope you enjoyed part two of machine learning. Why reinforcement learning? Training a machine learning model requires a lot of data which might not always be available to us. Further,


04:33:57
the data provided might not be reliable. Learning from a small subset of actions will not help expand the vast realm of solutions that may work for a particular problem. And you can see here we have the robot learning to walk. U very complicated setup when you're learning how to walk. And you'll start asking questions like if I'm taking one step forward and left, what happens if I pick up a 50 lb object? How does that change how a robot would walk? These things are very difficult to program because


04:34:26
there's no actual information on it until the it's actually tried out. Learning from a small subset of actions will not help expand the vast realm of solutions that may work for a particular problem. And we'll see here it learned how to walk. This is going to slow the growth that technology is capable of. Machines need to learn to perform actions by themselves and not just learn off humans. And you see the objective climb a mountain. Real interesting point here is that as human beings, we can go into


04:34:59
a very unknown environment and we can adjust for it and kind of explore and play with it. Most of the models, the non-reinforcement models in computer u machine learning aren't able to do that very well. Uh there's a couple of them that can be used or integrated. See how it goes is what we're talking about with reinforcement learning. So what is reinforcement learning? Reinforcement learning is a subbranch of machine learning that trains a model to return an optimum solution for a problem by


04:35:32
taking a sequence of decisions by itself. Consider a robot learning to go from one place to another. The robot is given a scenario must arrive at a solution by itself. The robot can take different paths to reach the destination. It will know the best path by the time taken on each path. It might even come up with a unique solution all by itself. And that's really important as we're looking for unique solutions. Uh we want the best solution, but you can't find it unless you try it. So when


04:36:01
we're looking at uh our different systems, our different model, we have supervised versus unsupervised versus reinforcement learning. And with the supervised learning, that is probably the most controlled environment. Uh we have a lot of different supervised learning models, whether it's linear regression, neural networks, um there's all kinds of things in between, decision trees. The data provided is labeled data with output values specified. And this is important because when we talk about


04:36:29
supervised learning, you already know the answer for all this information. You already know the picture has a motorcycle in it. So you're supervised learning. You already know that um the outcome for tomorrow for you know going back a week. You're looking at stock. You can already have like the graph of what the next day looks like. So you have an answer for it. And you have labeled data which is used. You have an external supervision and solves problems by mapping labeled input to known output. So very


04:36:59
controlled unsupervised learning and unsupervised learning is really interesting because it's now taking part in many other models. They start with an you can actually insert an unsupervised learning model um in almost either supervised or reinforcement learning as part of the system which is really cool. Uh data provided is unlabeled data. The outputs are not specified. machine makes its own predictions used to solve association with clustering problems. Unlabeled data is used. No supervision.


04:37:30
Solves problems by understanding patterns and discovering output. Uh so you can look at this and you can think um some of these things go with each other. They belong together. So it's looking for what connects in different ways. And there's a lot of different algorithms that look at this. Um when you start getting into those there's some really cool images that come up of what unsupervised learning is. How it can pick out say uh the area of a donut. One model will see the area of the donut and the other one will


04:38:00
divide it into three sections based on its location versus what's next to it. So there's a lot of stuff that goes in with unsupervised learning. And then we're looking at reinforcement learning. Probably the biggest industry in today's market uh in machine learning or growing market. It's very in its very infant stage uh as far as how it works and what it's going to be capable of. The machine learns from its environment using rewards and errors used to solve rewardbased problems. No predefined data


04:38:29
is used. No supervision follows trail and error problem solving approach. Uh so again we have a random at first you start with a random I try this it works and this is my reward. doesn't work very well maybe or maybe it doesn't even get you where you're trying to get it to do and you get your reward back and then it looks at that and says well let's try something else and it starts to play with these different things finding the best route. So let's take a look at important terms in today's reinforcement


04:38:57
model and this has become pretty standardized over the last uh few years so these are really good to know. We have the agent uh agent is the model that is being trained via reinforcement learning. So this is your actual uh entity that has however you're doing it whether you're using a neural network or a Q table or whatever combination thereof. This is the actual agent that you're using. This is the model and you have your environment. Uh the training situation that the model must optimize to is called its


04:39:30
environment. Uh and you can see here I guess we have a robot who's trying to get a chest full of gems or whatever. And that's the output. And then you have your action. This is all possible steps that can be taken by the model and it picks one action and you can see here it's picked three different uh routes to get to the chest of diamonds and gems. We have a state the current position condition returned by the model. And you could look at this uh if you're playing like a video game. This


04:39:59
is the screen you're looking at. Uh so when you go back here uh the environment is a whole game board. So, if you're playing one of those Mobius games, you might have the whole game board going on. Uh, but then you have your current position. Where are you on that game board? What's around that? What's around you? Um, if you were talking about a robot, the environment might be moving around the yard, where it is in the yard, and what it can see, what input it has in that location. That would be the


04:40:28
current position condition returned by the model. And then the reward uh to help the model move in the right direction. It is rewarded. Points are given to it to appraise some kind of action. So yeah, you did good or if uh didn't do as good trying to maximize the reward and have the best reward possible. And then policy. Policy determines how an agent will behave at any time. It acts as a mapping between action and present state. This is part of the model. What what is your action that you're you're going to take? what's


04:40:59
the policy you're using to have an output from your agent. One of the reasons they separate a policy as its own entity is that you usually have a prediction um of a different options and then the policy well how am I going to pick the best based on those predictions I'm going to guess at different options and we'll actually weigh those options in and find the best option we think will work. Uh, so it's a little tricky, but the policy thing is actually pretty cool how it works. Let's go ahead and


04:41:31
take out look at a reinforcement learning example. And just in looking at this, we're going to take a look uh consider what a dog um that we want to train. Uh so the dog would be like the agent. So you have your your puppy or whatever. Uh and then your environment is going to be the whole house or whatever it is where you're training them. And then you have an action. We want to teach the dog to fetch. So action equals fetching. Uh and then we have a little biscuit. So we can get the dog to perform various actions by


04:42:01
offering incentives such as a dog biscuit as a reward. The dog will follow a policy to maximize this reward and hence will follow every command and might even learn new actions like begging by itself. Uh so you have b you know so we start off with fetching. It goes oh I get a biscuit for that. It tries something else. and you get a handshake or begging or something like that and it goes oh this is also reward-based and so it kind of explores things to find out what will bring it as biscuit and that's


04:42:30
very much like how a reinforced model goes is it uh looks for different rewards how do I find can I try different things and find a reward that works the dog also will want to run around and play and explore it environment uh this quality of model is called exploration so there's a little randomness going on in exploration and explores new parts of the house. Climbing on the sofa doesn't get a reward. In fact, it usually gets kicked off the sofa. So, let's talk a little bit about Marov's decision process. Uh, Marov's


04:43:05
decision process is a reinforcement learning policy used to map a current state to an action where the agent continuously interacts with the environment to produce new solutions and receive rewards. And you'll see here's all of our different uh uh vocabulary we just went over. We have a reward, our state, our agent, our environment, and our action. And so even though the environment kind of contains everything um that you you really when you're actually writing the program, your environment is going to put out a reward


04:43:36
and state that goes into the agent. Uh the agent then looks at this uh state or it looks at the reward usually um first and it says okay I got rewarded for whatever I just did or I didn't get rewarded and then it looks at the state and then it comes back and if you remember from policy the policy comes in um and then we have a reward. The policy is that part that's connected at the bottom. And so it looks at that policy and it says, "Hey, what's a good action that will probably be similar to what I


04:44:09
did?" Or um uh sometimes they're completely random, but what's a good action that's going to bring me a different reward? So, taking the time to just understand these different pieces as they go is pretty important in most of the models today. Um, and so a lot of them actually have templates based on this that you can pull in and start using. Um, pretty straightforward as far as once you start seeing how it works. Uh, you can see your environment sends it says, "Hey, this is the agent did


04:44:40
this. If you're a character in a game, this happened and it shoots out a reward in a state." The agent looks at the reward, looks at the new state, and then takes a little guess and says, "I'm going to try this action." And then that action goes back into the environment. it affects the environment. The environment then changes depending on what the action was and then it has a new state and a new reward that goes back to the agent. So in the diagram shown, we need to find the shortest path


04:45:07
between node A and D. Each path has a reward associated with it and the path with a maximum reward is what we want to choose. The nodes A, B, C, Denote the nodes to travel from node uh A to B is an action. Reward is the cost of each path and policy is each path taken. And you can see here A can go uh to B or A can go to C right off the bat or it can go right to D. And if you explored all three of these uh you would find that A going to D was a zero reward. Um A going to C and D would generate a different reward. Or you


04:45:45
could go AC B D. There's a lot of options here. Um and so when we start looking at this diagram, you start to realize that even though uh today's reinforced learning models do really good at um finding an answer, they end up trying almost all the different directions you see. And so they take up a lot of work uh or a lot of processing time for reinforcement learning. They're right now in their infant stage and they're really good at solving simple problems. And we'll take a look at one


04:46:15
of those in just a minute in a tic-tac-toe game. Uh but you can see here uh once it's gone through these and it's explored, it's going to find the AC D is the best reward. It gets a full 30 points for it. So let's go ahead and take a look at a reinforcement learning demo. Uh in this demo, we're going to use reinforcement learning to make a tic-tac-toe game. You'll be playing this game against the machine learning model. And we'll go ahead and we're doing it in Python. So, let's go ahead


04:46:44
and go through um I always uh not always actually have a lot of Python tools. Let's go through um Anaconda, which will open up a Jupyter notebook. Seems like a lot of steps, but it's worth it to keep all my stuff separate, and it's also has a nice display when you're in the Jupyter notebook for doing Python. So, here's our Anaconda Navigator. I open up the notebook, which is going to take me to a web page. And I've gone in here and created a new uh Python folder. In this case, I've


04:47:12
already done it and enabled it. Change the name to tic-tac-toe. Uh, and then for this example, uh, we're going to go ahead and import a couple things. We're going to, um, import numpy as np. We'll go ahead and import pickle. Numpy, of course, is our number array. And, uh, pickle is just a nice way sometimes for storing, uh, different information, uh, different states that we're going to go through on here. Uh, and so we're going to create a class called state. We're going to start


04:47:41
with that. And there's a lot of uh lines of code to this uh class that we're going to put in here. Don't let that scare you too much. There's not as much here. Uh it looks like there's going to be a lot here, but there really is just a lot of setup going on in the in our class state. And so we have up here, we're going to initialize it. Um we have our board. Um, it's a tic-tac-toe board, so we're only dealing with nine spots on the board. Uh, we have player one, player two,


04:48:13
uh, is end. We're going to create a board hash. U, we'll look at that in just a minute. We're just going to store some information in there. Symbol of player equals one. Um, so there's a few things going on as far as the initialization. Uh, then something simple. We're just going to get the hash um of the board. You get the information from the board on there, which is uh columns and rows. We want to know when a winner occurs. Uh so if you get three in a row, that's what this whole section


04:48:40
here is for. Uh let me go ahead and scroll up a little bit. And you can get a copy of this code if you send a note over to SimplyLearn. We'll send you over u this particular file and you can play with it yourself and see how it's put together. I don't want to spend a huge amount of time on this uh because this is just some real general Python coding. Uh but you can see here we're just going through all the rows and you add them together and if it equals three, three in a row. Same thing with


04:49:12
columns. Uh diagonal. So you got to check the diagonal. That's what all this stuff does here is it just goes through the different areas. Actually, let me go ahead and put There we go. Um, and then it comes down here and we do our sum and it says true uh minus three. It just says did somebody win or is it a tie? So, you got to add up all the numbers on there anyway just in case they're all filled up. And next, we also need to know available positions. Um, these are ones that don't no one's ever


04:49:42
used before. This way, when you try something or the computer tries something, uh, it's not going to give it an illegal move. That's what the available positions is doing. Uh then we want to update our state. And so you have your position going in. We're just sending in the position that you just chose. And you'll see there's a little user interface we put in there. You p pick the row and column in there. And again, I mean, this is a lot of code. Uh so really it's kind of a


04:50:12
thing you'd want to go through and play with a little bit and just read through it, get a copy of it. Uh great way to understand how this works. And here is a given reward. Um, so we're going to give a reward. Result equals self-winner. This is one of the hearts of what's going on here. Uh, is we have a result self.winner. So if there's a winner, then we have a result. If the result equals one, here's our feedback. Uh, if it doesn't equal one, then it gets a zero. So it only gets a reward in this


04:50:44
particular case if it wins. And that's important to know because different uh systems of reinforced learning do rewarding a lot differently depending on what you're trying to do. This is a very simple example with a 3x3 board. Imagine if you're playing a video game. Uh certainly you only have so many actions, but your environment is huge. You have a lot going on in the environment. And suddenly a reward system like this is going to be just um is going to have to change a little bit.


04:51:17
it's going to have to have different rewards and different setup. And there's all kinds of advanced ways to do that as far as weighing you add weights to it. And so they can add the weights up depending on where the reward comes in. So it might be that you actually get a reward. In this case, you get the reward at the end of the game. And I'm spending just a little bit of time on this because this is an important thing to note. But there's different ways to add up those rewards. it might have like if


04:51:43
you take a certain path um the first reward is going to be weighed a little bit less than the last reward because the last reward is actually winning the game or scoring or whatever it is. So this reward system gets really complicated on some of the more advanced uh setups. Um, in this case though, you can see right here that they give a a 0.1 and a 0.5 reward um just for getting a picking the right value and something that's actually valid instead of picking an invalid value. So, rewards again, that's like


04:52:17
key. It's huge. How do you feed the rewards back in? Uh, then we have a board reset. That's pretty straightforward. It just goes back and resets the board to the beginning because it's going to try out all these different things while it's learning. It's going to do it by trial and error. So, you have to keep resetting it. And then, of course, there's the play. We want to go ahead and play uh rounds equals 100. Depends on what you want to do on here. Um you can set this different. You obviously set that to


04:52:44
higher level, but this is just going to go through and you'll see in here uh that we have player one and player two. This is this is the computer playing itself. Uh, one of the more powerful ways to learn to play a game or even learn something that isn't a game is to have two of these models that are basically trying to beat each other. And so they always they keep finding explore new things. This one works for this one. So this one tries new things, it beats this. We've seen this in um chess, I


04:53:16
think, was a big one where they had the two players in chess with reinforcement learning. uh is one of the ways they train one of the top um computer chess playing algorithms. Uh so this is just what this is. It's going to choose an action. It's going to try something and the more it tries stuff um the more we're going to record the hash. We actually have a board hash where they self get the hash set up on here where it stores all the information. And then once you get to a win, one of them wins, it gets the


04:53:46
reward. Uh then we go back and reset and try again. And then kind of the fun part we actually get down here is uh we're going to play with a human. So we'll get a chance to come in here and see what that looks like when you put your own information in. And then it just comes in here and does the same thing it did above. It gives it a reward for its things um or sees if it wins or ties um looks at available positions, all that kind of fun stuff. And then finally, we want to show the board. Uh so it's going


04:54:16
to print the board out each time. Really um as an integration is not that exciting. What's exciting uh in here is one looking at this reward system. Whoops. Play one more up. The reward system is really the heart of this. How do you reward the different uh setup and the other one is when it's playing it's got to take an action. And so what it chooses for an action is also the heart of reinforcement learning. How do we choose that action? And those are really key to right now where reinforcement learning is uh in today's


04:54:55
uh technology is uh figuring this out. How do we reward it and how do we guess the next best action? So we have our uh environment and you can see the environment is we're going to be or the state uh which is kind of like what's going on. We're going to return the state depending on what happens. And we want to go ahead and create our agent. Uh in this case, our player. So each one is let me go and grab that. And so we look at a class player. Um this is where a lot of the magic is really going on is what how is this


04:55:28
player figuring out how to maneuver around the board? And then the board of course returns a state uh that it can look at and a reward. Uh so we want to take a look at this. We have a name uh self state. This is class player. And when you say class player, we're not talking about a human player. We're talking about um just a uh the computer players. And this is kind of interesting. So remember I told you depending on what you're doing, there's going to be a decay gamma. Um explore rate. Uh these are what I'm


04:55:58
talking about is how do we train it? Um as you try different moves, it gets to the end. The first move is important, but it's not as as important as the last one. And so you could say that the last one has the heaviest weight. And then as you as you get there, the first one, let's see, the first move gives you a five reward, the second gives you a two reward, and the third one gives you a 10 reward because that's the final ending. You got it. The 10's going to count more than the first step. Uh, and here's our


04:56:29
uh we're going to, you know, get the board information coming in and then choose an action. This was the second part that I was talking about that was so important. Uh so once you have your training going on, we have to do a little randomness. And you can see right here is our NP random uh uniform. So it's picking out a random number. Take a random action. This is going to just pick which row and which column it is. Um and so choosing the action. This one you can see we're just doing random


04:57:00
states. uh choice, length of positions, action position, and then it skips in there and takes a look at the board uh for P and positions, and you it's actually storing the different boards each time you go through. So, it has a record of what it did so it can properly weigh the values. And this simply just appends a hash state. What's the last state? Pinned it to the uh u to our states on here. Here's our feedback reward. the reward comes in and it's going to take a look at this and


04:57:29
say is it none uh what is the reward and here is that formula remember I was telling you about up here um that was important because it has decay gamma times the reward this is where as it goes through each step and this is really important this is this is kind of the heart of this of what I was talking about earlier uh you have step one and And this might have a a reward of two. You have step two. I probably should have done ABC. This has a step three. Uh step four. So on till you get to step in. And


04:58:09
this might have a reward of 10. Uh so reward of 10. We're going to add that. But we're not adding uh let's say this one right here. Uh let's say this reward here right before 10 was um let's say it's also 10. That just makes the the uh math easy. So we had 10 and 10. Uh we had 10. This is 10 and 10 n whatever it is. But it's time it's 0.9. Uh so instead of putting a full 10 here, we only do nine. That's uh 0.9 time 10. And so this formula um as far as the decay times the reward minus the cell


04:58:56
state value uh it basically adds in it says here's one or here's two. I'm sorry I should have done this ABC would have been easier. Uh so the first move goes in here and it puts two in here. Uh then we have our self uh setup on here. You can see how this gets pretty complicated in the math, but this is really the key is how do we train our states and we want the the final state, the win, to get the most points. If you win, you get most points. Um, and the first step gets the least amount of


04:59:28
points. So, you're really training this almost in reverse. You're training, you're training it from the last place where you have like it says, "Okay, this is now where I where need to sum up my rewards and I want to sum them up going in reverse and I want to find the answer in reverse." Kind of an interesting uh uh play on the mind when you're trying to figure this stuff out. And of course, we want to go ahead and reset the board down here. Uh save the policy, load policy. These are the different things


04:59:57
that are going in between the agent and the state to figure out what's going on. Let's go ahead and load that up. And then finally, we want to go ahead and create a human player. And the human player is going to be a little different uh in that uh you choose an action row and column. Here's your action. Uh if action is if action in positions, meaning positions that are available, uh you return the action. If not, it just keeps asking you until you get an action that actually works. And then


05:00:31
we're going to go ahead and append to the hash state, which uh we don't need to worry about because it returns the action up here. And feed forward. Uh again, this is because it's a human. Um at the end of the game, bat propagate and update state values. This part isn't being done because it's not programming uh the model. Uh the model is getting its own rewards. So, we've gone ahead and loaded this in here. Uh, so here's all our pieces. And the first thing we want to


05:01:03
do is set up uh P1, player one, uh, P2, player two. And then we're going to send our players to our state. So, now it has P1, P2, and it's going to play, and it's going to play 50,000 rounds. Now, we can probably do a lot less than this, and it's not going to get the full results. In fact, you know what? Uh, let's go ahead and just do five. Um, just to play with it because I want to show you something here. Oops. Somewhere in there I forgot to load something. There we go. I must have


05:01:36
forgot to run this run. Oops, forgot a reference there for the board rows and columns 3x3. Um, there is actually in the state it references that. We just tack it on on the end. It was supposed to be at the beginning. Uh, so now I've only set this up with um, see where are we going here? I've only set this up to train five times. And the reason I did that is we're going to uh, come in and actually play it and then I'm going to change that and we can see how it differs on there. There we go. And it didn't make


05:02:16
it through a run. And we're going to go ahead and save the policy. Um, so now we have our player one and our player two policy. Uh, the way we set it up, it has two separate policies loaded up in there. And then we're going to come in here and we're going to do uh player one is going to be the computer experience rate zero. Load policy one. Human player human. And we're going to go ahead and play this. Now remember, I only went through it um uh just one round of training. In fact, minimal training. And


05:02:48
so it puts an X there. And I'm going to go ahead and do row zero, column one. You can see this is very uh basic on here. And so I put in my zero. And then I'm going to go zero. Block it. Zero zero. And you can see right here it let me win. Uh just like that, I was able to win. Zero two. And woo, human wins. So I only trained it five times. We're going to run this again. And this time, uh, instead of five, let's do 5,000 or 50,000. I think that's what the guys in the back had. And this takes a while to


05:03:29
train it. This is where reinforcement learning really falls apart. Look how simple this game is. We're talking about a 3x3 set of columns. And so for me to train it on this um I could do a Q table which would take which would go much quicker. Um you could build a quick Q table with almost all the different options on there and uh you would probably get a the same result much quicker. We're just using this as an example. So when we look at reinforcement learning, you need to be very careful what you apply it to. It


05:04:08
sounds like a good deal until you do like a large neural network where you're doing um you set the neural network to a learning increment of one. So every time it goes through it learns and then you do your actions. So you pick from the learning uh setup and you actually try actions on the learning setup until you get the what you think is going to be the best action. So you actually feed what you think is right back through the neural network. There's a whole layer there which is really fun to play with.


05:04:36
and then it has an output. Well, think of all those processes. I mean, that is just a huge amount of work it's going to do. Uh, let's go ahead and skip ahead here. Give it a moment. It's going to take a a minute or two to go ahead and run. Now, to train it, uh, we went ahead and let it run and it took a while. This this took, um, I got a pretty powerful processor and it took about five minutes plus to run it. and we'll go ahead and uh run our player setup on here. Oops, it brought in the last Whoops, it


05:05:11
brought in the last round. So, give me just a moment to reddo the policy save. There we go. I forgot to save the policy back in there and then go ahead and run our player again. So, we we've saved the policy and then we want to go ahead and load the policy for P1 as a computer. And we can see the computer's gone in the bottom right corner. I'm going to go ahead and go uh one one which is the center and it's gone right up the top. And if you have ever played tic-tac-toe, you know the computer has me. Uh but


05:05:44
we'll go ahead and play it out. Row zero, column two. There it is. And then it's gone here. And so I'm going to go ahead and go row 0 one two. No, 0 one. There we go. And column zero. That's where I want it. Oh, and it says I Okay, you your action. There we go. Boom. Uh, so you can see here we've got a didn't catch the win on this. It said tie. Um, kind of funny that it didn't catch the win on there. But if we play this a bunch of times, you'll find it's going to win


05:06:17
more and more. The more we train it, the more the reinforcement happens. This lengthy training process uh is really the stopper on reinforcement learning. As this changes, reinforcement learning will be one of the more powerful uh packages evolving over the next decade or two. In fact, I would even go as far as to say it is the most important uh machine learning tool and artificial intelligence tool out there as it learns not only a simple tic-tac-toe board, but we start learning environments. And the environment would


05:06:50
be like in language. If you're translating a language or something from one language to the other, so much of it is lost if you don't know the context it's in. What's the environments it's in? And so being able to attach environment and context and all those things together is going to require reinforcement learning to do. So again, if you want to get a copy of the tic-tac-toe board, it's kind of fun to play with. Uh run it, you can test it out, you can do u you know, test


05:07:17
it for different uh uh values. You can switch from P1 computer uh where we loaded the policy one to load the policy two and just see how it varies. There's all kinds of things you can do on there. So what is Q-learning? Q-learning is reinforcement learning policy which will fill the next best action given a current state. It chooses this action at random and aims to maximize the reward. And so you can see here's our standard reinforcement learning graph. Um by now if you're doing any reinforcement


05:07:46
learning you should be familiar with this where you have your agent. Your agent takes an action. The action affects the environment and then the environment sends back the reward or the feedback and the state it's the new state the agents in. Where is it at on the chessboard? Where is it at in the video game? Um if your robot's out there picking trash up off the side of the road, where is it at on the road? Consider an ad recommendation system. Usually when you look up a product online, you get ads which will suggest


05:08:17
the same product over and over again. Using Q-learning, we can make an ad recommendation system which will suggest related products to our previous purchase. The reward will be if user clicks on the suggested product. And again, you can see um you might have a lot of products on uh your web advertisement or your pages, but it's still not a float number. It's still a set number. And that's something to be aware of when you're using Q-learning. And you can see here that if you have a 100 people clicking on ads


05:08:52
and you click on one of the ads, it might go in there and say, "Okay, this person clicked on this ad. What is the best set of ads based on clicking on this ad or these two ads afterwards based on where they are browsing? So let's go and take a look at some important terms when we talk about Q-learning. Uh we have states the state S represents the current position of an agent in an environment. Um the action the action A is the step taken by the agent when it is particular state. Rewards for every


05:09:22
action the agent will get a positive or negative reward. And again, uh, when we talk about states, we're usually not with when you're using a Q table, you're not usually talking about float variables. You're talking about true false. Uh, we'll take a closer look at that in a second. And episodes when an agent ends up in a terminating state and can't take a new action. Uh, this might be if you're playing a video game, your character stepped in and is now dead or whatever.


05:09:53
uh Q values used to determine how good an action A taken at a particular state S is Q A of S and temporal difference a formula used to find the Q value by using the value of the current state and action and previous state and action and very I mean there's Bellman's equation which basically is the equation that kind of uh covers what we just looked at in all those different terms. The Bellman equation is used to determine the values of a particular state and deduce how good it is to be in take that


05:10:27
state. The optimal the optimal state will give us the highest optimal value factor influencing Q values. The current state and action that's your SA. So your current state and your action. Uh then you have your previous state and action which is your S um I guess prime. I'm not sure how they how they reference that S prime A prime. So this is what happened before. Uh then you have a reward for action. So you have your R reward and you have your maximum expected future reward. And you can see there's also a


05:11:02
learning rate put in there and a discount rate. Uh so when we're looking at these just like any other model, we don't want to have an absolute um final value on here. We don't want it to. If you do absolute values instead of taking smaller steps, you don't really have that approach to the solution. You just have a jump and then pretty soon if you jump one solution out, that's what's going to be the new solution. Whichever one jumps up really high first. Um kind of ruining the whole idea of doing a


05:11:33
random selection. And I'll go into the random selection in just a second. Steps in Q-learning. Step one, create an initial Q table with all values initialized to zero. Again, we're looking at 01. Uh, so are you, you know, here's our action. We start, we're in idle. We took a wrong action. We took a correct action and end. And then we have our um actions fetching, sitting, and running. And of course, we're just using the dog example. And choose an action and perform it. Update values in


05:12:07
the table. And of course, when we're choosing an action, we're going to kind of do something random and just randomly pick one. So, you start out and you sit and you have then a um then depending on that um um action you took, you can now update the value for sitting after you start from start to sitting. Get the value of the reward and calculate the val the value Q value using the Bellman equation. And so now we attach a reward to sitting. And when we attach all those rewards, we continue the same until the


05:12:39
table's filled with or an episode ends. And and imagine is going to come back to the random side of this. And there's a few different formulas they use for the random um setup to pick it. I usually let whatever Q model I'm using do their standard one because someone's usually gone in and done the math uh for the optimal uh spread. Uh but you can look at this. If I have running has a reward of 10, sitting has a reward of seven, fetching has a reward of five. Um, just kind of without doing like a a


05:13:11
means, you know, using the bell curve for the means value. And like I said, there's some math you can put in there to pick so that you're more like so that running has even a higher chance. Uh, but even if you were just going to do an average on this, you could do an average, a random number by adding them all together. Uh so you get 10 + 7 + 5 is 22. You could do 0 to 22 and or 0 to 21 but 1 to 22. 1 to five would be fetching uh and so forth. You know the last 10. So you can just look at this as


05:13:44
what percentage are you going to go for that particular option. Um and then that gets your random setup in there. And then as you slowly increment these up, uh you see that uh u if you're idle, u where's one? Here we go. Sitting at the end, if you're at the end of wherever you're at, sitting gets a reward of one. Um where's a good one on here? Oh, wrong action. Running for a wrong action gets almost no reward. So that becomes very very less likely to happen, but it still might happen. It still might have a


05:14:15
percentage of coming up. And that's where the random programming in Q-learning comes in. The below table gives us an idea of how many times an action has been taken and how positively correct action or negatively wrong action it is going to affect the next state. So let's go ahead and dive in and pull up a little piece of code and see what this looks like um in Python. Uh in this demo we'll use Q-learning to find the shortest path between two given points. If getting your learning started is half the


05:14:46
battle, what if you could do that for free? Visit Skillup by SimplyLearn. Click on the link in the description to know more. If you've seen my videos before, um I like to do it in the uh Anaconda Jupiter notebook um setup just because it's really easy to see and it's a nice demo. Uh and so here's my Anaconda. This one I'm actually using a Python 36 environment that I set up in here. And we'll go ahead and launch the Jupyter notebook on this. And once we're in our Jupyter notebook, uh, which has


05:15:19
the kernel loaded with Python 3, we'll go ahead and create a new Python 3 uh, folder in here. And we'll call this, uh, Q learning. And to start this demo, let's go ahead and import our uh, numpy array. We'll just run that. So, it's imported. And like a lot of these uh, model programs, when you're building them, you spend a lot of time putting it all together. Um and then you end up with this really short answer at the end. Uh and we'll we'll take a look at that as we come


05:15:53
into it. So we we go ahead and start with our location to state. Uh so we have um L1 L2. These are our nine locations 1 to nine. And then of course the state is going to be 0 1 2 3 4. It's just a mapping of our location to a integer on there. And then we have our actions. Our actions are simply uh moving from uh one location to another. So I can go to I can go to location zero. I can go to location 1 2 3 4 5 6 7 8. Uh so these are my actions. I can choose these are the locations of our state.


05:16:31
And if you remember earlier, I mentioned uh um that the limitation is that you you don't want to put in um a continually growing table because you can actually create a dynamic Q table where you continually add in new values as they arise because um if you have float values, it just becomes infinite and then your memory in your computer's gone or you know it's not going to work. At the same time, you might think, well, that kind of really limits the the Q uh T learning setup, but there are ways to use it in


05:17:05
conjunction with other systems. And so you might look at uh well I do um I've been doing some work in stock um and one of the questions that comes out is to buy or sell the stock and the stake coming in might be um you might take it and create what we call buckets um where anything that you predict is going to return more than a certain amount of money um the error for that stock that you've had in the past you put those in bucket ets and suddenly you start putting the creating these buckets


05:17:41
you realize you do have a limited amount of information coming in. You no longer have a float number. You now have um bucket 1 2 3 and four. And then you can take those buckets, put them through a a Q-learning table and come up with the best action. Which stock should I buy? It's like gambling. Stock is pretty much gambling if you're doing day trading. You're not doing long-term u investments. And so you can start looking at it like that. A lot of the um current feeds say that the best


05:18:12
algorithms used for day traders where you're doing it on your own is really to ask the question, do I want to trade the stock? Yes or no? And now you have it in a Q-learning table and now you can take it to that next level and you can see where that can be a really powerful tool at the end of doing a basic linear regression model or something. um what is the best investment and you start getting the best reward on there. Uh and so if we're going to have rewards, these rewards we just create um


05:18:40
it says uh if basically if you're uh this should match our Q table because it's going to be uh you have your state and you have your action across the top if you remember from the dog. And so we have whatever state we're in going down and then the next action and what the reward is for it. Um, and of course, if you were actually doing a um something more connected, your reward would be based on uh the actual environment it's in. And then we want to go ahead and create a state to location uh so we can


05:19:11
map the indexes. So, just like we defined our rewards, uh we're going to go and do state to location. Um and you can see here it's a a dictionary setup for location state and locationto state with items and We also need to um define what we want for learning rates. Uh you remember we had our two different rates um as far as like learning from the past and learning from the current. So we'll go ahead and set those to uh 75 and the alpha set to 0.9. And we'll see that when we do the


05:19:48
formula. And of course any of this code uh send a note to our SimplyLearn team. They'll get you a copy of this code on here. Let's go ahead and pull. There we go. The new next two sections. Um, since we're going to keep it short and sweet. Here we go. So, let's go ahead and create our agent. Um, so our agent is going to have our initialization where we send it all the information. Uh, we'll define our self gamma equals gamma. We could have just set the gamma rate down here instead of


05:20:24
uh submitting it. It's kind of nice to keep them separate because you can play with these numbers. Uh our self-alpha. Um then we have our location state. We'll set that in here. Um we have our choice of actions. Um we're going to go ahead and just embed the rewards right into the agent. So obviously this would be coming from somewhere else uh instead of from uh self-generated. And then a self state to location equals our state to location uh dictionary. And we go ahead and create a Q-learning table. And


05:20:57
I went ahead and just set the Q-learning table up to um uh 0 to zero. What what what the setup is uh location to state. How many of them are there? Uh and this just creates an array of zero to zero setup on there. And then the big part is the training. We have our rewards new equals a copy of self.rewards. rewards. Ending state equals the self-location state end location. So this is whatever we end up at. Rewards new equals ending state plus ending state equals 999. Just kind of goes to a dead end. And we start going through


05:21:37
iterations and we'll go ahead. Um let's do this. Uh so this we're going to come back and we're going to call call it on here. Uh let me just erase that. Switch it to an arrow. There we go. Uh so what we're doing is we're going to send in here to train it. We're going to say, "Hey, um I want to iterate through this a thousand times and see what happens." Now this part would actually be instead of iterating, you might have your external environment and they're going back and


05:22:08
forth and you iterate through outside of here. Uh, but just for ease of use, our agent's going to come in here and iterate through this. Sometimes I'll put this iteration in here and I'll have it call the environment and say, "Hey, this is what I did. What's the next state?" And the environment does its thing right in here as I iterate through it. Uh, and then we want to go ahead and pick a random state to start with. That's what's going on here. You have to start somewhere. Um, and then you have


05:22:38
your playable actions. We're going to start with just an empty thing for playable actions and we'll fill that up. So that's what choices I have. And so we're going to iterate through the rewards matrix to get the states uh directly reachable from the randomly chosen current state. Assign those states to a list named playable actions. And so you can see here we have uh range nine. I usually use length of whatever I'm looking at uh which is our locations or states as they are. uh we


05:23:08
have a reward. So we want to look at the current the rewards uh the new reward is our uh is in our chart here of rewards new uh current state um plus J. Uh J being what is the next state we want to try. And so we go ahead and do our playable actions and we append J. And so we're doing is we're randomly trying different things in here to see what's going to generate a better reward. And then of course we go ahead and choose our next state. Uh so we have our random choice playable actions. And if


05:23:45
you remember I mentioned on this let me just go ahead and uh oops let's do a free form. When we were talking about the next state uh this right here just does a random selection. Instead of a random uh selection, you might do something where uh whatever the best selection is, which might be option three here. And then so you can see that it might use a bell curve and then option two over here might have a bell curve like this. Oops. And we start looking at these averages and these spreads. Um or we can just add them all


05:24:23
together and pick the one that kind of goes in all of those. Uh so those are some of the options we have in here. We just go with a random choice. Uh that's usually where you start, play with it. Um and then we have our reward section down here. And so we want to go ahead and find well in this case the temporal difference. Uh so you have your rewards new plus the self gamma. And this is the formula we were looking at. This is Bellman's equation here. Uh so we have our current value, our learning rate,


05:24:52
our discount rate involved in there. the reward system coming in for that. Uh, and we can add it all together. This is, of course, our maximum expected future setup in here. Uh, so this is all of our our Bellman's equation that we're looking at here. And then we come up in here and we update our Q table. That's all this is on this one. Uh, that's right here. we have um self Q current state next state and we add in our um alpha because we don't want to we don't want to train all of it at once in case


05:25:25
there's slight differences coming in there we want to slowly approach the answer uh and then we have our route equals the start location and next location equals start location so we're just incrementing we took a step forward and then finally remember I was telling you how uh we're going to do all this and just have some simple thing at the and where it just generates a simple path. We're going to go ahead and and get the optimal route. We want to find the best route in here. And so we've created a definition for


05:25:55
the optimal route down here. Just scroll down for that. And we get the optimal route. We go ahead and put the information in including the Q table, self uh start location, end location, next location, route Q. And it says while next location is not equal to end location. So while we can still go our start location equals self-loation to state start location. So we already have our best value for the start location. Uh the next state looks at the Q table and says hey what's uh the next one with the best value and then the


05:26:28
next location we go ahead and pull that in and we just append it. That's what's going on down here. And then our start location equals the next location. And we just go through all the steps. And we'll go ahead and run this. And now that we have our Q table, our um Q agent loaded, we're going to go ahead and uh take our Q agent, load them up with our alpha, gamma that we set up above um along with the location step, action, reward, state to location, and uh our goal is to plot a course between


05:27:02
L9 and L1. And we're going to go through a 100 a thousand iterations on here. And so when I run that, it runs pretty quick. Uh why is this so fast? Um if you've been running neural networks and you've been doing all these other models, you sit here and wait a long time. Well, we're very small amount of data. These are all integers. These aren't float values. There's not a the math is not heavy on the on the processing end. And this is where Q tables are so powerful. If you have a small amount of


05:27:31
information coming in, you very quickly uh get an answer off of this even though we went through it a thousand times to train it. And you'll see here we have L98 52 and one. And that's based on our reward table we had set up on there. And this is the shortest path going between these different uh setups in here. And if you remember in our reward table, uh you can see that if you start here, you can go to here. there's places you can't go. That's how this reward table was set


05:28:01
up. So I can only go to certain places. Uh so kind of a little maze setup in there. And you can play with it. This is really fun uh setup to play with. Uh and you can see how you can take this whole code and you can like I was saying earlier, you can embed it into another setup and model and predictions where you put things into buckets and you're trying to guess the best investment, the best course of action. long as you can take that course of action and and uh uh reduce it down to a yes no um or if you're using text,


05:28:35
you can use one hot encoder. Which word is next? There's all kinds of things you can do with a Q table uh depending on just how much information you're putting in there. So, that wraps up our demo. In this demo, we've uh found the shortest distance between two paths based on whatever rules or state rewards we have to get from point A to point B and what available actions there are. Hello and welcome to this tutorial on deep learning. My name is Moan and in the next about one one and a half hours I


05:29:05
will take you through what is deep learning and into TensorFlow environment to show you an example of deep learning. Now there are several applications of deep learning really very interesting and innovative applications and one of them is identifying the geographic location based on a picture. And how does this work? The way it works is pretty much we train an artificial neural network with millions of images which are tagged. their geoloccation is tagged and then when we feed a new picture, it will be able to identify the


05:29:42
geoloc image. For example, you have all these images especially with maybe some significant monuments or or u significant locations and you train with millions of such images and then when you feed another image, it need not be exactly one of those that you have trained. It can be completely different. That is the whole idea of cleaning. It will be able to recognize for example that this is a picture from Paris because it is able to recognize the Eiffel Tower. So the way it works internally if we have to look a little


05:30:16
bit under the hood is these images are nothing but this is digital information in the form of pixels. So each image could be a certain size. It can be 256 x 256 pixel kind of a resolution and then each pixel is either having a certain grade of color and all that is fed into the neural network and it then gets trained in and it's able to based on these pixels pixel information it is able to get trained and able to recognize the features and extract the features and thereby it is able to identif identify these images and the


05:30:58
location of these images and then when you feed a new image it kind of based on the training it will be able to figure out where this images from. So that's the way a little bit under the hood how it works. So what are we going to do in this tutorial? We will see what is deep learning and what do we need for deep learning and one of the main components of deep learning is neural network. So we will see what is neural network what is a perceptron and how to implement logic gates like and or nor and so on


05:31:29
using perceptrons the different types of neural networks and then applications of deep learning and we will also see how neural networks works. So how do we do the training of neural networks and at the end we will end up with a small demo code which will take you through in tensorflow. Now in order to implement deep learning code there are multiple libraries or development environments that are available and TensorFlow is one of them. So the focus at the end of this would be on how to use TensorFlow to


05:32:06
write a piece of code using Python as a programming language. And we will take up a an example which is a very common one which is like the hello world of deep learning. the handwriting number recognition which is a MNEST commonly known as MNEST database. So we will take a look at MNEST database and how we can train a neural network to recognize handwritten numbers. So that's what you will see in this particular video. So let's get started. What is deep learning? Deep learning is like a subset


05:32:40
of what is known as a highlevel concept called artificial intelligence. You must be already familiar must have heard about this term artificial intelligence. So artificial intelligence is like the highle concept if you will and in order to implement artificial intelligence applications we use what is known as machine learning and within machine learning a subset of machine learning is deep learning. Machine learning is a little bit more generic concept and deep learning is one type of machine learning


05:33:15
if you will and we will see a little later in maybe the following slides a little bit more in detail how deep learning is different from traditional machine learning. But to start with we can mention here that deep learning uses one of the differentiators between deep learning and traditional machine learning is that deep learning uses neural networks and we will talk about what are neural networks and how we can implement neural networks and so on and so forth as a part of this tutorial. So a little deeper into deep learning. Deep


05:33:45
learning primarily involves working with complicated unstructured data compared to traditional machine learning where where we normally use structured data. In deep learning the data would be primarily images or voice or maybe text file. So and it is large amount of data as well. And deep learning can handle complex operations. It involves complex operations. And the other difference between traditional machine learning and deep learning is that the feature extraction happens pretty much automatically. In traditional machine


05:34:19
learning, feature engineering is done manually. The data scientists, we data scientists have to do feature engineering, feature extraction. But in deep learning that happens automatically and of course deep learning for large amounts of data, complicated unstructured data, deep learning gives very good performance. Now as I mentioned one of the secret sources of deep learning is neural networks. Let's see what neural networks is. Neural networks is based on our biological neurons. The whole concept of deep


05:34:55
learning and artificial intelligence is based on human brain and human brain consists of billions of tiny stuff called neurons. And this is how a biological neuron looks. And this is how an artificial neuron looks. So neural networks is like a simulation of our human brain. Human brain has billions of biological neurons and we are trying to simulate the human brain using artificial neurons. This is how a biological neuron looks. It has dendrites and the corresponding component with an artificial neural


05:35:36
network is or an artificial neuron are the inputs. They receive the inputs through dendrites and then there is the cell nucleus which is basically the processing unit in a way. So in artificial neuron also there is a piece which is an equivalent of this cell nucleus and based on the weights and biases. We will see what exactly weights and biases are as we move the input gets processed and that results in an output. In a biological neuron the output is sent through a synapse and in an artificial neuron there is an equivalent


05:36:12
of that in the form of an output. And biological neurons are also interconnected. So there are billions of neurons which are interconnected. In the same way, artificial neurons are also interconnected. So this output of this neuron will be fed as an input to another neuron and so on. Now in neural network, one of the very basic units is a perceptron. So what is a perceptron? A perceptron can be considered as one of the fundamental units of neural networks. It can consist at least one neuron but sometimes it can be more than


05:36:48
one neuron but you can create a perceptron with a single neuron and it can be used to perform certain functions. It can be used as a basic binary classifier. It can be trained to do some basic binary classification. And this is how a basic perceptron looks like. And this is nothing but a neuron. You have inputs x1, x2, x uh to xn and there is a summation function and then there is what is known as an activation function and based on this input what is known as the weighted sum. The activation function either gets gives an


05:37:26
output like a zero or a one. So we say the neuron is either activated or not. So that's the way it works. So you get the inputs. These inputs are each of the inputs are multiplied by a weight and there is a bias that gets added and that whole thing is fed to an activation function and then that results in an output. And if the output is correct, it is accepted. If it is wrong, if there is an error, then that error is fed back and the neuron then adjusts the weights and biases to give a new output and so


05:37:58
on and so forth. So that's what is known as the training process of a neuron or a neural network. There's a concept called perceptron learning. So perceptron learning is again one of the very basic learning processes. The way it works is somewhat like this. So you have all these inputs like x1 to xn and each of these inputs is multiplied by a weight and then that sum this is the formula the equation. So that sum wi x i sigma of that which is a sum of all these product of x and w is added up and then


05:38:34
a bias is added to that. The bias is not dependent on the input but or the input values but the bias is common for one neuron. However, the bias value keeps changing during the training process. Once the training is completed, the values of these weights W1, W2 and so on and the value of the bias gets fixed. So that is basically the whole training process and that is what is known as the perceptron training. So the weights and biases keep changing till you get the accurate output and the summation is of


05:39:10
course passed through the activation function. As you see here, this wixi summation plus B is passed through activation function and then the neuron gets either fired or not and based on that there will be an output that output is compared with the actual or expected value which is also known as labeled information. So this is the process of supervised learning. So the output is already known and um that is compared and thereby we know if there is an error or not and if there is an error the error is fed back and the weights and


05:39:48
biases are updated accordingly till the error is reduced to the minimum. So this iterative process is known as perceptron learning or perceptron learning rule and this error needs to be minimized. So till the error is minimized this iteratively the weights and biases keep changing and that is what is the training process. So the whole idea is to update the weights and the bias of the perceptron till the error is minimized. The error need not be zero. The error may not ever reach zero. But the idea is to keep changing these


05:40:29
weights and bias so that the error is minimum. the minimum possible that it can have. So this whole process is an iterative process and this is the iteration continues till either the error is zero which is uh unlikely situation or it is the minimum possible within these given conditions. Now in 1943 two scientists Warren Mccelikch and Walter Pittz came up with an experiment where they were able to implement the logical functions like and or and nor using neurons and that was a significant breakthrough in a sense. So they were


05:41:12
able to come up with the most common logical gates. they were able to implement some of the most common logical gates which could take two inputs like a and b and then give a corresponding result. So for example in case of an and gate a and b and then the output is a b in case of an or gate it is a plus b and so on and so forth and they were able to do this using a single layer perceptron. Now most of these gates it was possible to use single layer perceptron except for XR and we will see why that is in a little bit. So


05:41:51
this is how an AND gate works. The inputs A and B the output should be fired or the neuron should be fired only when both the inputs are one. So if you have 0 0 the output should be zero. For 0 1 it is again 0 1 0 again 0 and 1 one the output should be one. So how do we implement this with a neuron? So it was found that by changing the values of weights it is possible to achieve this logic. So for example if we have equal weights like 7.7 and then if we take the sum of the weighted product. So for


05:42:27
example 7 into 0 and then 7 into 0 will give you zero and so on and so forth. And in the last case when both the inputs are one you get a value which is greater than one which is the threshold. So only in this case the neuron gets activated and the output is there is an output. In all the other cases there is no output because the threshold value is one. So this is implementation of an AND gate using a single perceptron or a single neuron. Similarly an orgate. In order to implement an argate in case of


05:42:59
an argate the output will be one if either of these inputs is one. So for example 01 will result in one or other in all the cases it is one except for 0 0. So how do we implement this using a perceptron once again if you have a perceptron with weights for example 1.2. Now if you see here if in the first case when both are zero the output is zero. In the second case when it is 0 and 1 1.2 2 into 0 is 0 and then 1.2 into 1 is 1 and in the second case similarly the output is 1.2 in the last case when both


05:43:34
the inputs are one the output is 2.4. So during the training process these weights will keep changing and then at one point where the weights are equal to w1 is equal to 1.2 and w2 is equal to 1.2 the system learns that it gives the correct output. So that is implementation of orgate using a single neuron or a single layer perceptron. Now XR gate this was one of the challenging ones. They tried to implement an XR gate with a single level perceptron but it was not possible and therefore in order


05:44:09
to implement an XR. So this was like a a roadblock in the progress of U neural network. However, subsequently they realized that this can be implemented an XR gate can be implemented using a multi-level perceptron or MLP. So in this case there are two layers instead of a single layer and this is how you can implement an XR gate. So you will see that X1 and X2 are the inputs and there is a hidden layer and that's why it is denoted as H3 and H4 and then you take the output of that and feed it to


05:44:45
the output at 05 and provide a threshold here. So we will see here that this is the numerical calculation. So the weights are in this case for X1 it is 20 and minus 20 and once again 20 and minus 20. So these inputs are fed into H3 and H4. So you'll see here for H3 the input is 0 1 1 and for H4 it is 1 0 1 1. And if you now look at the output final output where the threshold is taken as one. If you use a sigmoid with the threshold one you will see that in these two cases it is zero and in the last two


05:45:27
cases it is one. So this is the implementation of XR. In case of XR only when one of the inputs is one you will get an output. So that is what we are seeing here. If we have either both the inputs are one or both the inputs are zero then the output should be zero. So that is what is an exclusive orgate. So it is exclusive because only one of the inputs should be one and then only you'll get an output of one which is satisfied by this condition. So this is a special implementation. An XR gate is


05:45:57
a special implementation of a perceptron. Now that we got a good idea about perceptron, let's take a look at what is a neural network. So we have seen what is a perceptron, we have seen what is a neuron. So we will see what exactly is a neural network. So neural network is nothing but a network of these neurons and they are different types of neural networks. There are about five of them. These are artificial neural network, convolutional neural network, then recursive neural network or recurrent neural network, deep neural


05:46:29
network and deep belief network. So, and each of these types of neural networks have a special you know they can solve a special kind of problems. For example, convolutional neural networks are very good at performing image processing and image recognition and so on. Whereas RNN are very good for speech recognition and also text analysis and so on. So each type has some special characteristics and they can uh they're good at performing certain special kind of tasks. What are some of the applications


05:47:03
of deep learning? Deep learning is today used extensively in gaming. You must have heard about Alph Go which is a game created by a startup called Deep Mind which got acquired by Google and Alph Go is an AI which defeated the human world champion Lee Sedall in this game of Go. So gaming is an area where deep learning is being extensively used and a lot of research happens in the area of gaming as well. In addition to that nowadays there are neural networks a special type called generative adversarial networks


05:47:44
which can be used for synthesizing either images or music or text and so on and they can be used to compose music. So the neural network can be trained to compose a certain kind of music and autonomous cars. So you must be familiar with Google, Google's self-driving car and today a lot of automotive companies are investing in this space and uh deep learning is a core component of this autonomous cars. The cars are trained to recognize for example the road the the lane markings on the road signals any


05:48:23
objects that are in front any obstruction and so on and so forth. So all this involves deep learning. So that's another major application and uh robots we have seen several robots including Sophia you may be familiar with Sophia who was given a citizenship by Saudi Arabia and there are several such robots which are very humanlike and the underlying technology in many of these robots is deep learning. Medical diagnostics and healthc care is another major area where deep learning is being used. And within healthcare diagnostics


05:49:01
again there are multiple areas where deep learning and image recognition image processing can be used. For example for cancer detection as you may be aware if cancer is detected early on it can be cured. And one of the challenges is in the availability of specialists who can diagnose cancer using these diagnostic images and various scans and and so on and so forth. So the idea is to train neural network to perform some of these activities so that the load on the cancer specialist doctors or oncologists


05:49:37
uh comes down and there is a lot of research happening here and there are already quite a few applications that are claimed to be performing better than human beings in this space. Can be lung cancer, it can be breast cancer and so on and so forth. So healthcare is a major area where deep learning is being applied. Let's take a look at the inner working of a neural network. So how does an artificial neural network let's say identify can we train a neural network to identify the shapes like squares and


05:50:10
circles and triangles when these images are fed. So this is how it works. Any image is nothing but it is a digital information of the pixels. So in this particular case, let's say this is an image of 28x 28 pixel and this is an image of a square. There's a certain way in which the pixels are lit up. And so these pixels have a certain value maybe from 0 to 256 and 0 indicates that it is black or it is dark and 256 indicates it is completely it is white or lit up. So that is like an indication or a measure


05:50:50
of the how the pixels are lit up. And so this is an image is let's say consisting of information of 784 pixels. So all the information what is inside this image can be kind of compressed into these 784 pixels. The way each of these pixels is lit up provides information about what exactly is the image. So we can train neural networks to use that information and identify the images. So let's take a look how this works. So each neuron the value if it is close to one that means it is white whereas if it is close to


05:51:35
zero that means it is black. Now this is a an animation of how this whole thing works. So these pixels one of the ways of doing it is we can flatten this image and take this complete 784 pixels and feed that as input to our neural network. The neural network can consist of probably several layers. There can be a few hidden layers and then there is an input layer and an output layer. Now the input layer take these 784 pixels as input the values of each of these pixels and then you get an output which can be


05:52:12
of three types or three classes. One can be a square, a circle or a triangle. Now during the training process there will be initially obviously you feed this image and it will probably say it's a circle or it will say it's a triangle. So as a part of the training process, we then send that error back and the weights and the biases of these neurons are adjusted till it correctly identifies that this is a square. That is the whole training mechanism that happens out here. Now let's take a look at a circle.


05:52:48
Same way. So you feed these 784 pixels. There is a certain pattern in which the pixels are lit up and the neural network is trained to identify that pattern. And during the training process once again it would probably initially identify it incorrectly saying this is a square or a triangle and then that error is fed back and the weights and biases are adjusted finally till it finally gets the image correct. So that is the training process. So now we will take a look at same way a triangle. So now if you feed another


05:53:32
image which is consisting of triangle. So this is the training process. Now we have trained our neural network to classify these images into a triangle or a circle and a square. So now this neural network can identify these three types of objects. Now if you feed another image and it will be able to identify whether it's a square or a triangle or a circle. Now what is important to be observed is that when you feed a new image it is not necessary that the image or the the triangle is exactly in this position. Now the neural


05:54:10
network actually identifies the patterns. So even if the triangle is let's say positioned here not exactly in the middle but maybe at the corner or in the side it would still identify that it is a triangle and that is the whole idea behind pattern recognition. So how does this training process work? This is a quick view of how the training process works. So we have seen that a neuron consists of inputs. It receives inputs and then there is a weighted sum which is nothing but this xi wi summation of


05:54:45
that plus the bias and this is then fed to the activation function and that in turn gives us a output. Now during the training process initially obviously when you feed these images when you send maybe a square it will identify it as a triangle and when you maybe feed a triangle it will identify as a square and so on. So that error information is fed back and initially these weights can be random. Maybe all of them have zero values and then it will slowly keep changing. So the as a part of the training process the values of these


05:55:20
weights W1, W2 up to WN keep changing in such a way that towards the end of the training process it should be able to identify these images correctly. So till then the weights are adjusted and that is known as the training process. So and these weights are numeric values could be 0.5.25.35 and so on. It could be positive or it could be negative. And the value that is coming here is the pixel value as we have seen. It can be anything between 0 to 1. You can scale it between 0 to 1 or 0 to 256 whichever


05:55:54
way 0 being black and 256 being white and then all the other colors in between. So that is the input. So these are numerical values. this multiplication or the product wixi is a numerical value and the bias is also a numerical value. We need to keep in mind that the bias is fixed for a neuron. It doesn't change with the inputs whereas the weights are one per input. So that is one important point to be noted. So but the bias also keeps changing. Initially it will again have a random value but as a part of the training


05:56:24
process the weights the values of the weights W1 W2 WN and the value of B will change and ultimately once the training process is complete these values are fixed for this particular neuron W1 W2 up to WN and plus the value of the B is also fixed for this particular neuron and in this way there will be multiple neurons and each there may multiple levels of neurons here and that's the way the training process works. So this is another example of multilayer. So there are two hidden layers in between


05:57:01
and then you have the input layer values coming from the input layer. Then it goes through multiple layers hidden layers and then there is an output layer. And as you can see there are weights and biases for each of these neurons in in each layer and all of them gets keeps changing during the training process. And at the end of the training process, all these weights have a certain value and that is a trained model and those values will be fixed once the training is completed. All right. Then there is something known as


05:57:33
activation function. Neural networks consists of one of the components in neural networks is activation function and every neuron has an activation function and there are different types of activation functions that are used. It could be a relu, it could be sigmoid and so on and so forth. And the activation function is what decides whether a neuron should be fired or not. So whether the output should be zero or one is decided by the activation function and the activation function in turn takes the input which is the


05:58:05
weighted sum. Remember we talked about wixi + b. That weighted sum is fed as a input to the activation function and then the output can be either a zero or a one. And there are different types of activation functions which are covered in an earlier video you might want to watch. All right. So as a part of the training process we feed the inputs the label data or the training data and then it gives an output which is the predicted output by the network which we indicate as yhat and then there is a


05:58:36
labeled data because we for supervised learning we already know what should be the output. So that is the actual output and in the initial process before the training is complete obviously there will be error. So that is measured by what is known as a cost function. So the difference between the predicted output and the actual output is the error and the cost function can be defined in different ways. There are different types of cost functions. So in this case it is like the average of the squares of


05:59:06
the error. So and then all the errors are added which can sometimes be called as sum of squares sum of square errors or SSC and that is then fed as a feedback in what is known as backward propagation or back propagation. And that helps in the network adjusting the weights and biases. And so the weights and biases get updated till this value the error value or the cost function is minimum. Now there is a optimization technique which is used here called gradient descent optimization and this algorithm works in a way that the error


05:59:48
which is the cost function needs to be minimized. So there's a lot of mathematics that goes behind this. For example, they find the uh local minima, the global minima using the differentiation and so on and so forth. But the idea is this. So as a training process, as the as the part of training, the whole idea is to bring down the error which is like let's say this is the function the cost function at certain levels it is very high. The cost value of the cost function or the the output of the cost function is very


06:00:23
high. So the weights have to be adjusted in such a way and also the bias of course that the cost function is minimized. So there is this optimization technique called gradient descent that is used and this is known as the learning rate. Now gradient descent you need to specify what should be the learning rate and the learning rate should be optimal because if you have a very high learning rate then the optimization will not converge because at some point it will cross over to the side. On the other hand if you have very


06:01:00
low learning rate then it might take forever to converge. So you need to come up with the optimum value of the learning rate and once that is done using the gradient descent optimization the error function is reduced and that's like the end of the training process. All right. So this is another view of gradient descent. So this is how it looks. This is your cost function the output of the cost function and that has to be minimized using gradient descent algorithm. And these are like the parameters and weight could be one of


06:01:34
them. So initially we start with certain random values. So cost will be high and then the weights keep changing and in such a way that the cost function needs to come down and at some point it may reach the minimum value and then it may increase. So that is where the gradient descent algorithm decides that okay it has reached the minimum value and it will kind of try to stay here. This is known as the global minima. Now sometimes these curves may not be just for explanation purpose. This has been


06:02:07
drawn in a nice way. But sometimes these curves can be pretty erratic. There can be some local minima here and then there is a peak and then and so on. So the whole idea of gradient descent optimization is to identify the global minima and to find the weights and the bias at that particular point. So that's what is gradient descent and then this is another example. So you can have these multiple local minima. So as you can see at this point when it is coming down it may appear like this is a


06:02:39
minimum value but then it is not. This is actually the global minimum value and the gradient descent algorithm will make an effort to reach this level and not get stuck at this point. So the algorithm is already there and it knows how to identify this global minimum and that's what it does during the training process. Now in order to implement deep learning there are multiple platforms and languages that are available but the most common platform nowadays is TensorFlow and so that's the reason we


06:03:11
have uh this tutorial we have created this tutorial for TensorFlow. So we will take you through a quick demo of how to write a TensorFlow code using Python and TensorFlow is uh an opensource platform created by Google. So let's just take a look at the details of TensorFlow and so this is a a library a Python library. So you can use Python or any other languages. It's also supported in other languages like Java and R and so on. But Python is the most common language that is used. So it is a library for


06:03:46
developing deep learning applications especially using neural networks and it consists of primarily two parts if you will. So one is the tensors and then the other is the graphs or the flow that's the way the name that's the reason for this kind of a name called tensorflow. So what are tensors? Tensors are like multi-dimensional arrays if you will that's one way of looking at it. So usually you have a one-dimensional array. So first of all you can have what is known as a scalar which means a


06:04:19
number and then you have a one-dimensional array something like this which means this is like a set of numbers. So that is a one-dimensional array. Then you can have a two-dimensional array which is like a matrix and beyond that sometimes it gets difficult. So this is a three-dimensional array but TensorFlow can handle many more dimensions. So it can have multi-dimensional arrays. That is the strength of TensorFlow and which makes computation deep learning computation much faster and that's the


06:04:49
reason why TensorFlow is used for developing deep learning applications. So TensorFlow is a deep learning tool and this is the way it works. So the data basically flows in the form of tensors and the way the programming works as well is that you first create a graph of how to execute it and then you actually execute that particular graph in the form of what is known as a session. We will see this in the tensorflow code as we move forward. So all the data is managed or manipulated in tensors and then the processing


06:05:25
happens using this graphs. There are certain terms called like for example ranks of a tensor. The rank of a tensor is like a dimensional dimensionality in a way. So for example if it is scalar so there is just a number just one number the rank is supposed to be zero and then it can be a one-dimensional vector in which case the rank is supposed to be one and then you can have a two-dimensional vector typically like a matrix then in that case we say the rank is two and then if it is a three-dimensional array then it rank is


06:06:03
three and so on. So it can have more than three as well. So it is possible that you can store multi-dimensional arrays in the form of tensors. So what are some of the properties of TensorFlow? I think today it is one of the most popular platform. TensorFlow is the most popular deep learning platform or library. It is open source. So developed by Google, developed and maintained by Google. But it is open source. One of the most important things about TensorFlow is that it can run on CPUs as well as GPUs. GPU is a graphical


06:06:38
processing unit just like CPU is central processing unit. Now in earlier days GPU was used for primarily for graphics and that's how the name has come and one of the reasons is that it cannot perform generic activities very efficiently like CPU but it can perform iterative actions or computations extremely fast and much faster than a CPU. So they are really good for computational activities and in deep learning there is a lot of iterative computation that happens. So in the form of matrix multiplication and


06:07:13
so on. So GPUs are very well suited for this kind of computation and TensorFlow supports both GPU as well as CPU. And there's a certain way of writing code in TensorFlow. We will see as we go into the code. And of course, TensorFlow can be used for traditional machine learning as well, but then that would be an overkill. But just for understanding, it may be a good idea to start writing code for a normal machine learning use case so that you get a hang of how TensorFlow code works and then you can move into


06:07:46
neural networks. So that is u just a suggestion but if you're already familiar with how TensorFlow works then probably yeah you can go straight into the neural networks part. So in this tutorial we will take the use case of recognizing handwritten digits. This is like a hello world of deep learning. And this is a nice little emnest database is a nice little database that has images of handwritten digits nicely formatted because very often in deep learning and neural networks we end up spending a lot


06:08:23
of time in preparing the data for training and with MNEST database we can avoid that. you already have the data in the right format which can be directly used for training and MNEST also offers a bunch of built-in utility functions that we can straight away use and call those functions without worrying about writing our own functions and that's one of the reasons why MNEST database is very popular for training purposes initially when people want to learn about deep learning and TensorFlow this


06:08:58
is the database that is closed and it has a collection of 70,000 handwritten digits and a large part of them are for training. Then you have test just like in any machine learning process and then you have validation and all of them are labeled. So you have the images and their label and these images they look somewhat like this. So they are handwritten images collected from a lot of individuals. People have these are samples written by human beings. They have handwritten these numbers. These numbers going from 0 to 9. So people


06:09:35
have written these numbers and then the images of those have been taken and formatted in such a way that it is very easy to handle. So that is MNEST database and the way we are going to implement this in our TensorFlow is we will feed this data especially the training data along with the label information and uh the data is basically these images are stored in the form of the pixel information as we have seen in one of the previous slides. All the images are nothing but these are pixels. So an image is nothing but an


06:10:12
arrangement of pixels and the value of the pixel either it is lit up or it is not or in somewhere in between. That's how the images are stored and that is how they are fed into the neural network and for training. Once the network is trained when you provide a new image it will be able to identify within a certain error of course and for this we will use one of the simpler neural network configurations called softmax and for simplicity what we will do is we will flatten these pixels. So instead of


06:10:50
taking them in a two-dimensional arrangement we just flatten them out. So for example, it starts from here. It is a 28x 28. So there are 784 pixels. So pixel number one starts here. It goes all the way up to 28. Then 29 starts here and goes up to 56 and so on. And the pixel number 784 is here. So we take all these pixels, flatten them out and feed them like one single line into our neural network. And this is a what is known as a softmax layer. What it does is once it is trained it will be able to


06:11:28
identify what digit this is. So there are in this output layer there are 10 neurons each signifying a digit and at any given point of time when you feed an image only one of these 10 neurons gets activated. So for example, if this is trained properly and if you feed a number nine like this, then this particular neuron gets activated. So you get an output from this neuron. Let me just use uh a pen or a laser to show you here. Okay. So you're feeding a number nine. Let's say this has been trained.


06:12:08
And now if you're feeding a number nine, this will get activated. Now let's say you feed one to the trained network then this neuron will get activated. If you feed two this neuron will get activated and so on. I hope you get the idea. So this is one type of a neural network or an activation function known as softmax layer. So that's what we will be using here. This is one of the simpler ones for quick and easy understanding. So this is how the code would look. We will go into our lab environment in the cloud


06:12:42
and uh we will show you there directly but very quickly this is how the code looks and uh let me run you through briefly here and then we will go into the Jupyter notebook where the actual code is and we will run that as well. So as a first step first of all we are using Python here and that's why the syntax of the language is Python and the first step is to import the TensorFlow library. So and we do this by using this line of code saying import tensorflow as TF. TF is just for convenience. So you


06:13:16
can name give any name and once you do this TF is TensorFlow is available as an object in the name of TF and then you can run its uh methods and accesses its attributes and so on and so forth. And MNEST database is actually an integral part of TensorFlow. And that's again another reason why we as a first step we always use this example mnest database example. So you just simply import mnest database as well using this line of code and you slightly modify this so that the labels are in this format what is known


06:13:52
as one hot true which means that the label information is stored like an array and uh let me just uh use the pen to show what exactly it is. So when you do this one hot true what happens is each label is stored in the form of an array of 10 digits and let's say the number is uh 8. Okay. So in this case all the remaining values there will be a bunch of zeros. So this is like array at position zero. This is at position one position two and so on and so forth. Let's say this is position 7. Then this


06:14:33
is position 8 that will be one because our input is 8 and again position 9 will be zero. Okay. So one hot encoding this one hot encoding true will kind of load the data in such a way that the labels are in such a way that only one of the digits has a value of one and that indicates. So based on which digit is one we know what is the label. So in this case the eighth position is one. Therefore, we know this sample data the value is 8. Similarly, if you have a two here, let's say, then the labelled


06:15:09
information will be somewhat like this. So, you have your labels. So, you have this as zero, the zeroth position, the first position is also zero. The second position is one because this indicates number two. And then you have third as zero and so on. Okay. So, that is the significance of this one hot true. All right. And then we can check how the data is uh looking by displaying the the data. And as I mentioned earlier, this is pretty much in the form of digital form like numbers. So all these are like


06:15:44
pixel values. So you will not really see an image in this format. But there is a way to visualize that image. I will show you in a bit. And uh this tells you how many images are there in each set. So the training there are 55,000 images in training and in the test set there are 10,000 and then validation there are 5,000. So altogether there are 70,000 images. All right. So let's uh move on and we can view the actual image by uh using the mattplot clip library. And this is how you can view this is the


06:16:20
code for viewing the images. And you can view them in color or you can view them in grayscale. So the cap is what tells in what way we want to view it. And what are the maximum values and the minimum values of the pixel values. So these are the max and minimum values. So of the pixel values. So maximum is one because this is a scaled value. So one means it is uh white and uh zero means it is black and in between is it can be anywhere in between black and white. And the way to train the model there is a


06:16:58
certain way in which you write your TensorFlow code and um the first step is to create some placeholders and then you create a model. In this case, we will use the softmax model, one of the simplest ones. And um placeholders are primarily to get the data from outside into the neural network. So this is a very common mechanism that is used. And uh then of course you will have variables which are your you remember these are your weights and biases. So for in our case there are 10 neurons and uh each neuron actually has


06:17:33
784 because each neuron takes all the inputs. So if we go back to our slide here actually every neuron takes all the 784 inputs right this is the first neuron it has it receives all the 784. This is the second neuron. This also receives all the 78. So each of these inputs needs to be multiplied with a weight and that's what we are talking about here. So these are this is a a matrix of 784 values for each of the neurons and uh so it is like a 10x 784 matrix because there are 10 neurons and uh


06:18:14
similarly there are biases. Now remember I mentioned bias is only one per neuron. So it is not one per input unlike the weights. So therefore there are only 10 biases because there are only 10 neurons in this case. So that is what we are creating a variable for biases. So this is uh something little new in tensorflow you will see unlike our regular programming languages where everything is a variable here the variables can be of three different types. You have placeholders which are primarily used


06:18:46
for feeding data. You have variables which can change during the course of computation. And then a third type which is not shown here are constants. So these are like fixed numbers. All right. So in a regular programming language you may have everything as variables or at the most variables and constants. But in TensorFlow you have three different types placeholders, variables and constants. And then you create what is known as a graph. So TensorFlow programming consists of graphs and tensors as I mentioned earlier. So this


06:19:18
can be considered ultimately as a tensor and then the graph tells how to execute the whole implementation. So that the execution is stored in the form of a graph and in this case what we are doing is we are doing a multiplication. TF you remember this TF was created as a TensorFlow object here. One more level one more. So TF is available here. Now, TensorFlow has what is known as a matrix multiplication or matal function. So that is what is being used here in this case. So we are using the matrix


06:19:52
multiplication of TensorFlow so that you multiply your input values X with W. Right? This is what we were doing. XW plus B. You're just adding B. And this is in very similar to one of the earlier slides where we saw sigma xi wi. So that's what we are doing here. Matrix multiplication is multiplying all the input values with the corresponding weights and then adding the bias. So that is the graph we created. And then we need to define what is our loss function and what is our optimizer. So


06:20:27
in this case we again use the tensorflow's APIs. So tf.n NN softmax cross entropy with logits is the uh API that we will use and reduce mean is what is like the mechanism whereby which says that you reduce the error and optimizer for doing deduction of the error. What optimizer are we using? So we are using gradient descent optimizer. We discussed about this in couple of slides uh earlier. And for that you need to specify the learning rate. You remember we saw that there was a a slide somewhat


06:21:07
like this. And then you define what should be the learning rate. How fast you need to come down. That is the learning rate. And this again needs to be tested and tried and to find out the optimum level of this learning rate. It shouldn't be very high in which case it will not converge or shouldn't be very low because it will in that case it will take very long. So you define the optimizer and then you call the method minimize for that optimizer and that will kickstart the training process and


06:21:38
so far we've been creating the graph and in order to actually execute that graph we create what is known as a session and then we run that session and once the training is completed we specify how many times how many iterations we want it to run. So for example in this case we are saying thousand steps. So that is a exit strategy in a way. So you specify the exit condition. So a training will run for thousand iterations. And once that is done we can then evaluate the model using some of the techniques shown


06:22:10
here. So let us get into the code quickly and see how it works. So this is our cloud environment. Now you can install TensorFlow on your local machine as well. I'm showing this demo on our existing cloud but you can also install TensorFlow on your local machine and uh there is a separate video on how to set up your TensorFlow environment. You can watch that if you want to install your local environment or you can go for other any cloud service like for example Google cloud, Amazon or cloud labs. Any


06:22:47
of these you can use and u run and try the code. Okay. So it has got started. We will log in. All right. So this is our deep learning tutorial uh code and uh this is our tensorflow environment and uh so let's get started. The first we have seen a little bit of a code walk through uh in the slides as well. Now you will see the actual code in action. So the first thing we need to do is import tensorflow and then we will import the data and we need to adjust the data in such a way that the one hot is encoding is set to true


06:23:36
one hot encoding right as I explained earlier. So in this case the label values will be shown appropriately and if we just check what is the type of the data. So you can see that this is a uh data sets Python data sets and if we check the number of images the way it looks. So this is how it looks. It is an array of type float 32. Similarly the number if you want to see what is the number of training images there are 55,000 then there are test images 10,000 and then validation images 5,000. Now let's take


06:24:16
a quick look at the data itself visualization. So we will use um mattplot lip for this and um if we take a look at the shape now shape gives us like the dimension of the tensors or or or the arrays if you will. So in this case the training data set if we see the size of the training data set using the method shape it says there are 55,000 and 55,000 by 784. So remember this 784 is nothing but the 28x 28 28 into 28. So that is equal to 784. So that's what it is showing. Now we can take just uh one


06:24:55
image and just see what is the the first image and see what is the shape. So again size obviously it is only 784. Similarly you can look at the image itself the data of the first image itself. So this is how it it shows. So large part of it will probably be zeros because as you can imagine in the image only certain areas are written rest is black. So that's why you will mostly see zeros either it is black or white. But then there are these values are so the values are actually they are scaled. So


06:25:30
the values are between 0 and one. Okay. So this is what you're seeing. So certain locations there are some values and then other locations there are zeros. So that is how the data is stored and loaded. If we want to actually see what is the value of the handwritten image. Uh if you want to view it, this is how you view it. So you create like do this reshape and um mattplot lib has this um feature to show you these images. So we will actually use the function called um IM show and then if you pass this parameters appropriately


06:26:12
you will be able to see the different images. Now I can change the values in this position. So which image we are looking at right? So we can say if I want to see what is there in maybe 5,000 right? So 5,000 has three. Similarly you can just say five. What is in five? Five as eight. What is in [Music] 50? Again eight. So basically by the way if you're wondering uh how I'm executing this code shift enter in case you're not familiar with Jupyter notebooks. Shift enter is how you execute each cell


06:26:53
individual cell. And if you want to execute the entire program, you can go here and say run all. So that is how this code gets executed. And um here again we can check what is the maximum value and what is the minimum value of this pixel values. As I mentioned this is it is scaled. So therefore it is between the values lie between 1 and zero. Now this is where we create our model. The first thing is to create the required placeholders and variables and that's what we are doing here as we have


06:27:27
seen in the slides. So we create one placeholder and we create two variables which is for the weights and biases. These two variables are actually matrices. So each variable has 784x 10 actual values. Okay. So one for this 10 is for each neuron there are 10 neurons and 784 is for the pixel values inputs that are given which is 28 into 28 and the biases as I mentioned one for each neuron. So there will be 10 biases they are stored in a variable by the name B. And this is the graph which is basically


06:28:08
the multiplication of these matrix multiplication of X into W and then the bias is added for each of the neurons and the whole idea is to minimize the error. So let me just execute. I think this code is executed. Then we define what is our the Y- value is basically the label value. So this is another placeholder. We had x as one placeholder and y true as a second placeholder and this will have values in the form of uh 10digit 10digit uh arrays and uh since we said one hot encoded the position which has a one value indicates what is


06:28:53
the label for that particular number. All right. Then we have cross entropy which is nothing but the loss loss function and we have the optimizer. We have chosen gradient descent as our optimizer. Then the training process itself. So the training process is nothing but to minimize the cross entropy which is again nothing but the loss function. So we define all of this in the form of a graph. So the up to here remember what we have done is we have not exactly executed any tensorflow code till now we are just preparing the


06:29:34
graph the execution plan that's how the TensorFlow code works. So the whole structure and format of this code will be completely different from how we normally do programming. So even with people with programming experience may find this a little difficult to understand it and it needs quite a bit of practice. So you may want to view this uh video also maybe a couple of times to understand this flow because the way TensorFlow programming is done is slightly different from the normal programming. Some of you who let's say


06:30:11
have done uh maybe Spark programming to some extent will be able to easily understand this. Uh but even in Spark the the programming the code itself is pretty straightforward behind the scenes the execution happens slightly differently but in TensorFlow even the code has to be written in a completely different way. So the code doesn't get executed uh in the same way as you have written. So that that's something you need to understand and a little bit of practice is needed for this. So so far


06:30:43
what we have done up to here is creating the variables and feeding the variables and um or rather not feeding but setting up the variables and uh the graph that's all defining maybe the uh what kind of a network you want to use for example we want to use softmax and so on. So you have created the variables how to load the data, loaded the data, viewed the data and prepared everything but you have not yet executed anything in TensorFlow. Now the next step is the execution in TensorFlow. So the first


06:31:17
step for doing any execution in TensorFlow is to initialize the variables. So anytime you have any variables defined in your code, you have to run this piece of code always. So you need to basically create what is known as a a node for initializing. So this is a node. You still are not yet executing anything here. You just created a node for the initialization. So let us go ahead and create that. And here onwards is where you will actually execute your code uh in TensorFlow. And in order to execute


06:31:53
the code, what you will need is a session. TensorFlow session. So tf session will give you a session. And there are a couple of different ways in which you can do this. But one of the most common methods of doing this is with what is known as a with loop. So you have a with tf do session as says and with a colon here. And this is like a block starting of the block and these indentations tell how far this block goes. And this session is valid till this block gets executed. So that is the purpose of creating this width block.


06:32:35
This is known as a width block. So with tf session as ces you say cs do.run in it. Now cs.run run will execute a node that is specified here. So for example here we are saying ces run. Ces is basically an instance of the session right. So here we are saying tf session. So an instance of the session gets created and we are calling that sess and then we run a node within that one of the nodes in the graph. So one of the nodes here is in it. So we say run that particular node and that is when the initialization of the variables happens.


06:33:20
Now what this does is if you have any variables in your code in our case we have w is a variable and b is a variable. So any variables that we created you have to run this code you have to run the initialization of these variables otherwise you will get an error. Okay so that is the that's what this is doing. Then we within this width block we specify a for loop and we are saying we want the system to iterate for thousand steps and perform the training. That's what this for loop does. Run


06:33:57
training for thousand iterations. And what it is doing basically is it is fetching the data or these images. Remember there are about 50,000 images but it cannot get all the images in one shot because it will take up a lot of memory and performance issues will be there. So this is a very common way of performing deep learning training. You always do in batches. So we have maybe 50,000 images but you always do it in batches of 100 or maybe 500 depending on the size of your system and so on and so forth. So in this case


06:34:35
we are saying okay get me 100 uh images at a time and get me only the training images. Remember we use only the training data for training purpose and then we use test data for test purpose. You must be familiar with machine learning. So you must be aware of this but in case you are not in machine learning also not this is not specific to deep learning but in machine learning in general you have what is known as training data set and test data set. your available data typically you will be splitting into two parts and using


06:35:08
the training data set for training purpose and then to see how well the model has been trained you use the test data set to check or test the validity or the accuracy of the model. So that's what we are doing here and you observe here that we are actually calling an MNEST function here. So we are saying mnest train.n nextbatch right? So this is the advantage of using mnest database because they have provided some very nice helper functions which are readily available otherwise this activity itself


06:35:40
we would have had to write a piece of code to fetch this data in batches that itself is a a lengthy exercise. So we can avoid all that if we are using mnest database and that's why we use this for the initial learning phase. Okay. So when we say fetch what it will do is it will fetch the images into X and the labels into Y and then you use this batch of 100 images and you run the training. So says run basically what we are doing here is we are running the training mechanism which is nothing but


06:36:16
it passes this through the neural network passes the images through the neural network finds out what is the output and if the output obviously the initially it will be wrong so all that feedback is given back to the neural network and thereby all the W's and B's get updated till it reaches thousand iterations in this case the exit criteria is thousand but you can also specify probably accuracy rate or something like that for the as an exit criteria. So here it is it it just says that okay this particular image was


06:36:52
wrongly predicted so you need to update your weights and biases that's the feedback given to each neuron and that is run for thousand iterations and typically by the end of this thousand iterations the model would have learned to recognize these handwritten images obviously it will not be 100% accurate okay so once that is done after so this happens for thousand iterations. Once that is done, you then test the accuracy of these models by using the test data set. Right? So this is what we are


06:37:30
trying to do here. The code may appear a little complicated because if you're seeing this for the first time, you need to understand uh the various methods of TensorFlow and so on. But it is basically comparing the output with what has been what is actually there. That's all it is doing. So you have your test data and uh you're trying to find out what is the actual value and what is the predicted value and seeing whether they are equal or not. TF do equal right and how many of them are correct and so on


06:37:58
and so forth and based on that the accuracy is uh calculated as well. So this is the accuracy and uh that is what we are trying to see how accurate the model is in predicting these uh numbers or these digits. Okay. So let us run this. This entire thing is in one cell. So we will have to just run it in one shot. It may take a little while. Let us see. And u not bad. So it has finished the thousand iterations. And what we see here as an output is the accuracy. So we see that the accuracy of this model is


06:38:36
around 91%. Okay. Now which is pretty good for such a short exercise. Within such a short time we got 90% accuracy. However, in real life this is probably not sufficient. So there are other ways in to increase the accuracy. We will see probably in some of the later tutorials how to improve this accuracy, how to change maybe the hyperparameters like number of neurons or number of layers and so on and so forth. and uh so that this accuracy can be increased beyond 90%. Welcome to the RNN tutorial that's


06:39:15
the recurrent neural network. So we talk about a feed forward neural network. In a feed forward neural network, information flows only in the forward direction from the input nodes through the hidden layers if any and to the output nodes. There are no cycles or loops in the network. And so you can see here we have our input layer. I was talking about how it just goes straight forward into the hidden layers. So each one of those connects and then connects to the next hidden layer, connects to the output layer. And of course we have


06:39:39
a nice simplified version where it has a predicted output and they refer to the input as X a lot of times and the output as Y. Decisions are based on current input, no memory about the past, no future scope. Why recurrent neural network? Issues in feed forward neural network. So one of the biggest issues is because it doesn't have a scope of memory or time. A feed forward neural network doesn't know how to handle sequential data. Uh it only considers only the current input. So if you have a


06:40:08
series of things and because three points back affects what's happening now and what your output affects what's happening. That's very important. So whatever I put as an output is going to affect the next one. Um a feed forward doesn't look at any of that. It just looks at this is what's coming in and it cannot memorize previous inputs. So it doesn't have that list of inputs coming in. Solution to feed forward neural network. You'll see here where it says recurrent neural network and we have our


06:40:31
X on the bottom going to H going to Y. That's your feed forward. Uh but right in the middle it has a value C. So there's a whole another process. So it's memorizing what's going on in the hidden layers. And the hidden layers as they produce data feed into the next one. So your hidden layer might have an output that goes off to Y. Uh but that output goes back into the next prediction coming in. What this does is this allows it to handle sequential data. It considers the current input and also the


06:40:59
previously received inputs. And if we're going to look at general drawings and um solutions, we should also look at applications of the RNN. Image captioning. RNN is used to caption an image by analyzing the activities present in it. A dog catching a ball in midair. Uh that's very tough. I mean, you know, we have a lot of stuff that analyzes images of a dog and they have of a ball, but it's able to add one more feature in there that's actually catching the ball in midair. Time series


06:41:27
prediction. Any time series problem like predicting the prices of stocks in a particular month can be solved using RNN. And we'll dive into that in our use case and actually take a look at some stock. One of the things you should know about analyzing stock today is that it is very difficult. And if you're analyzing the whole stock, the stock market at the New York Stock Exchange in the US produces somewhere in the neighborhood, if you count all the individual trades and fluctuations by the second, um, it's like 3 terabytes a


06:41:56
day of data. So, we're only going to look at one stock. Just analyzing one stock is really tricky. In here, we'll give you a little jump on that. So, that's exciting, but don't expect to get rich off of it immediately. Another application of the RNN is natural language processing. text mining and sentiment analysis can be carried out using RNN for natural language processing. And you can see right here the term natural language processing when you stream those three words together is very different than I if I


06:42:22
said processing language naturally. So the time series is very important when we're analyzing sentiments. It can change the whole value of a sentence just by switching the words around. Or if you're just counting the words, you might get one sentiment where if you actually look at the order they're in, you get a completely different sentiment. When it rains, look for rainbows. When it's dark, look for stars. Both of these are positive sentiments and they're based upon the order of which the sentence is going in.


06:42:48
Machine translation. Given an input in one language, RNN can be used to translate the input into a different languages as output. I myself am very linguistically challenged. But if you study languages and you're good with languages, you know right away that if you're speaking English, you would say big cat. And if you're speaking Spanish, you would say cat big. So that translation is really important to get the right order to get uh there's all kinds of parts of speech that are important to know by the order of the


06:43:17
words. Here this person is speaking in English and getting translated. And you can see here a person is speaking in English and this little diagram. I guess that's denoted by the flags. I have a flag. I own it. No. Um, but they're speaking in English and it's getting translated into Chinese, Italian, French, German, and Spanish languages. Some of the tools coming out are just so cool. So, somebody like myself who's very linguistically challenged, I can now travel into worlds I would never


06:43:45
think of because I can have something translate my English back and forth readily and I'm not stuck with a communication gap. So let's dive into what is a recurrent neural network. Recurrent neural network works on the principle of saving the output of a layer and feeding this back to the input in order to predict the output of the layer. Sounds a little confusing. When we start breaking it down, it'll make more sense. And usually we have a propagation forward neural network with the input layers, the hidden layers, the


06:44:12
output layer. With the recurrent neural network, we turn that on its side. So here it is. And now our X comes up from the bottom into the hidden layers into Y. Okay. And they usually draw it very simplified. X to H with C is a loop. A to Y where A, B, and C are the perimeters. A lot of times you'll see this kind of drawing in here. Digging closer and closer into the H and how it works. Going from left to right, you'll see that the C goes in and then the X goes in. So the X is going upward bound


06:44:41
and C is going to the right. A is going out and C is also going out. That's where it gets a little confusing. So here we have uh x in uh c in and then we have y out and c out and c is based on ht minus one. So our value is based on the y and the h value are connected to each other. They're not necessarily the same value because h can be its own thing. And usually we draw this or we represent it as a function h of t equals a function of c where h of t minus one that's the last h output and x of t


06:45:14
going in. So it's the last output of h combined with the new input of x. Uh where ht is the new state, fc is a function with the parameter c. That's a common way of denoting it. Uh ht minus one is the old state coming out and then x of t is an input vector at time of step t. Well, we need to cover types of recurrent neural networks. And so the first one is the most common one which is a one one single output. one to one neural network is usually known as a vanilla neural network used for regular


06:45:46
machine learning problems. Why? Because vanilla is usually considered kind of a just a real basic flavor. But because it's a very basic a lot of times they'll call it the vanilla neural network uh which is not the common term but it is you know like kind of a slang term. People will know what you're talking about usually if you say that. Then we run one to many. So you have a single input and you might have a uh multiple outputs. In this case, uh, image captioning as we looked at earlier where


06:46:10
we have not just looking at it as a dog, but a dog catching a ball in the air. And then you have many to one network takes in a sequence of inputs. Examples, sentiment analysis where a given sentence can be classified as expressing positive or negative sentiments. And we looked at that as we were discussing if it rains, look for a rainbow. So positive sentiment where rain might be a negative sentiment if you were just adding up the words in there. And then of course if you're going to do a one


06:46:37
one mini to one one to many there's many to many networks takes in a sequence of inputs and generates a sequence of outputs. Example machine translation. So we have a lengthy sentence coming in in English and then going out in all the different languages. Uh you know just a wonderful tool very complicated set of computations. You know if you're a translator you realize just how difficult it is to translate into different languages. One of the biggest things you need to understand when we're


06:47:02
working with this neural network is what's called the vanishing gradient problem. While training an RNN, your slope can be either too small or very large. And this makes training difficult. When the slope is too small, the problem is known as vanishing gradient. And you'll see here they have a nice u image, loss of information through time. So if you're pushing not enough information forward, that information is lost. And then when you go to train it, you start losing the third word in the sentence or something


06:47:31
like that or it doesn't quite follow the full logic of what you're working on. Exploding gradient problem. Oh, this is one that runs into everybody when you're working with this particular neural network. When the slope tends to grow exponentially instead of decaying, this problem is called exploding gradient. Issues in gradient problem. Long training time, poor performance, bad accuracy. And I'll add one more in there. uh your computer if you're on a a lower-end computer testing out a model


06:47:59
will lock up and give you the memory error. Explaining gradient problem. Consider the following two examples to understand what should be the next word in the sequence. The person who took my bike and blank a thief. The students who got into engineering with blank from Asia. And you can see in here we have our x value going in. We have the previous value going forward. And then you back propagate the error like you do with any neural network. And as we're looking for that missing word, maybe


06:48:26
we'll have the person took my bike and blank was a thief and the student who got into engineering with a blank were from Asia. Consider the following example. The person who took the bike. So we'll go back to the person who took the bike was blank a thief. In order to understand what would be the next word in the sequence, the RNN must memorize the previous context whether the subject was singular noun or a plural noun. So was a thief is singular. the student who got into engineering. Well, in order to


06:48:54
understand what would be the next word in the sequence, the RNN must memorize the previous context, whether the subject was singular noun or a plural noun. And so, you can see here the students who got into engineering with blank were from Asia. It might be sometimes difficult for the error to back propagate to the beginning of the sequence to predict what should be the output. So, when you run into the gradient problem, we need a solution. The solution to the gradient problem. First, we're going to look at exploding


06:49:22
gradient where we have three different solutions depending on what's going on. One is identity initialization. So, the first thing we want to do is see if we can find a way to minimize the identities coming in instead of having it identify everything just the important information we're looking at. Next is to truncate the back propagation. So, instead of having uh whatever information it's sending to the next series, we can truncate what it's sending. we can lower that particular uh


06:49:50
set of layers, make those smaller. And finally is a gradient clipping. So when we're training it, we can clip what that gradient looks like and narrow the training model that we're using. When you have a vanishing gradient, the option problem, uh we can take a look at weight initialization. Very similar to the identity, but we're going to add more weights in there so it can identify different aspects of what's coming in better. Choosing the right activation function, that's huge. So we might be


06:50:16
activating based on one thing and we need to limit that. We haven't talked too much about activation functions. So we'll look at that just minimally. Uh there's a lot of choices out there. And then finally there's long shortterm memory networks the LSTMS and we can make adjustments to that. So just like we can clip the gradient as it comes out we can also um expand on that. We can increase the memory network the size of it so it handles more information. And one of the most common problems in


06:50:45
today's uh setup is what they call long-term dependencies. Suppose we try to predict the last word in the text. The clouds are in the and you probably said sky. Here we do not need any further context. It's pretty clear that the last word is going to be sky. Suppose we try to predict the last word in the text. I have been staying in Spain for the last 10 years. I can speak fluent. Maybe you said Portuguese or French. No, you probably said Spanish. The word we predict will depend on the previous few words in context. Here we


06:51:14
need the context of Spain to predict the last word in the text. It's possible that the gap between the relevant information and the point where it is needed to become very large. LSTMs help us solve this problem. So the LSTMs are a special kind of recurrent neural network capable of learning long-term dependencies. Remembering information for long periods of time is their default behavior. All recurrent neural networks have the form of a chain of repeating modules of neural network connections. In standard RNNs, this


06:51:47
repeating module will have a very simple structure such as a single tangent H layer. LSTMs's also have a chain-like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four interacting layers communicating in a very special way. LSTMS are a special kind of recurrent neural network capable of learning long-term dependencies. Remembering information for long periods of time is their default behavior. LSTMS's also have a chain-like structure, but the


06:52:17
repeating module has a different structure. Instead of having a single neural network layer, there are four interacting layers communicating in a very special way. As you can see, the deeper we dig into this, the more complicated the graphs get. In here, I want you to note that you have x at t minus one coming in. You have x of t coming in and you have x at t + one. And you have h of t minus one and h of t coming in and h of t + one going out. And of course uh on the other side is the output a. Um in the middle we have


06:52:48
our tangent h but it occurs in two different places. So not only when we're computing the x of t + one are we getting the tangent h from x of t, but we're also getting that value coming in from the x of t minus one. So the short of it is as you look at these layers, not only does it does the propagate through the first layer goes into the second layer back into itself, but it's also going into the third layer. So now we're kind of stacking those up. And this can get very complicated as you


06:53:17
grow that in size. It also grows in memory too and in the amount of resources it takes. Uh but it's a very powerful tool to help us address the problem of complicated long sequential information coming in like we were just looking at in the sentence. And when we're looking at our long shortterm memory network uh there's three steps of processing processing in the LSTMs that we look at. The first one is we want to forget irrelevant parts of the previous state. you know, a lot of times like,


06:53:44
you know, is, as in, a, unless we're trying to look at whether it's a plural noun or not, they don't really play a huge part in the language. So, we want to get rid of them. Then, selectively update cell state values. So, we only want to update the cell state values that reflect what we're working on. And finally, we want to put only output certain parts of the cell state. So, whatever is coming out, we want to limit what's going out, too. And let's dig a little deeper into this. Let's just see


06:54:10
what this really looks like. Uh so step one decides how much of the past it should remember. First step in the LSTM is to decide which information to be omitted in from the cell in that particular time step. It is decided by the sigmoid function. It looks at the previous state h at t minus one and the current input x of t and computes the function. So you can see over here we have a function of t equals the sigmoid function of the weight of f the h at t minus one and then x of t plus of course you have a bias in there with any of our


06:54:44
neural networks. So we have a bias function. So f of t equals forget gate decides which information to delete that is not important from the previous time step. Considering an STM is fed with the following inputs from the previous and present time step. Alice is good in physics. John on the other hand is good in chemistry. So previous output, John plays football well. He told me yesterday over the phone that he had served as a captain of his college football team. That's our current input. So as we look at this, the first step is


06:55:13
the forget gate realizes there might be a change in context after encounting the first full stop. Compares with the current input sentence of XAT. So we're looking at that full stop and then compares it with the input of the new sentence. The next sentence talks about John. So the information on Alice is deleted. Okay, that's important to know. So we have this input coming in. And if we're going to continue on with John, then that's going to be the primary information we're looking at. The


06:55:40
position of the subject is vacated and is assigned to John. And so in this one, we've seen that we've weeded out a whole bunch of information and we're only passing information on John since that's now the new topic. So step two is then to decide how much should this unit add to the current state. In the second layer, there are two parts. One is a sigmoid function and the other is a tangent h. In the sigmoid function, it decides which values to let through zero or one. Tangent h function gives a


06:56:07
weightage to the values which are passed deciding their level of importance minus1 to one. And you can see the two formulas that come up. Uh the i of t equals the sigmoid of the weight of i h of t minus one x of t plus the bias of i. And the C of T equals the tangent of H of the weight of C of H of T minus one X of T plus the bias of C. So our I of T equals the input gate determines which information to let through based on its significance in the current time step. If this seems a little complicated,


06:56:39
don't worry cuz a lot of the programming is already done when we get to the case study. Understanding though that this is part of the program is important when you're trying to figure out these what to set your settings at. You should also note when you're looking at this, it should have some semblance to your forward propagation neural networks where we have a value assigned to a weight plus a bias. Very important steps in any of the neural network layers, whether we're propagating into them the


06:57:05
information from one to the next or we're just doing a straightforward neural network propagation. Let's take a quick look at this what it looks like from the human standpoint. Um, as I step out in my suit again, consider the current input at XT. John plays football. Well, he told me yesterday over the phone that he had served as a captain of his college football team. That's our input. Input gate analysis the important information. John plays football and he was a captain of his college team is important. He told me


06:57:33
over the phone yesterday is less important. Hence, it is forgotten. This process of adding some new information can be done via the input gate. Now, this example is as a human form, and we'll look at training this stuff in just a minute. Uh, but as a human being, if I wanted to get this information from a conversation, maybe it's a Google voice listening in on you or something like that. Um, how do we weed out the information that he was talking to me on the phone yesterday? Well, I don't want


06:57:59
to memorize that he talked to me on the phone yesterday. Or maybe that is important, but in this case, it's not. I want to know that he was the captain of the football team. I want to know that he served. I want to know that John plays football and he was the captain of the college football team. Those are the two things that I want to take away as a human being. Again, we measure a lot of this from the human viewpoint and that's also how we try to train them so we can understand these neural networks.


06:58:21
Finally, we get to step three. Decides what part of the current cell state makes it to the output. The third step is to decide what will be our output. First, we run a sigmoid layer which decides what parts of the cell state make it to the output. Then we put the cell state through the tangent h to push the values to be between minus1 and one and multiply it by the output of the sigmoid gate. So when we talk about the output of t, we set that equal to the sigmoid of the weight of zero of the h of t minus one back one step in time by


06:58:54
the x of t plus of course the bias. The h of t equals the out of t times the tangent of the tangent h of c of t. So our o equals the output gate allows the passedin information to impact the output in the current time step. Let's consider the example to predicting the next word in the sentence. John played tremendously well against the opponent and one for his team. For his contributions, brave blank was awarded player of the match. There could be a lot of choices for the empty space. Current input brave is an adjective.


06:59:26
Adjectives describe a noun. John could be the best output after brave. Thumbs up for John. Awarded player of the match. And if you were to pull just the nouns out of the sentence, team doesn't look right because that's not really the subject. We're talking about contributions. Uh you know, brave contributions or brave team, brave player, brave match. Um so you look at this and you can start to train this these this neural network. So it starts looking at and goes, "Oh no, John is


06:59:52
what we're talking about. So brave is an adjective. Jon's going to be the best output." and we give John a big thumbs up. And then of course we jump into my favorite part, the case study. Use case implementation of LSTM. Let's predict the prices of stocks using the LSTM network based on the stock price data between 2012 2016. We're going to try to predict the stock prices of 2017. And this will be a narrow set of data. We're not going to do the whole stock market. It turns out that the New York Stock


07:00:24
Exchange generates roughly three terabytes of data per day. That's all the different trades up and down of all the different stocks going on and each individual one uh second to second or nancond to nanocond. Uh but we're going to limit that to just some very basic fundamental information. So don't think you're going to get rich off this today. But you at least you can give an eye you can give a step forward in how to start processing something like stock prices. a very valid use for machine learning in


07:00:52
today's markets. Use case implementation of LSTM. Let's dive in. We're going to import our libraries. We're going to import the training set and uh get the scaling going. Um now, if you watch any of our other tutorials, a lot of these pieces should start to look very familiar because it's very similar setup. Uh but let's take a look at that. And um just a reminder, we're going to be using Anaconda, the Jupiter notebook. So here I have my Anaconda navigator. When we go under environments, I've


07:01:22
actually set up a KAS Python 3.6. I'm in Python 3.6. And u nice thing about Anaconda, especially the newer version. I remember a year ago messing with Anaconda in different versions of Python and different environments. Um Anaconda now has a nice interface. Um and I have this installed both on a Ubuntu Linux machine and on Windows, so it works fine on there. You can go in here and open a terminal window. And then in here, once you're in the terminal window, this is where you're going to start uh


07:01:49
installing using pip to install your different modules and everything. Now, we've already pre-installed them, so we don't need to do that in here. Uh but if you don't have them installed in your particular environment, you'll need to do that. And of course, you don't need to use the Anaconda or the Jupiter. You can use whatever favorite Python IDE you like. I'm just a big fan of this because it keeps all my stuff separate. You can see on this machine I have specifically installed one for K crossass since we're


07:02:14
going to be working with KAS under TensorFlow. We go back to home. I've gone up here to application and that's the environment I've loaded on here and then we'll click on the launch Jupyter notebook. Now I've already in my Jupyter notebook um have set up a lot of stuff so that we're ready to go. Kind of like uh Martha Stewart's in the old cooking shows. We want to make sure we have all our tools for you so you're not waiting for them to load. And uh if we go up here to where it says new, you can see


07:02:40
where you can u create a new Python 3. That's what we did here underneath the setup. So it already has all the modules installed on it. And I'm actually renamed this. So if you go under file, you can rename it. We I'm calling it RNN stock. And let's just take a look at start diving into the code. Let's get into the exciting part. Now we've looked at the tool. And of course, you might be using a different tool, which is fine. Uh let's start putting that code in there and seeing what those imports and


07:03:04
uploading everything looks like. Now, first half is kind of boring when we hit the run button because we're going to be importing numpy as np that's uh uh the number python which is your numpy array and the mattplot library. So, we're going to do some plotting at the end and our pandas for our data set. Our pandas is pd. And when I hit run, uh it really doesn't do anything except for load those modules. Just a quick note, let me just do a quick uh draw here. Whoops. Shift alt. There we go. You'll notice


07:03:31
when we're doing this setup, if I was to divide this up, oops, I'm gonna actually um let's overlap these. Here we go. Uh this first part that we're going to do is our data prep. A lot of prepping involved. Um in fact, depending on what your system is, since we're using Kass, I put an overlap here. Uh but you'll find that almost maybe even half of the code we do is all about the data prep. And the reason I overlap this with uh KAS, let me just put that down because that's


07:04:10
what we're working in. Uh is because KAS has like their own preset stuff. So it's already pre-built in which is really nice. So there's a couple steps a lot of times that are in the KAS setup. Uh we'll take a look at that to see what comes up in our code as we go through and look at stock. And then the last part is to evaluate. And if you're working with um shareholders or uh you know classroom whatever it is you're working with uh the evaluate is the next biggest piece. Um so the actual code


07:04:37
here cross is a little bit more but when you're working with uh some of the other packages you might have like three lines that might be it all your stuff is in your pre-processing in your data. Since KASS has is is cutting edge and you load the individual layers, you'll see that there's a few more lines here and cross is a little bit more robust. And then you spend a lot of times uh like I said with the evaluate. You want to have something you present to everybody else to say, hey, this is what I did. This is


07:05:02
what it looks like. So let's go through those steps. This is like a kind of a just general overview and let's just take a look and see what the next set of code looks like. And in here we have a data set train and it's going to be read using the PD or pandas read CSV and it's a Google stockpric train.csv. And so under this we have training set equals data set train.loation and we've kind of sorted out part of that. So what's going on here? Let's just take a look at that.


07:05:31
Let's let's look at the actual file and see what's going on there. Now, if we look at this, uh, ignore all the extra files on this. Um, I already have a train and a test set where it's sorted out. This is important to notice because a lot of times we do that as part of the pre-processing of the data. We take 20% of the data out so we can test it and then we train the rest of it. That's what we use to create our neural network. That way, we can find out how good it is. Uh, but let's go ahead and


07:05:56
just take a look and see what that looks like as far as the file itself. And I went ahead and just opened this up in a basic word pad and text editor just so we can take a look at it. Certainly you can open up in Excel or any other kind of spreadsheet. Um and we note that this is a commaepparated variables. We have a date uh open, high, low, close, volume. This is the standard stuff that we import into our stock with the most basic set of information you can look at in stock. It's all free to download. Um


07:06:24
in this case, we downloaded it from uh Google. That's why we call it the Google stock price. Um and it specifically is Google. This is the Google stock values from uh as you can see here we started off at 1320. So when we look at this first setup up here uh we have a data set train equals pd_csv. And if you noticed on the original frame uh let me just go back there. They had it set to home Ubuntu downloads Google stock price train. I went ahead and changed that because we're in the same file where I'm running


07:06:58
the code. So, I've saved this particular Python code and I don't need to go through any special paths or have the full path on there. And then, of course, we want to take out um certain values in here. And you're going to notice that we're using um our data set and we're now in pandas. Uh so, pandas basically it looks like a spreadsheet. Um, and in this case, we're going to do I location, which is going to get specific locations. The first value is going to show us that we're pulling all the rows


07:07:30
in the data. And the second one is we're only going to look at columns one and two. And if you remember here from our data, as we switch back on over columns, we always start with zero, which is the date. And we're going to be looking at open and high, which would be one and two. I'll just label that right there so you can see. Now, when you go back and do this, you certainly can extrapolate and do this on all the columns. Um, but for the example, let's just limit a little bit here so that we can focus on just


07:08:02
some key aspects of stock. And then we'll go up here and run the code. And uh, again, I said the first half is very boring. Whenever we hit the run button, it doesn't do anything because we're still just loading the data and setting it up. Now that we've loaded our data, we want to go ahead and scale it. We want to do what they call feature scaling. And in here, we're going to pull it up from the SKLearn or the SK kit pre-processing import minm max scaler. And when you look at this, you got to remember that


07:08:33
um biases in our data, we want to get rid of that. So if you have something that's like a really high value, um let's just draw a quick graph. And I have something here like the maybe the stock has a value one stock has a value of a 100 and another stock has a value of five. Um you start to get a bias between different stocks. And so when we do this we go ahead and say okay 100 is going to be the max and five is going to be the min and then everything else goes and then we change this. So we just squish


07:09:04
it down. I like the word squish. So it's between one and zero. So 100 equals 1 or 1 equals 100 and 0 equals 5. And you can just multiply. It's usually just a simple multiplication. We're using uh multiplication. So it's going to be uh minus5 and then 100 divided or 95 divided by one. So or whatever value is is divided by 95. And uh once we've actually created our scale, we've telling it's going to be from 0 to one. We want to take our training set and we're going to create a training set


07:09:37
scaled. And we're going to use our scaler SC and we're going to fit we're going to fit and transform the training set. Uh so we can now use the SC this this particular object. We'll use it later on our testing set because remember we have to also scale that when we go to test our uh model and see how it works. And we'll go ahead and click on the run again. Uh it's not going to have any output yet because we're just setting up all the variables. Okay. Okay, so we pasted the


07:10:02
data in here and we're going to create the data structure with the 60 time steps and output. First note, we're running 60 time steps and that is where this value here also comes in. So the first thing we do is we create our X train and Y train variables and we set them to an empty Python array. Very important to remember what kind of array we're in and what we're working with. And then we're going to come in here. We're going to go for I in range 60 to 1258. There's our 60 60 time steps. And


07:10:34
the reason we want to do this is as we're adding the data in there, there's nothing below the 60. So if we're going to use 60 time steps, uh we have to start at 60 because it includes everything underneath of it. Otherwise, you'll get a pointer error. And then we're going to take our X train and we're going to append training set scaled. This is a scaled value between 0 and one. And then as I is equal to 60, this value is going to be um 60 - 60 is zero. So this actually is 0 to I. So


07:11:04
it's going to be 0 to 60, 1 to 61. Let me just circle this part right here. 1 to 61, uh 2 to 62, and so on and so on. And if you remember, I said 0 to 60. That's incorrect because it does not count. Remember, it starts at zero. So this is a count of 60. So it's actually 59. Important to remember that as we're looking at this. And then the second part of this that we're looking at. So if you remember correctly, here we go. we go from uh 0 to 59 of i and then we have a comma a zero right here. And so


07:11:36
finally we're just going to look at the open value. Now I know we did put it in there for one to two. Um if you remember correctly it doesn't count the second one. So it's just the open value we're looking at just open. Um and then finally we have ytrain.append training set i to zero. And if you remember correctly I to or I comma zero. If you remember correctly, this is 0 to 59. So there's 60 values in it. Uh so when we do I down here, this is number 60. So we're going to do this


07:12:06
is we're creating an array and we have 0 to 59. And over here we have number 60 which is going into the Y train. It's being appended on there. And then this just goes all the way up. So this is down here is uh uh 0 to 59 and we'll call it 60 since that's the value over here and it goes all the way up to 12 58. That's where this value here comes in. That's the length of the data we're loading. So we've loaded two arrays. We loaded one array that has uh which is filled with arrays from 0 to 59. And we


07:12:43
loaded one array which is just the value. And what we're looking at, you want to think about this as a time sequence. Uh here's my open open open. What's the next one in the series? So we're looking at the Google stock and each time it opens we want to know what the next one uh 0 through 59 what's 60 1 through 60 what's 61 2 through 62 what's 62 and so on and so on going up. And then once we've loaded those in our for loop we go ahead and take xra and yrain equals nparray x-train. NP array ytrain.


07:13:16
We're just converting this back into a numpy array. That way, we can use all the cool tools that we get with numpy array, including reshaping. So, if we take a look and see what's going on here, we're going to take our x train. We're going to reshape it. Wow. What the heck does reshape mean? Uh, that means we have an array, if you remember correctly, um, so many numbers by 60. That's how wide it is. And so, we're when you when you do x-train.shape, shape that gets one of the shapes and


07:13:47
you get um x-train.shape of one gets the other shape. And we're just making sure the data is formatted correctly. And so you use this to pull the fact that it's 60 by um in this case, where's that value? 60 by 1199. 1258us 60 1199. And we're making sure that that is shaped correctly. So the data is grouped into uh 1199 by 60 different arrays. And then the one on the end just means at the end because this when you're dealing with shapes and numpy they look at this as layers and so


07:14:22
the end layer needs to be one value that's like the leaf of a tree where this is the branch and then it branches out some more um and then you get the leaf. np.resshape comes from and using the existing shapes to form it. We'll go ahead and run this piece of code. Again, there's no real output. And then we'll import our different cross modules that we need. So from cross models, we're going to import the sequential model. We're dealing with sequential data. We have our dense layers. We have actually


07:14:51
three layers. We're going to bring in our dense, our LSTM, which is what we're focusing on, and our dropout. And we'll discuss these three layers more in just a moment. But you do need the with the LSTM, you do need the dropout, and then the final layer will be the dense. But let's go ahead and run this and that'll bring our modules. And you'll see we get an error on here. And if you read it closer, it's not actually an error. It's a warning. What does this warning mean?


07:15:14
These things come up all the time when you're working with such cutting edge modules are completely being updated all the time. We're not going to worry too much about the warning. All it's saying is that the H5PY module, which is part of KAS, is going to be updated at some point. And uh if you're running new stuff on KAS and you start updating your KASS system, you better make sure that your H5 PI is updated too. Otherwise, you're going to have an error later on. And you can


07:15:40
actually just run an update on the H5 PI now if you wanted to. Not a big deal. We're not going to worry about that today. And I said we were going to jump in and start looking at what those layers mean. I meant that. And uh we're going to start off with initializing the RNN. And then we'll start adding those layers in. And you'll see that we have the LSTM and then the dropout. LSTM then dropout. LSTM then dropout. What the heck is that doing? So let let's explore that. We'll start by initializing the


07:16:08
RNN regressor equals sequential because we're using the sequential model. And we'll run that and load that up. And then we're going to start adding our LSTM layer and some dropout regularization. And right there should be the Q dropout regularization. And if we go back here and remember our exploding gradient, well, that's what we're talking about. The uh dropout drops out unnecessary data so we're not just shifting huge amounts of data through um the network. So, and so we go


07:16:36
in here. Let's just go ahead and uh add this in. I'll go ahead and run this. And we had three of them. So, let me go ahead and put all three of them in. And then we can go back over them. There's the second one. And let's put one more in. So, let's put that in. And we'll go and put uh two more in. I meant to put I said one more in, but it's actually two more in. And then let's add one more after that. And as you can see, each time I run these, they don't actually


07:16:57
have an output. So let's take a closer look and see what's going on here. So we're going to add our first LSTM layer in here. We're going to have units 50. The units is the positive integer and it's the dimensionality of the output space. This is what's going out into the next layer. So we might have 60 coming in, but we have 50 going out. We have a return sequence because it is a sequence data. So we want to keep that true. And then you have to tell it what shape it's


07:17:22
in. Well, we already know the shape by just going in here and looking at X train shape. So input shape equals the X train shape of one comma one. Makes it really easy. You don't have to remember all the numbers that put in 60 or whatever else is in there. You just let it tell the regressor what model to use. And so we follow our STM with a dropout layer. Now understanding the dropout layer is kind of exciting because one of the things that happens is we can overtrain our network. That means that


07:17:50
our neural network will memorize such specific data that it has trouble predicting anything that's not in that specific realm. To fix for that, each time we run through the training mode, we're going to take 02 or 20% of our neurons, they just turn them off. So, we're only going to train on the other ones. And it's going to be random. That way, each time we pass through this, we don't overtrain. These nodes come back in in the next training cycle. We randomly pick a different 20. And


07:18:17
finally, I see a big difference as we go from the first to the second and third and fourth. The first thing is we don't have to input the shape because the shapes already the output units is 50 here. This auto the next step automatically knows this layer is putting out 50. And because it's the next layer, it automatically sets that and says, oh, 50 is coming out from our last layer that's coming out, you know, goes into the regressor. And of course, we have our dropout and that's what's


07:18:41
coming into this one. And so on and so on. And so the next three layers, we don't have to let it know what the shape is. It automatically understands that. And we're going to keep the units the same. We're still going to do 50 units. It's still a sequence coming through. 50 units and a sequence. Now, the next piece of code is what brings it all together. Let's go ahead and take a look at that. And we come in here, we put the output layer, the dense layer. And if you remember up here, we had the three


07:19:06
layers. We had uh LSTM, dropout, and dense. uh dense just says we're going to bring this all down into one output. Instead of putting out a sequence, we just know want to know the answer at this point. And let's go ahead and run that. And so in here, you notice all we're doing is setting things up one step at a time. So far, we've brought in our uh way up here, we brought in our data. We brought in our different modules. We formatted the data for training it. We set it up. You know, we


07:19:32
have our yx train and our y train. We have our source of data and the answers we're we know so far that we're going to put in there. We've reshaped that. We've come in and built our cross. We've imported our different layers. And we have in here, if you look, we have what uh five total layers. Now, KAS is a little different than a lot of other systems because a lot of other systems put this all in one line and do it automatic, but they don't give you the options of how those layers interface


07:19:59
and they don't give you the options of how the data comes in. KAS is cutting edge for this reason. So even though there's a lot of extra steps in building the model, this has a huge impact on the output and what we can do with this these new models from KAS. So we brought in our dense, we have our full model put together, our regressor. So we need to go ahead and compile it and then we're going to go ahead and fit the data. We're going to compile the pieces so they all come together and then we're


07:20:25
going to run our training data on there and actually recreate our regressor so it's ready to be used. So let's go ahead and compile that. And I can go ahead and run that. And uh if you've been looking at any of our other tutorials on neural networks, you'll see we're going to use the optimizer atom. Atom is optimized for big data. There's a couple other optimizers out there uh beyond the scope of this tutorial, but certainly Adam will work pretty good for this. And loss equals mean squared value. So when we're


07:20:51
training it, this is what we want to base the loss on. How bad is our error? We're going to use the mean squared value for our error and the atom optimizer for its differential equations. You don't have to know the math behind them, but certainly it helps to know what they're doing and where they fit into the bigger models. And then finally, we're going to do our fit. Fitting the RNN to the training set. We have the regressor.fit, X-rain, Yra, epics, and batch size. So, we know where


07:21:16
this is. This is our data coming in for the X-ray. Our Y train is the answer we're looking for of our data, our sequential input. Epics is how many times we're going to go over the whole data set. We created a whole data set of X-ray. So this is each each of those rows which includes a time sequence of 60 and batch size. Another one of those things where KAS really shines is if you were pulling this say from a large file instead of trying to load it all into RAM, it can now pick smaller batches up


07:21:45
and load those indirectly. We're not worried about pulling them off a file today because this isn't big enough to uh cause the computer too much of a problem to run. Not too straining on the resources. But as we run this, you can imagine what would happen if I was doing a lot more than just one column in one set of stock. In this case, Google stock. Imagine if I was doing this across all the stocks and I had instead of just the open, I had open, close, a high, low, and you can actually find


07:22:11
yourself with about 13 different variables times 60 because there's a time sequence. suddenly you find yourself with a gig of memory you're loading into your RAM which will just completely, you know, if it's just if you're not on multiple computers or cluster, you're going to start running into resource problems. But for this, we don't have to worry about that. So, let's go ahead and run this. And this will actually take a little bit on my computer cuz it's an older laptop. And


07:22:34
give it a second to kick in there. There we go. All right. So, we have epic. So, this is going to tell me it's running the first run through all the data. And as it's going through, it's batching them in 32 pieces. So 32 uh lines each time and there's 1198. I think I said 1199 earlier, but it's 1198. I was off by one. And each one of these is 13 seconds. So you can imagine this is roughly 20 to 30 minutes run time on this computer. Like I said, it's an older laptop running at uh 0.9 GHz on a


07:23:02
dual processor. And that's fine. What we'll do is I'll go ahead and stop, go get a drink of coffee, and come back and let's see what happens at the end and where this takes us. And like any good cooking show. I've gone and gotten my latte. I also had some other stuff running in the background. So you'll see these numbers jumped up to like 19 seconds, 15 seconds. But you can scroll through and you can see we've run it through 100 steps or 100 epics. So the question is what does all this mean? One


07:23:26
of the first things you'll notice is that our loss can is over here. It kind of stopped at 0.0014, but you can see it kind of goes down until we hit about 0.014 three times in a row. So we guessed our epic pretty close since our loss has remained the same on there. So to find out what we're looking at, we're going to go ahead and load up our test data, the test data that we didn't process yet, and real stock price data set test location. This is the same thing we did when we prepped the data in


07:23:52
the first place. So let's go ahead and go through this code. And we can see we've labeled it part three, making the predictions and visualizing the results. So the first thing we need to do is go ahead and read the data in from our test CSV. You see I've changed the path on it for my computer. And uh then we'll call it the real stock price. And again, we're doing just the one column here and the values from IO location. So it's all the rows and just the values from these


07:24:19
that one location. That's the open stock open. And let's go ahead and run that. So that's loaded in there. And then let's go ahead and uh create. We have our inputs. We're going to create inputs here. And this should all look familiar. This is the same thing we did before. We're going to take our data set total. We're going to do a little pandas concat from the data set train. Now remember the end of the data set train is part of the data going in. And let's just


07:24:43
visualize that just a little bit. Here's our train data. Let me just put tr for train. And it went up to this value here. But each one of these values generated a bunch of columns. It was 60 across. And this value here equals this one. And this value here equals this one. And this value here equals this one. And so we need these top 60 to go into our new data. So to find out what we're looking at, we're going to go ahead and load up our test data, the test data that we didn't process yet.


07:25:12
And a real stock price data set test I location. This is the same thing we did when we prepped the data in the first place. So let's go ahead and go through this code. And we can see we've labeled it part three, making the predictions and visualizing the results. So the first thing we need to do is go ahead and read the data in from our test CSV. You see I've changed the path on it for my computer. And uh then we'll call it the real stock price. And again we're doing just the one column here and the


07:25:40
values from IO location. So it's all the rows and just the values from these that one location. That's the open stock open. And let's go ahead and run that. So that's loaded in there. And then let's go ahead and uh create we have our inputs. We're going to create inputs here. And this should all look familiar. This is the same thing we did before. We're going to take our data set total. We're going to do a little pandas concat from the data set train. Now remember


07:26:04
the end of the data set train is part of the data going in. And let's just visualize that just a little bit. Here's our train data. Let me just put tr for train. And it went up to this value here. But each one of these values generated a bunch of columns. It was 60 across. And this value here equals this one. And this value here equals this one. And this value here equals this one. And so we need these top 60 to go into our new data cuz that's part of the next data or it's actually the top 59.


07:26:34
So that's what this first setup is over here is we're going in. We're doing the real stock price and we're going to just take the data set test and we're going to load that in. And then the real stock price is our data test location. So we're just looking at that first uh column, the open price. And then our data set total, we're going to take pandas and we're going to concat. And we're going to take our data set train for the open and our data set test open.


07:26:58
And this is one way you can reference these columns. We've referenced them a couple different ways. We've referenced them up here with the one two, but we know it's labeled as a panda set as open. So pandas is great that way. Lots of versatility there. And we'll go ahead and go back up here and run this. There we go. And uh you'll notice this is the same as what we did before. We have our open data set. We pended our two different or concatenated our two data sets together. We have our inputs equals


07:27:22
data set total length. Data set total minus length of data set minus test minus 60 values. So we're going to run this over all of them. And you'll see why this works because normally when you're running your test set versus your training set, you run them completely separate. But when we graph this, you'll see that we're just going to be we'll be looking at the part that uh we didn't train it with to see how well it graphs. And we have our inputs equals inputs reshapes or reshaping like we did


07:27:47
before. We're transforming our inputs. So if you remember from the transform between 0 and one and uh finally want to go ahead and take our X test and we're going to create that X test and for I in range 60 to 80. So here's our X test and we're appending our inputs I to 60 which remember is 0 to 59 and I comm, 0 on the other side. So it's just the first column which is our open column. And uh once again we take our X test, we convert it to a numpy array. We do the same reshape we did before. And uh then


07:28:16
we get down to the final two lines. And here we have something new right here on these last two lines. Let me just highlight those or or mark them. Predicted stock price equals regressor.predictsx test. So we're predicting all the stock including both the training and the testing model here. And then we want to take this prediction and we want to inverse the transform. So remember we put them between zero and one. Well, that's not going to mean very much to me to look at a at a float number between 0 and one. And I want the


07:28:42
dollar amounts. I want to know what the cash value is. And we'll go ahead and run this. And you'll see it runs much quicker than the training. That's what's so wonderful about these neural networks. Once you put them together, takes just a second to run the same neural network that took us what, a half hour to train. Head and plot the data. We're going to plot what we think it's going to be and we're going to plot it against the real data. What what the Google stock actually did. So let's go


07:29:04
ahead and take a look at that in code. And let's uh pull this code up. So we have our PLT. That's our uh oh, if you remember from the very beginning, let me just go back up to the top. We have our mattplot library.pipplot as plt. That's where that comes in. And we come down here. We're going to plot. Let me get my drawing thing out again. We're going to go ahead and plt is basically kind of like an object. It's one of the things that always threw me when I'm doing graphs in Python because I always think


07:29:31
you have to create an object and then it loads that class in there. Well, in this case, PLT is like a canvas you're putting stuff on. So if you've done HTML 5, you'll have the canvas object. This is the canvas. So we're going to plot the real stock price. That's what it actually is. And we're going to give that color red. So it's going to be in bright red. We're going to label it real Google stock price. And then we're going to do our predicted stock. And we're


07:29:55
going to do it in blue. And it's going to be labeled predicted. And we'll give it a title because it's always nice to give a title to your uh graph, especially if you're going to present this to somebody, you know, to your uh shareholders in the office. And uh the x label is going to be time because it's a time series. And we didn't actually put the actual date and times on here. But that's fine. We just know they're incremented by time. And then of course the y label is the actual stock price.


07:30:18
plt.leend tells us to build the legend on here so that the color red and and real Google stock price show up on there. And then the plot shows us that actual graph. So let's go ahead and run this and see what that looks like. And you can see here we have a nice graph. And let's talk just a little bit about this graph before we wrap it up. Here's our legend I was telling you about. That's why we have the legend showed the prices. We have our title and everything. And you'll notice on the


07:30:43
bottom we have a time sequence. We didn't put the actual time in here. Now, we could have we could have gone ahead and um plotted the x since we know what the the dates are and plotted this to dates, but we also know this only the last piece of data that we're looking at. So last piece of data which ends somewhere probably around here on the graph. I think it's like about 20% of the data probably less than that. We have the Google price and the Google price has this little up jump and then


07:31:10
down. And you'll see that the actual Google instead of uh a turndown here just didn't go up as high and didn't low go uh down. So our prediction has the same pattern but the overall value is pretty far off as far as um stock. But then again we're only looking at one column. We're only looking at the open price. We're not looking at how many volumes were traded. Like I was pointing out earlier when we talk about stock. Just right off the bat, there's six columns. There's open, high, low, close,


07:31:38
volume. Then there's whether uh I mean volume shares. Then there's the adjusted open, adjusted high, adjusted low, adjusted close. They have a special formula to predict exactly what it would really be worth based on the value of the stock. And then from there, there's all kinds of other stuff you can put in here. So, we're only looking at one small aspect, the opening price of the stock. And as you can see here, we did a pretty good job. This curve follows the curve pretty well. It has like, you


07:32:04
know, little jumps on it, bends. They don't quite match up. So, this bend here does not quite match up with that bend there, but it's pretty darn close. We have the basic shape of it, and the prediction isn't too far off. And you can imagine that as we add more data in and look at different aspects in the specific domain of stock, we should be able to get a better representation each time we drill in deeper. Of course, this took a half hour for my program, my computer to train. So you can imagine


07:32:31
that if I was running it across all those different variables, might take a little bit longer to train the data. Not so good for doing a quick tutorial like this. So we're going to dive right into what is KAS. We'll also uh go all the way through this into a couple of tutorials because that's where you really learn a lot is when you roll up your sleeves. So we talk about what is KAS. Kass is a high-level deep learning API written in Python for easy implement implementation of neural networks. It


07:32:58
uses deep learning frameworks such as TensorFlow, PyTorch, etc. as backend to make computation faster. And this is really nice because as a uh programmer there is so much stuff out there and it's evolving so fast it can get confusing and having some kind of highlevel order in there we can actually view it and easily program these different neural networks uh is really powerful. It's really powerful to to um uh have something out really quick and also be able to start testing your models and seeing where you're going.


07:33:32
So, Kass works by using complex deep learning frameworks such as TensorFlow, PyTorch, um, ML played, etc. as a backend for fast computation while providing a userfriendly and easy tolearn front end. And you can see here we have the KASS API uh, specifications and under that you'd have like TF Karass for TensorFlow, Thano Karass and so on and then you have your TensorFlow workflow that this is all sitting on top of. And this is, like I said, it organizes everything. The heavy lifting is still done by TensorFlow or whatever,


07:34:07
you know, underlying package you put in there. And this is really nice because you don't have to um dig as deeply into the heavy stuff while still having a very robust package. You can get up and running rather quickly and it doesn't distract from the processing time because all the heavy lifting is done by packages like TensorFlow. This is the organization on top of it. So the working principle of KASS uh the working principle of KASS is Kass uses computational graphs to express and evaluate mathematical expressions.


07:34:42
You can see here we put them in blue. They have the expression u expressing complex problems as a combination of simple mathematical operators uh where we have like the percentage or in this case in Python that's usually your uh left your remainder or multiplication uh you might have the operator of x uh to the power of.3 and it uses useful for calculating derivatives by using uh back propagation. So if we're doing with neural networks, we send the error back up to figure out how to change it. Uh


07:35:13
this makes it really easy to do that without really having not banging your head and having to hand write everything. It's easier to implement distributed computation and for solving complex problems uh specify input and outputs and make sure all nodes are connected. And so this is really nice as you come in through is that um as your layers are going in there, you can get some very complicated uh different setups nowadays, which we'll look at in just a second. And this just makes it really easy to start spinning this stuff


07:35:43
up and trying out the different models. So we look at cross models. Uh cross model, we have a sequential model. Sequential model is a linear stack of layers where the previous layer leads into the next layer. And this, if you've done anything else, even like the sklearn with their neural networks and propagation and any of these setups, this should look familiar. You should have your input layer. It goes into your layer 1, layer two, and then to the output layer. And it's useful for simple classifier decoder


07:36:14
models. And you can see down here we have the model equals a crosssequential. And this is the actual code. You can see how easy it is. Uh we have a layer that's dense. your layer one as an activation. Uh they're using the RLU in this particular example. And then you have your name layer one, layer dense RLU, name layer two, and so forth. Uh and they just feed right into each other. So it's really easy just to stack them as you can see here. And it automatically takes care of everything


07:36:40
else for you. And then there's a functional model. And this is really where things are at. This is new. Make sure you update your KAS or you'll find yourself running this um doing the functional model. you'll run into an error code because this is a fairly new release and it uses multi-input and multi-output model. The complex model which forks into two or more branches and you can see here we have our image inputs equals your cross input shape equals 32x 32x3. You have your uh dense layers


07:37:12
dense 64 activation ru similar to what you already saw before. Uh but if you look at the graph on the right, it's going to be a lot easier to see what's going on. You have two different inputs. Uh and one way you could think of this is maybe one of those is a small image and one of those is a full-sized image. And that feedback goes into you might feed both of them into one node because it's looking for one thing and then only into one node for the other one. And so you can start to get kind of an idea that there's a


07:37:44
lot of use for this kind of split and this kind of setup uh where you have multiple information coming in but the information's very different even though it overlaps and you don't want it to send it through the same neural network. Um and they're finding that this trains faster and is also has a better result depending on how you split the data up and and how you fork the models coming down. And so in here we do have the two complex uh models coming in. Uh we have our image inputs which is a 32x 32x3 or


07:38:15
three channels or four if you're having an alpha channel. Uh you have your dense your layers dense is 64 activation using the ru uh x equals dense inputs x layers dense x64 activation equals ru x outputs equals layers dense 10 x model equals cross model inputs equals inputs outputs equals outputs name equals minced model. Uh so we add a little name on there and again this is this kind of split here. This is setting us up to um have the input go into different areas. So if you're already looking at caress, you


07:38:52
probably already have this answer. What are neural networks? Uh but it's always good to get on the same page and for those people who don't fully understand neural networks to dive into them a little bit or do a quick overview. Neural networks are deep learning algorithms modeled after the human brain. They use multiple neurons which are mathematical operations to break down and solve complex math problems. And so just like the neuron, one neuron fires in and it fires out to all these other neurons or nodes as we


07:39:22
call them. And eventually they all come down to your output layer. And you can see here we have that really standard graph input layer, a hidden layer, and an output layer. One of the biggest parts of any data processing is your data prep-processing. Uh so we always have to touch base on that with a neural network like many of these models. They're kind of uh when you first start using them they're like a black box. You put your data in, you train it and you test it and see how good it was and you have to pre-process


07:39:56
that data because bad data in is uh bad outputs. So in data prep-processing we will create our own data examples set with KAS. The data consists of a clinical trial conducted on 2100 patients ranging from ages 13 to 100 with a the patients under 65 and the other half over 65 years of age. We want to find the possibility of a patient experiencing side effects due to their age. And you can think of this in today's world with uh COVID uh what's going to happen on there. And we're going to go ahead and do an example of


07:40:30
that in our uh live hands-on. Like I said, most of this you really need to have hands-on to understand. So, let's go ahead and bring up our Anaconda and uh open that up and open up a Jupyter notebook for doing the Python code in. Now, if you're not familiar with those, you can use pretty much any of your uh setups. I just like those for doing demos and uh showing people, especially shareholders. It really helps because it's a nice visual. So, let me go and flip over to our Anaconda. And the


07:40:58
Anaconda has a lot of cool tools. They just added data lore and IBM Watson Studio cloud into the Anaconda framework, but we'll be in the Jupyter Lab or Jupyter Notebook. Um, I'm going to do Jupyter Notebook for this because I use the lab for like large projects with multiple pieces because it has multiple tabs where the notebook will work fine for what we're doing. And this opens up in our browser window because that's how Jupyter notebook Jupyter notebook is set to run. and we'll go


07:41:28
under new create a new Python 3 and uh it creates an untitled Python. We'll go ahead and give this a title and we'll just call this uh KAS tutorial. And let's change that to capital. There we go. We'll go and just rename that. And the first thing we want to go ahead and do is uh get some pre-processing tools involved. And so we need to go ahead and import some stuff for that like our numpy do some random number generation. Um I mentioned sklearn or your scikite kit if you're


07:42:04
installing sklearn the sklearn stuff it's a scite kit you want to look up that should be a tool of anybody who is um doing data science. If you if you're not if you're not familiar with the sklearn toolkit, it's huge. Uh but there's so many things in there that we always go back to. And we want to go ahead and create some train labels and train samples uh for training our data. And then just a note of what we're we're actually doing in here. Uh let me go ahead and change this. This is kind


07:42:39
of a fun thing you can do. We can change the code to markdown. And then markdown code is nice for doing examples once you've already built this. Uh our example data we're going to do experimental there we go experimental drug was tested on 2100 individuals between 13 to 100 years of age. Half the participants under 65 and 95% of participants are under 65 experience no side effects. Well 95% of participants over 65 um experience side effects. So, that's kind of where we're starting at.


07:43:15
Um, and this is just a real quick example because we're going to do another one with a little bit more uh complicated information. Uh, and so we want to go ahead and generate our setup. Uh, so we want to do for I and range and we want to go ahead and create, if you look here, we have random integers, train the labels, append. So, we're just creating some random data. Uh, let me go ahead and just run that. And so once we've created our random data and if you if I mean you can certainly ask for a copy of the code


07:43:50
from simply learn they'll send you a copy of this or you can zoom in on the video and see how we went ahead and did our train samples a pin um and we're just using this I do this kind of stuff all the time. I was running a thing on uh that had to do with errors following a bellshaped curve on uh a standard distribution error. And so what do I do? I generate the data on a standard distribution error to see what it looks like and how my code processes it since that was the baseline I was looking for. In this we're just


07:44:21
doing uh uh generating random data for our setup on here. And uh we could actually go in uh print some of the data. Let's just do this. Print um we'll do [Music] train samples and we'll just gen do the first um five pieces of data in there to see what that looks like. And you can see the first five pieces of data in our train samples is 49 85 41 681 19. Just random numbers generated in there. That's all that is. Uh and we generated significantly more than that. Um let's see 50 up here. 1,000. Yeah. So, there's


07:44:59
1,00 here. 1,00 numbers we generated. And we could also, if we wanted to find that out, we could do a quick uh print the length of it. And so, or you could do a shape kind of thing. And if you're using numpy, although the length for this is just fine. And there we go. It's actually 2100 like we said in the demo setup in there. And then we want to go ahead and take our labels. Oh, that was our train labels. We also did samples, didn't we? Uh, so we could also print do the same thing.


07:45:39
Labels. Um, and let's change this to labels and [Music] labels. And run that just to double check. And sure enough, we have 2100. And they're labeled 1 0 1 0 1 0. I guess that's if they have symptoms or not. One symptoms uh 0 none. And so we want to go ahead and take our train labels and we'll convert it into a numpy array. And the same thing with our samples. And let's go ahead and run that. And we also shuffle. Uh this is just a neat feature you can do in uh numpy right here. Put


07:46:21
my drawing thing on which I didn't have on earlier. Um, I can take the data and I can shuffle it. Uh, so we have our So it's it just randomizes it. That's all that's doing. Um, we've already randomize it so it's kind of an overkill. It's not really necessary. But if you're doing uh a larger package where the data is coming in and a lot of times it's organized somehow and you want to randomize it just to make sure that that you know the input doesn't follow a certain pattern uh that might


07:46:53
create a bias in your model. And we go ahead and create a scaler uh the scaler range uh minimum max scaler feature range 0 to one. Uh then we go ahead and scale the uh scaled train samples. So we're going to go ahead and fit and transform the data uh so it's nice and scaled and that is the age. Uh so you can see up here we have 49 85 41. We're just moving that so it's going to be uh between zero and one. And so this is true with any of your neural networks. You really want to convert the


07:47:24
data uh to zero and one otherwise you create a bias. Uh so if you have like a 100 creates a bias versus the math behind it gets really complicated. Uh if you actually start multiplying stuff there's a lot of multiplication addition going on in there that higherend value will eventually multiply down and it will have a huge bias as to how the model fits it and then it will not fit as well. And then one of the fun things we can do in Jupyter Notebook is that if you have a variable and you're not doing


07:47:56
anything with it, it's the last one on the line, it will automatically print. Um, and we're just going to look at the first five samples on here. And so it's going to print the first five samples. And you can see here we go uh 95 791. So everything's between zero and one. And that just shows us that we scaled it properly and it looks good. Uh it really helps a lot to do these kind of printups halfway through. Uh you never know what's going to go on there. I don't know how many tents I've


07:48:29
gotten down and found out that the data sent to me that I thought was scaled was not and then I have to go back and track it down and figure it out on there. Uh so let's go ahead and create our artificial neural network. And for doing that, this is where we start diving into TensorFlow and KAS. Uh TensorFlow, if you don't know the history of TensorFlow, it helps to uh jump into we'll just use Wikipedia. Careful, don't quote Wikipedia on these things because you get in trouble. Uh but it's a good place


07:49:07
to start. Uh back in 2011, Google Brain built disbelief as a proprietary machine learning setup. TensorFlow became the open source for it. Uh so TensorFlow was a Google product and then it became uh open sourced and now it's just become probably one of the de facto when it comes for neural networks as far as where we're at. Uh so when you see the TensorFlow setup, it it's got like a huge following. There are some other setups like a um the sidekit under the skarn has their own little neural network. uh


07:49:42
but the TensorFlow is the most robust one out there right now and KAS sitting on top of it makes it a very powerful tool so we can leverage both the KAS uh easiness in which we can build a sequential setup on top of TensorFlow. And so in here we're going to go ahead and do our input of TensorFlow. Uh and then we have the rest of this is all KAS here from number two down. Uh we're going to import from TensorFlow the KAS uh connection and then you have your TensorFlow cross models import sequential. It's a


07:50:18
specific kind of model. We'll look at that in just a second. If you remember from the files, that means it goes from one layer to the next layer to the next layer. There's no funky splits or anything like that. Uh and then we have from TensorFlow across layers. We're going to import our activation and our dense layer. And we have our optimizer, Adam. Um, this is a big thing to be aware of how you optimize uh your data. When you first do it, Adam's as good as any. Atom is usually uh there's a number of


07:50:51
optimizer out there. There's about uh there's a couple main ones, but atom is usually assigned to bigger data. Uh it works fine. Usually the lower data does it just fine, but atom is probably the mostly used, but there are some more out there. And depending on what you're doing with your layers, your different layers might have different activations on them. And then finally down here, you'll see um our setup where we want to go ahead and use the metrics. And we're going to use the TensorFlow cross


07:51:19
metrics um for categorical cross entropy. Uh so we can see how everything performs when we're done. That's all that is. Um, a lot of times you'll see us go back and forth between TensorFlow and then Scikit has a lot of really good metrics also for measuring these things. Um, again it's the end of the, you know, at the end of the story. How good does your model do? And we'll go ahead and load all that. And then comes the fun part. Um, I actually like to spend hours messing with these


07:51:49
things. And uh, four lines of code. You're like, "Ah, you're going to spend hours on four lines of code." Um, no. We don't spend hours on four lines of code. That's not what we're talking about when I say spend hours on four lines of code. Uh what we have here, I'm going to explain that in just a second. We have a model and it's a sequential model. If you remember correctly, we mentioned the sequential up here where it goes from one layer to the next. And our first


07:52:14
layer is going to be your input. It's going to be uh what they call dense, which is u usually it's just dense. And then you have your input and your activation. Um, how many units are coming in? We have 16. Uh, what's the shape? What's the activation? And this is where it gets interesting. Uh, because we have in here uh, ReLU on two of these and softmax activation on one of these. There are so many different options for what these mean um, and how they function. How does the Reu, how


07:52:53
does the softmax function? and they do a lot of different things. Um, we're not going to go into the activations in here. That is what really you spend hours doing is looking at these different activations. Um, and just some of it is just um almost like you're playing with it like an artist. You start getting a feel for like a uh inverse tangent activation or the 10H activation takes up a huge processing amount. Uh so you don't see it a lot yet. It comes up with a better solution especially when you're doing uh when


07:53:31
you're analyzing word documents and you're tokenizing the words. And so you'll see this shift from one to the other because you're both trying to build a better model and if you're working on a huge data set um you'll crash the system. It'll just take too long to process. Um, and then you see things like softmax. Uh, softmax generates an interesting um, setup where a lot of these when you talk about RLU it Oops, let me do this. Uh, ROU. There we go. RLU has um a setup


07:54:05
where if it's less than zero, it's zero and then it goes up. Um, and then you might have what they call lazy uh, setup where it has a slight negative to it so that the errors can translate better. Same thing with softmax. It has a slight laziness to it so that errors translate better. All these little details make a huge different on your model. Um so one of the really cool things about data science that I like is you build your uh what they call you build to fail. And it's an interesting


07:54:38
uh design setup. Oops, I forgot the um end of my code here. The concept of build a fail is you want the model as a whole to work. So you can test your model out so that you can do uh you can get to the end and you can do your let's see where was it overshot down here. You can test your test out how the quality of your setup on there and see where did I do my tensorflow. Oh, here we go. I just it was right above me. There we go. we start doing your cross entropy and stuff like that is you need a full functional set of


07:55:15
code so that when you run it you can then test your model out and say hey it's either this model works better than this model and this is why um and then you can start swapping in these models and so when I say spend a huge amount of time on pre-processing data is probably 80% of your programming time um well between those two it's like 8020 you'll spend a lot of time on the models Once you get the model down, once you get the whole code and the flow down, uh, set, depending on your data,


07:55:46
your models get more and more robust as you start experimenting with different inputs, different data streams, and all kinds of things. And we can do a simple model summary here. Uh, here's our sequential, here's our layer, our output, a parameter. This is one of the nice things about KAS is you just, you can see right here, here's our sequential one model. Boom, boom, boom, boom. everything set and clear and easy to read. So once we have our model built, uh the next thing we're going to want to


07:56:16
do is we're want to go ahead and uh train that model. And so the next step is of course model training. And when we come in here, this a lot of times it's just paired with the model because it's so straightforward. It's nice to print out the model setup so you can have a tracking. But here's our model. Uh the keyword in Kass is compile optimizer atom learning rate. Another term right there that we're just skipping right over that really becomes the meat of uh the setup is your


07:56:54
learning rate. Uh so whoops, I forgot that I had an arrow, but I'll just underline it. A lot of times the learning rate set to 0.0 uh set to 0.01 01. Uh depending on what you're doing, this learning rate um can overfit and underfit. Uh so you'd want to look up I know we have a number of tutorials out on overfitting and underfitting that are really worth reading once you get to that point and understanding. And we have our loss um sparse categorical cross entropy. So this is going to tell how far to go


07:57:28
until it stops. And then we're looking for metrics of accuracy. So we'll go ahead and run that. And now that we've compiled our model, we want to go ahead and um run it, fit it. So here's our model fit. Um we have our scaled to train samples, our train labels, our validation split. Um in this case, we're going to use 10% of the data for validation. Uh batch size, another number you kind of play with. Not a huge difference as far as how it works, but it does affect how long it takes to run


07:58:05
and it can also affect the bias a little bit. Uh most of the time though a batch size is between 10 to 100 um depending on just how much data you're processing in there. We want to go ahead and shuffle it. Uh we're going to go through 30 epics and uh put a verbose of two. Let me just go ahead and run this. And you can see right here, here's our epic. Here's our training. Um here's our loss. Now, if you remember correctly up here, we set the loss. See, where was it? Um, compiled our


07:58:35
data. There we go. Loss. Uh, so it's looking at the sparse categorical cross entropy. This tells us that as it goes, how how how much um how how much does the um error go down? Uh is the best way to look at that. And you can see here, the lower the number, the better. It just keeps going down. And vice versa. Accuracy. We want let's see where's my accuracy value accuracy at the end. Uh and you can see 619 69.74 it's going up. We want the accuracy would be ideal if it made it all the way to one. But we also the loss


07:59:15
is more important because it's a balance. Um you can have 100% accuracy and your model doesn't work because it's overfitted. Uh again, you want to look up overfitting and underfitting models. And we went ahead and went through uh 30 epics. It's always fun to kind of watch your code going. Um to be honest, I usually uh uh the first time I run it, I'm like, "Oh, that's cool. I get to see what it does." And after the second time of running it, I'm like, "I'd like to just not see that." And you


07:59:47
can repress those, of course, in your code. Uh repress the warnings in the printing. And so the next step is going to be building a test set and predicting it. Now, uh so here we go. We want to go ahead and build our test set. And we have uh just like we did our training set. A lot of times you just split your your initial setup. Uh but we'll go ahead and do a separate set on here. And this is just what we did above. Uh there's no difference as far as um the randomness that we're using to


08:00:20
build this set on here. Uh the only difference is that we already uh did our scalar up here. Well, it doesn't matter because the the data is going to be across the same thing, but this should just be just transform down here instead of fit transform. Uh because you don't want to refit your data um on your testing data. There we go. So now we're just transforming it because you never want to transform the test data. Um easy mistake to make especially on an example like this where we're not doing um you


08:00:57
know we're randomizing the data anyway. So doesn't matter too much because we're not expecting something weird. And then we want ahead and do our predictions. The whole reason we built the model is we take our model, we predict, and we're going to do here's our Xcale data batch size 10 verbose. And now we have our predictions in here. And we could go ahead and do a um oh, we'll print predictions. And then I guess I could just put down predictions and five so we can look at the first five of the


08:01:33
predictions. And what we have here is we have our age and uh the prediction on this age versus on what is what we think it's going to be what what we think is going to going to have uh symptoms or not. And the first thing we notice is that's hard to read because we really want a yes no answer. Uh so we'll go ahead and just uh round off the predictions using the argmax um the numpy arg max uh for predictions. And so it just goes to zero one. And if you remember this is a Jupyter notebook. So


08:02:06
I don't have to put the print. I can just put in uh rounded predictions. And we'll just do the first five. And you can see here 0 1 0 0. So that's what the predictions are that we have coming out of this um is no symptoms, symptoms, no symptoms, symptoms, no symptoms. And just as uh we were talking about at the beginning, we want to go ahead and um take a look at this. There we go. confusion matrixes for accuracy check. Uh most important part when you get down to the end of the story, how accurate is


08:02:40
your model before you go and play with the model and see if you can get a better accuracy out of it. And for this we'll go ahead and use the scikit u the sklearn metrics. Uh scikit being where that comes from. import confusion matrix, uh some iteration tools, and of course a nice map plot library that makes a big difference. So it's always nice to um have a nice graph to look at. Um picture is worth a thousand words. Um and then we'll go ahead and do call it CM for confusion matrix. Y true equals


08:03:13
test labels y predict rounded predictions. And we'll go ahead and load in our CM. And I'm not going to spend too much time on the plotting um going over the different plotting code. Uh you can spend uh like whole we have whole tutorials on how to do your different plotting on there. Uh but we do have here is we're going to do a plot confusion matrix. There's our CM our classes normalized false title confusion matrix. Cap is going to be in blues. And you can see here we have uh to the


08:03:51
nearest cap titles, all the different pieces whether you put tick marks or not, the marks, the classes, the color bar. Um so a lot of different information on here as far as how we're doing the printing of the of the confusion matrix. You can also just dump the confusion matrix um into a seaborn and real quick get an output. It's worth knowing how to do all this uh when you're doing a presentation to the shareholders. You don't want to do this on the fly. You want to take the time to


08:04:20
make it look really nice uh like our guys in the back did. And uh let's go ahead and do this. Forgot to put together our CM plot labels. We'll go ahead and run that. And then we'll go ahead and call the little the definition for our mapping. And you can see here plot confusion matrix. That's our the the little script we just wrote. And we're going to dump our data into it. Um, so our confusion matrix, our classes, um, title confusion matrix. And let's just go ahead and run


08:04:53
that. And you can see here we have our basic setup. Uh, no side effects, 195 had side effects, uh, 200, no side effects that had side effects. So we predicted that 10 of them who actually had side effects. And that's pretty good. I mean, I I don't know about you, but you know, that's 5% error on this. And this is because there's 200 here. That's where I get 5% is uh divide these both by by two and you get five out of 100. Uh you can do the same kind of math up here. Not as quick on the fly because


08:05:27
it's 15 and 195. Not an easily rounded number, but you can see here where they have 15 people who predicted to have no uh with the no side effects but had side effects kind of set up on there. And these confusion matrix are so important at the end of the day. This is really where where you show uh whatever you're working on comes up and you can actually show them hey this is how good we are or not how messed up it is. So this was a uh I spent a lot of time on some of the parts uh but you can


08:06:02
see here is really simple. uh we did the random generation of data, but when we actually built the model coming up here, uh here's our model summary and we just have the layers on here that we built with our model on this and then we went ahead and trained it and ran the prediction. Now, we can get a lot more complicated. Uh let me flip back on over here because we're going to do another uh demo. So, that was our basic introduction to it. We talked about the uh oops, here we go. Okay, so implementing a neural network


08:06:31
with KASS. After creating our samples and labels, we need to create our KASS neural network model. We will be working with a sequential model which has three layers. And this is what we did. We had our input layer, our hidden layers, and our output layers. And you can see the input layer uh coming in uh was the age factor. We had our hidden layer, and then we had the output. Are you going to have symptoms or not? So, we're going to go ahead and go with something a little bit more complicated. Uh training our


08:06:58
model is a two-step process. We first compile our model and then we train it in our training data set. Uh so we have compiling. Compiling converts the code into a form of understandable by machine. We used the atom in the last example a gradient descent algorithm to optimize a model. And then we trained our model which means it let it uh learn on training data. Uh, and I actually had a little backwards there, but this is what we just did is we, if you remember from our code we just had, Oops, let me go back


08:07:28
here. Um, here's our model that we created summarized. Uh, we come down here and we compile it. So, it tells it, hey, we're ready to build this model and use it. Uh, and then we train it. This is the part where we go ahead and fit our model and and put that information in here. there and it goes through the training on there and of course we scaled the data which was really important to do and then you saw we did the creating a confusion matrix with KAS um as we are performing classifications


08:08:01
on our data we need a confusion matrix to check the results a confusion matrix breaks down the various misclassifications as well as correct classifications to get the accuracy um and so you can see here this is what we did with a true positive false positive true negative false negative And that is what we went over. Let me just scroll down here on the end when we printed it out. And you can see we have a nice print out of our confusion matrix uh with the true positive, false positive, false negative, true negative. And so the blue


08:08:35
ones uh we want those to be the biggest numbers because those are the better side. And then uh we have our false predictions on here uh as far as this one. So it had no side effects but we predicted let's see no side effects predicting side effects and vice versa. So here's the open a documentation and you could see the new features introduced with the chat GP4. So these are the improvements. Uh one is the updated and interactive bar graphs or pie charts that you can create. And these are the features that


08:09:07
you could see here. You could change the color. You could download it. And what we have is you could update the latest file versions directly from Google Drive. and Microsoft one drive and we have the interaction with tables and charts in an new expandable view that I showed you here that is here you can expand it in the new window and you can customize and download charts for presentations and documents more you can create the presentation also that we'll see in further and here we have how data


08:09:37
analysis works in chat GBT you could directly upload the files from Google drive and Microsoft one drive I will show you Guys, how we can do that? And where's this option is? And we can work on tables in real time. And there we have customized presentation ready charts that is you can create a presentation with all the charts based on a data provided by you and moreover a comprehensive security and privacy feature. So with that guys, we'll move to chat GPT and here we have the chat for version. So before


08:10:13
commencing guys there's a quick info for you. If you're one of the aspiring data analyst looking for online training and graduating from the best universities or a professional who elicits to switch careers with data analytics by learning from the experts then try giving a sh to simply learn Purdue postgraduate program in data analytics in collaboration with IBM. You can find the link in the description box and pin comment. So let's get started with data analysis part. So this is the pin section or the


08:10:41
insert section where you can have the options to connect to Google drive, connect to Microsoft one drive and you can upload it from the computer. This option was already there that is upload from computer and you can upload at least or at max the 10 files that could be around excel files or documents. So the max limit is 10 and if you have connected to Google drive I'll show you guys uh I'm not connecting you but you guys can connect it too and you could upload it from there also and there's another cool update that is


08:11:18
ability to code directly in your chat. Uh so while chatting with chat GPT I'll show you guys how we can do that and you could find some new changes that is in the layout. So this is the profile section. It used to be at the left bottom but now it's moved to the top right and making it more accessible than ever. So let's start with the data analysis part and the first thing we need is data. So you can find it on kegle or you could ask chatgypt forro to provide the data. I will show you guys.


08:11:53
So this is the kegle website. You can sign in here and click on data sets. You can find all the data sets here that would be around computer science, education, classification, computer vision or else you could move back to chat GPT and you could ask the chat GP forum model to generate a data and provide it in Excel format. So we'll ask him we'll not ask him can you we'll just ask him provide a data set that I can use for data analysis and provide in CSV format. So you could see that it has


08:12:34
responded that I can provide a sample data set and he has started generating the data set here. So you could see that he has provided only 10 rows and he is saying that I will now generate this data set in CSV format. First he has provided the visual presentation on the screen and now he's generating the CSV format. So if you want more data like if you want 100 rows or thousand rows you could specify in the prompt and chat GP will generate that for you. So we already have the data. I will import that data. You could import it


08:13:09
from here or else you can import it from your Google drive. So we have a sales data here. We will open it. So we have the sales data here. So the first step we need to do is data cleaning. So this is the crucial step to ensure that the accuracy of our analysis is at its best. So we can do that by handling missing values. That is missing values can distort our analysis. And here chat GBT4 can suggest methods to impute these values such as using the mean, median or a sophisticated approach based on data patterns. And after


08:13:45
handling the missing values, we will remove duplicates and outlier detection. So we'll ask JPD clean the data if needed. So we can just write a simple prompt that would be clean the data if needed. And this is also a new feature. You can see the visual presentation of the data here that we have 100 rows here and the columns provided that is sales ID, date, product, category, quantity and price per unit and total sales. So this is also a new feature that okay uh we just headed back. We'll move back to our chat GP


08:14:30
chat here. Okay, so here we are. So you could see that Judge Gypy has cleaned the data and he has provided that it has checked for missing values, checked for duplicates and ensured consistent formatting and he's saying okay. Okay. So now we will ask him that execute these steps and provide the clean data as chip has provided that these are the steps to clean the data and let's see so he has provided a new CSV file with the clean sales data we will download it and Ask him to use the same


08:15:28
file only. Use this new cleaned sales data CSV file for further analysis. So you could see that he is providing what analysis we can do further. But once our data is clean, the next step is visualization. So visualizations help us understand the data better by providing a graphical representation. So the first thing we will do is we will create a prompt for generating the histograms and we'll do that for the age distribution part. So we'll write a prompt that generate a histogram. Generate a histogram to


08:16:18
visualize the distribution of customer ages. to visualize the distribution of customer ages. And what I was telling you guys is this code button. If you just select the text and you would find this reply section. Just click on that and you could see that it has selected the text or what you want to get all the prompts started with chat GPD. So we'll make it cross and you could see that it has provided the histogram here and these are the new features here and we could see that he's providing a


08:17:06
notification that interactive charts of this type are not yet supported. that is histogram don't have the color change option. I will show you the color change option in the bar chart section. So these features are also new. You can download the chart from here only. And this is the expand chart. If you click on that you could see that you could expand the chart here and continue chat with chat gypty here. So this is the interactive section. So you could see that he has provided the histogram that is showing the


08:17:36
distribution of customer ages and the age range are from 18 to 70 years with the distribution visualized in 15 bins that he has created 15 bins here. And now moving to another visualization that we'll do by sales region. So before that I will open the CSV file that is provided by the chat GPT. So you guys can also see what data he has provided. So this is the clean sales data and you could see that we have columns sales ID, date, product, category, quantity, price per item, total sales region and


08:18:17
salesperson. So now moving back to chat. So now we will create a bar chart showing total sales by region. So we'll enter this prompt that create a bar chart showing total sales by region. So what we are doing here is we are creating bar charts or histogram charts but we can do that for only two columns. If we want to create these data visualization charts we need two columns to do so. So you could see that he has provided the response and created the bar chart here. And this is the interactive section. You could see that


08:18:59
here's an option to switch to static chart. If we click on that, we can't like we are not getting any information. If we scroll on that and if I enable this option, you could see that I can visually see how many numbers this bar is indicating. And after that, we have the change color section. You can change the color of the data set provided. So we can change it to any color that is provided here or you could just write the color code here. And similarly we have other two options that is download and under is


08:19:33
the expand chart section. And if you need uh what code it has done to figure out this bar graph. So this is the code you could use any ID to do so. If you don't want the presentations or the visualizations of the bar charts here, you could use your ID and use the Python language and he will provide the code for you. Just take your data set and read it through pandas and generate the bar charts. So moving to next section that is category wise sales section. So here we will generate a pie chart showing the


08:20:14
proportion of sales for each product category. So for that we'll write a prompt generate a pie chart showing the proportion of sales for each product category. So you could see that it has started generating the pie chart and this is also an interactive section. If you click on that you would be seeing a static pie chart. And if you want to change the color you can change for any section that could be clothing, electronics, furniture or kitchen. And similarly we have the download section and the expand chart


08:21:05
section. So this is how this new chat JPD 4 model is better than chat JPD4. that you could use a more interactive pie charts. You could change the colors for that and you can just hover over these bar charts and found all the information according to them. So after this data visualization, now we'll move to statistical analysis. So this will help us uncover patterns and relationships in the data. So the first thing we'll do is correlation analysis and for that we'll write the prompt


08:21:39
analyze the correlation between age and purchase amount. So this correlation analysis help us understand the relationship between two variables. So this can indicate if older customers tend to spend more or less. So we will find out that by analyzing the data and we'll provide a promp to chat gypy that analyze the correlation between age and purchase amount. So let's see what it provides. Uh so here's the response by charg. You could see a scatter plot that shows the relationship between customer


08:22:24
age and total sales. That is with a calculated correlation coefficient of approximately 0.16. So this indicates a weak positive correlation between age and purchase amount suggesting that as customer age increases there's a slight tendency for total sales to increase as well. So you could just see the scatter plot here that if the age increases so it is not correlated to sales as you could see an empty graph here. So till 40 to 50 years of age or the 70 years of age you could find what amount they have spent here


08:23:03
that is the total sales accumulated by these ages. So now moving to sales trend. So here we will perform a time series analysis of purchase amount over the given dates. So what does this do is time series analysis allows us to examine how sales amount changes over time helping us identify trends and seasonal patterns. So for that we'll write a prompt perform a time series analysis of purchase amount over given dates. So you could see that Chad Gypty has provided us the response and here's


08:23:59
the time series plot showing total sales over the given dates and each point on the plot represents the total sales for a particular day. So through this you can find out and the businesses find out which is the seasonal part of the year and where to stock up their stocks for these kind of dates and after that you could also do customer segmentation. So what does this do is so we can use clustering here to segment customers based on age, income and purchase amount. So clustering groups customers into segments based on


08:24:36
similarities. This is useful for targeted marketing and personalized services. And after that we have the advanced usage for data analysis. Here we can draw a predictive modeling table and do the market basket analysis and perform a customer lifetime value analysis. So we will see one of those and what we'll do is we'll perform a market basket analysis and perform an association rule mining to find frequently bought together products. So the theory behind this is the association rule mining helps identify


08:25:12
patterns of products that are often purchased together aiding in inventory management and cross-selling strategies. So for that we'll write a prompt that so perform an association rule mining to find frequently bought together products. So for that we'll write a prompt here. perform an association rule mining to find frequently bought products together. So let's see for this prompt what does chat GP4 respond to us. Uh so you could see that he's providing a code here but we don't need


08:25:56
a code here. We need the analysis. Don't provide code. Do the market basket analysis and provide visualizations. So you could see that uh Chad Gypty has provided the response that given the limitations in this environment. So he's not able to do the market basket analysis here. So but he can help us how we can perform this in an id. So he's providing you can install the required libraries then prepare the data and here is providing the example code. So you could see there are some limitations to


08:26:53
chat gypy 4 also that it can't do advanced data analysis. So you could use the code in your ID and do the market basket analysis there. So there are some limitations to chat GP4 also. And now we will ask chat GPT can you create a presentation based on the data set and we'll provide a data set to it also. So we'll provide uh sample sales data and we'll ask him can you create a presentation or PowerPoint presentation based on this data set and only provide data visualization. graphs. So you can see that JP4 has


08:27:58
started analyzing the data and he is stating that and he will start by creating a data visualization from the provided data set and compile them into PowerPoint presentation. So you could see that charge JF4 has provided us the response and these are all the presentations or the bar graphs that he has created and now we have downloaded the presentation here. We will open that and here's the presentation that is created by chat GP for Sora is here. OpenAI has introduced Sora, an advanced AI tool for creating videos, now


08:28:49
available at sora.com. Earlier this year, Sora was launched to turn text into realistic videos, showcasing exciting progress in AI technology. Now, OpenAI has released Sora Turbo, a faster and more powerful version available to chat GBT Plus and Pro users. Sora lets user create videos in 1080p quality up to 20 second long and in different formats like widescreen, vertical or square. It includes tools like a storyboard for precise control and options to remix or create videos from scratch. There is


08:29:27
also a community section with featured and recent videos to spark ideas. Chat Beauty Plus users can make up to 50 videos per month at 480p resolution while pro user get access to more features like higher resolution and longer video duration. While Sora Turbo is much faster, OpenAI is still working to improve areas like handling complex action and making the technology more affordable to ensure safe and ethical use. Sora includes features like visible watermarks, content moderation, and metadata to identify videos created with


08:30:04
Sora. Sora makes it easier for people to create and share stories through video. OpenAI is excited to see how user will explore new creative possibilities with the powerful tool. So, welcome to the demo part of the Sora. So, this is the landing page when you will log in Sora. So let me tell you I have the charge plus version not the pro version. So I have some 721 credits left. Okay. Uh later on I will tell you what are the credits. Okay. So let's explore something here. So these are some recent


08:30:39
videos which I have created or tested you can say. And this featured version is all the users of Sora which are creating videos. So it's coming under featured. So we can learn or we can generate some new ideas like this. Okay. Like this parallet and all like this is very cool for learning and these are some the saved version and these are all videos and uploads like this. So let's come into the credit parts. Okay. So you can see I have 721 credit left. So if you will go this help openai.com page


08:31:16
and this page you can see what are the credit. So credits are used to generate videos with Sora. Okay. So if you will create 480p square 5-second video it will take only 20 credits. Okay. For 10 it will take 40. Then this then this. Okay. For 480p uh this much credit 25 credit 50 credit like this 720 it is different 1080p different. Okay. So here it is written please note that requesting multiple variation at once will be charged at the same rate as running two separate generation request. Okay. So here this


08:31:50
plus icon you can see. So here you can upload the image or video. Okay. So you can also do like this. You can upload the image and you can create the video from that image. Okay. And this is choose from library your personal library. This library right and this option is for the variation. Okay. Like these are basically presets like balloon world, stop motion, archive world, thin ner or cardboard and the paper. Okay. So this is the resolution. Okay. 480p this is the fastest in video generation.


08:32:23
Okay. 720p will take uh like 4x slower and 1080p 8x slower. I guess 1080p is only available in chat GD uh pro version. Got it? Okay. So we'll uh we are just you know doing I will I'm just uh showing you demo. So I will uh choose this fastest version only. Okay. So this is the time duration how long you want like 5 second 10 second 15 and 20 seconds is available in pro version okay of charge GPT and this is how much versions you want to take we will I will select only two okay because it will


08:33:01
again charge more credits to you okay and these credits are monthly basis I guess okay these credits are monthly basis okay see again rec blland loop to create content. This will take again more credits. Okay. See here, Chargity Plus up to 50 priority videos, 1,000 credits. Okay. Per month, I guess. Yeah. Per month. Up to 720p resolution and the five second duration. And Chad Pro up to 500 priority videos, 10,000 credits, unlimited relaxed videos, up to 1080p resolution, 20 second duration. Download


08:33:40
without watermark. Here you can download with watermark, I guess. I don't know. Yeah, we'll see uh about uh everything, you know. Okay, 30. But Chad GBD Pro is $200 per month. So, uh yeah, it's expensive, right? So, yes, let's uh do something creative. So, okay, I will write here. Okay, polar be enjoying on the Sahara desert. Okay. S does it. Yeah. Okay. You can do story board as well or you can create directly videos. Okay. So, let me show you the storyboard first. Yeah. So, frame by frame you can


08:34:28
give you know different uh what to say prompt. Okay. Here you can give different prompt. Okay. Polar beer with family. Okay. playing with sand like this. Okay. And later on it will create a whole the video. Okay. Third you can describe again you can add image like this. Okay. This is the story created by the chair GPT. Okay. Let's create. Okay. Add it to the queue. Okay. It's very fast actually. Almost done. Yeah. See with family you can see playing with the sand. Okay. So these are the two


08:35:25
variation. Okay. You can choose either this or either that one or either that one. Okay. I'm feeling this muches. Yeah. So here you can again edit your story record you can trim or extend this video in a new storyboard. Okay. So basically record uh features allow you to creators to you know pinpoint and isolate the most impactful frame in a video extending uh them in either direction to build out of like complete scene. Okay. If we'll choose recut. Okay, this thing fine. Then remix. What remix do is like


08:36:08
the remix features allows user to reimagine existing videos by alterating their components without losing you know that essence of the original originality you can say. Okay. You want to you know add or remove certain things. Okay. What if I want to remove you know that this polar bear or like this? Okay. Or you can say we can you know change colors or we can some tweak visual elements and this blend. So this blend feature allows you to combine with different video. If I want to upload some videos, it will


08:36:48
blend both the video. This video particular with that video which I will upload. Okay. Right. And the last loop you know by the name L features you know uh feature make it easy to create seamless repetition of the video. Okay. Uh this will like this one option is ideal for background visuals, music videos like this. Okay. So this is how you can uh create video in 2 minutes I can say just by giving prompt. Okay. This one is favorite. You can save it for the favorite and this this you can sharing options are there. Copy link or


08:37:28
this unpublish and you can download. See I told you without watermark is available in only pro version. So I have this with watermark you can download. See downloading video in just a click or you can download as a GFI as well right and uh add to a folder. Okay fine. This is the notification activity. Right. So let's create one. Okay. Monkey with family driving car on this space. Yeah. So okay I will choose this time 16 by9 like it takes more credit of mine. It's okay. Yeah. Add it to the


08:38:16
queue. If you'll go to favorites, it will come this one because I chose it. Okay. And if you will ask how this Sora is working. So it's like text to image generative AI model such as like DAL three stable diffusion and mid channel. So Sora is a diffusion models that means that it starts with each frame of the video consisting of the static noise. See? Oh, it's cartoonish. But yeah, see if you want Lamborghini, you can add that I want Lamborghini or Tesla, whatever. So, this is how you can


08:38:52
generate videos with Sora, you know, in a quick in quick 2 minutes. Hello everyone, I am M and welcome to today's video where we will be talking about LLM benchmarks tools used to test and measure how well large language models like GPT and Google Gemini performs. If you have ever wondered how AI models are evaluated, this video will explain it in simple terms. LLM benchmarks are used to check how good these models are at tasks like coding, answering questions, and translating languages or summarizing


08:39:24
text. These tests use sample data and a specific measurement to see how well the model perform. For example, the model might be tested with a few example like few short learning or none at all like zero short learning to see how it handles new task. So now the question arises why are these benchmarks important? They help developers understand where a model is strong and where it needs improvement. They also make it easier to compare different models helping people choose the best one for their needs. However, LLM


08:39:53
benchmarks do have some limits. They don't always predict how well a model will work in real world situation and sometimes model can overfit meaning they perform well on test data but struggle in practical use. We will also cover how LLM leaderboards rank different model based on their benchmark scores, giving us a clear picture of which models are performing the best. So stay tuned as we dive into how LLM benchmarks work and why they are so important for advancing AI. So without any further ado, let's


08:40:21
get started. So what are LLM benchmarks? LLM benchmarks are standardized tools used to evaluate the performance of LA language models. They provide a structured way to test LLMs on a specific task or question using sample data and predefined metrics to measure their capabilities. These benchmark assess various skills such as coding, common sense reasoning and NLP tasks like machine translation, question answering and text summarization. The importance of LLM benchmark lies in their role in advancing model


08:40:48
development. They track the progress of an LLM offering quantitative insights into where the model performs well and where improvement is needed. This feedback is crucial for guiding the fine-tuning process, allowing researchers and developers to enhance model performance. Additionally, benchmarks offers an objective comparison between different LLMs, helping developers and organization choose the best model for their needs. So, how LLM benchmarks work? LLM benchmark follow a clear and systematic


08:41:17
process. They present a task for LLM to complete, evaluate its performance using a specific metrics and assign a score based on how well the model performs. So here is a breakdown of how this process work. The first one is setup. Llm benchmark come with pre-prepared sample data including coding challenges, long documents, math problems and real world conversation. The task is spend various areas like common sense reasoning, problem solving, question answering, some regeneration and translation all


08:41:48
present to the model at the start of testing. The second step is testing. The model is tested on one of the three ways few short. The LLM is provided with a few example before being prompted to complete a task demonstrating its ability to learn from limited data. The second one is zero short. The model is asked to perform a task without any prior examples. Testing its ability to understand new concept and adapt to unfamiliar scenarios. The third one is fine-tune. The model is trained on a data set similar to the one used in the


08:42:17
benchmark aiming to enhance its performance on the specific task involved. The third step is the scoring. So after completing the task, the benchmark compares the model's output with the expected answer and generates a score typically ranging from zero to 100 reflecting how accurately the LLM performed. So now let's moving forwards. Let's see key metrics for benchmarking LLMs. So LLM's benchmark uses various metrics to assess performance of large language model. So here are some


08:42:44
commonly used matrices. The first one is accuracy of precision. Measure the percentage of correct prediction made by the model. The second one is recall also known as sensitivity. Measure the number of true positive reflecting the current prediction made by the model. The third one is F1 score. Combines both accuracy and recall into a single metric weighing them equally to address any false positive or negatives. F1 score ranging from 0 to one where one indicates perfect precision and recall. The fourth


08:43:12
one is exact match tracks the percentage of predictions that exactly match the correct answer which is especially used for the task like translation and question answering. The fifth one is perplexity goes. Here it will tell you how well a model predicts the next word or token. A lower perplexity score indicates better task comprehension by the model. The sixth one is blue. Bilingual evaluation under study is used for evaluating machine translation by comparing engrams sequence of adjacent text element between the model's output


08:43:44
and the human produced translation. So these quantitative metrics are often combined for more through evaluation. So in addition human evaluation introduces qualitatively factors like coherence, relevance and semantic meaning provide a nuance assessment. However, human evaluation can be time consuming and subjective making a balance between quantitative and qualitative measures important for comprehensive evaluation. So now let's moving forward see some limitation of LLM benchmarking. While


08:44:11
LLM benchmarking are valuable for assessing model performance, they have several limitation that prevents them from the fully predicting real world effectiveness. So here are some few. The first one is bounded scoring. Once a model achieves the highest possible scores on the benchmark, the benchmark loses its utility and must be updated with more challenging task to remain a meaningful assessment tool. The second one is broad data set. LLM benchmark often rely on sample data from diverse subject and task. So this wide scope may


08:44:40
not effectively evaluate a model performance in edge cases, specialized fields or specific use cases where more tailored data would be needed. The third one is finite assessment. Benchmark only tests a model current skills and as LLMs evolve and a new capabilities emerge new benchmarks must be created to measure these advancement. The fourth one is overfitting. So if an LLM is trained on the same data used for benchmarking it can be lead to overfitting where the model performs well on the test data but


08:45:09
struggles with the real task. So this result is scores that don't truly represent the model's broader capabilities. So now what are LLM leaderboards? So LLM leaderboards publish a ranking of LLMs based on the variety of benchmarks. Leaderboard provide a way to keep track to the Marriott LLMs and compare their performance. LLM leaderboards are especially beneficial in making decision on which models to use. So here are some. So in this you can see here OpenAI is leading and GPD 40 second and the


08:45:39
llama third with 405 parameter B and 3.5 is there. So this is best in multitasking. What about the best in coding? So here OpenAI 01 is leading. I guess this is the Orian one and the second one is 3.5 Sonet and after that in the third position there is GPT4. So this is in best encoding. So next comes fastest and most affordable models. So fastest models are LMA 8B parameter 8B parameter and the second one is LLA MA Lama 70B and the third one is 1.5 flash. This is Gemini one and lowest latency and here it is leading llama again in


08:46:22
cheapest models again Llama 8b is leading and in the second number we have Gemini flash 1.5 and in third we have GPT4 mini moving forward let's see standard benchmarks between claw 3 opus and GPT4 so in general they are equal in reasoning claw 3 opus is leading and in coding GPT4 4 is leading in math. Again, GPT4 is leading in tool use. Cloud3 opus is leading and in multilingual. Cloud3 opus is leading. Today, we are exploring hugging face, an amazing tool that makes working with language and AI super easy.


08:47:03
If you're curious about using advanced technology to understand or create text, this video is perfect for you. Hugging phase is a company that helps people use AI models for language tasks like translation, text analysis or even generating new text. They have created a library called transformer which comes with pre-trained models. So you don't have to build everything from scratch. It's simple, powerful, and perfect for developers, researchers, or even beginners. So in this video, I will show


08:47:32
you three cool things you can do with hugging face speech to text. Turn spoken words into written text easily. Great for captions or voice apps. Second one is sentiment analysis. Find out if a text is positive, negative or neutral. Helpful for understanding reviews or comments. The third one is text generation. Create humanlike text that sounds natural. Perfect for chatbots or creative writing. I will also explain some important basics like how pipelines make using hugging face model super easy


08:48:03
and how tokenization helps AI understand text. By end of this video, you will know how to use hugging phase to start building your own projects. It's simple, fun, and really powerful. So without any further ado, let's get started. So welcome to this demo part of this video. So here, as you know, we will be doing three things. First thing speech to text recognition. The second thing text generation from a particular sentence or a word. And the third thing we will do sentiment analysis. Okay. So we will


08:48:36
perform this one by one. So let's start with something called speech to text recognition. Okay. Using hugging face. So first I will write rename this. I will write here hugging face speech to text. Okay. Yeah. So first I will install transformers library. It is already installed on my system but again just for you guys I'm doing this okay pip install transformer. So now let's see what is transformers. So transformers is a powerful python library created by hugging face. So that allows you to download manipulate and


08:49:24
run thousands of pre-trained open-source AI model. Fine. So as you can see requirement already satisfied. it means like already installed. So these transformer models cover multiple tasks across uh you know modities like NLP, natural language processing, computer vision, audio and multimodel learning like many things. Fine. So now I will do from transformers import pipeline. Okay. So now let's run it. So now what is pipeline? Okay, so a transformer pipeline describes the flow of data from origin system to destination system.


08:50:12
Fine because you already know transformer means you can run or manipulate thousands of pre-trained opensource AI model. So pipeline describes the flow of data from origin system to destination system and defines how to transform you the data along the way. Okay. So let's check the versions transform version. Okay that long tab. Okay. Why it is coming? import transformers. Yeah. So we have currently 4.42.4 version of the transformer. Okay. So I'm using Google Collab. Okay. So you can use either Jupyter notebook, Visual


08:51:16
Code Studio or Collab. Okay. So now let's import libraries. Import Librosa. Okay. So what is librosa? So librosa is a python package for audio and music analysis right because we are doing speech to text. So we need this. So it provides various functions to quickly extract key audio features and metrics from the audio files. Okay. And this librosa can also be used to analyze and manipulate audio files in a variety of formats such as wav, mp3, m4a and like that. Okay. So next we will import torch. I hope everyone know


08:52:08
torch. So torch is nothing pytorch. It is a machine learning library based on the torch library used for applications such as computer vision and NLP and originally it was developed by the meta and now the part of Linux foundation you know umbrella let's import one more thing import so now let's import IPython Okay, it should be capital ipython dot display as display. So now you will be wondering what is this ipython display. Okay. So it is an interactive command line uh for the terminal for Python and it uh you


08:53:06
know it can provide a IPython terminal and the web-based notebook platform for Python computing and this uh ipython have you know more advanced features than the Python standard interpreter and we quickly execute a single line of Python code nothing okay for that I'm using this and the Next is from transformers import wav to vectorzer for CTC and WAV to vectorizer to tokenizer. Okay. So now what is this? So WA2 vectorizer for CTC is supported by the notebook on how to fine-tune a speech recognition


08:54:11
model. Okay. And this tokenizer. So tokenizer is nothing tokenization is a conversion of a text into meaningful lexical tokens belonging to categories defined lexa program. in case of NLP. So those categories include nouns, verbs, adjective, punctuation, ATC. Okay. Now, and the last library, let's import numpy, s np. I hope everyone know numpy. What is numpy? Again, numpy is a python library used for working with arrays. It al it also has a function you know for working in domain like linear algebra fa


08:54:52
transform and matrices and many others fine I hope everyone know about these library which we have imported okay so let's move forward by tokenizer 22 [Music] 22 to tokenizer dot from pre-trained. This is why I love this Google collab. You'll decide just some words and it will show the suggestions you know Facebook. So this is again one pre uh trained model. Okay. So we are just importing it base 960th. This is nothing the name. Okay. Then model equals to [Music] WAV2 for CTC dot from pre-trained then


08:56:16
Facebook WV2 vectorizer 2 then base 96 fine now let's run it so now you can see so we are loading this okay they they're downloading okay so you can ignore this warning so now let's load the audio file so here I will write audio sampling rate equals to librosa dot load Okay. So you know about libroser right? Then I have v dot m4 a. So we I have already one speech or one audio you can say. So I will play it. Don't worry. Before the final output I will you know show you. 16,000. Okay. So now I will write


08:57:43
Okay. Okay. I've already loaded I guess. Okay. Okay. No issues. I will load again. Okay. The file is loaded. I will rename it to V. Yeah, fine. So now it won't there won't come again. We have V.M.mpp4 in. Okay. So now what we have to do? Okay. Let me try it. We have V.M. MP4A. But I don't know why it is not coming. Okay, let me copy the path. And now it will run. I guess it's running. Yeah. So now I will write here audio comma sampling rate. Okay. Now listen carefully then I


08:59:09
will write display dot audio then path comma comma auto play equals to true. Okay. Hello and welcome. This is an AI voice message. I guess you heard but again let me play play it again. Hello and welcome. This is an AI voice message. Okay. So it is saying hello and welcome. This is an AI voice message. Fine. So now what I will do? I will input some values. Input values equals tokenizer then audio comma return tensors equals to pt findine dot input values. Okay. Then here input values. Okay. Input


09:00:42
values. Okay. Yeah. So now logits will come here. So we have to now store the logits which means non-normalized predictions. Okay. So logits equals to model input values dot logitics. Okay. Then here I'll write logits. Okay. It's running. Done. So now what we will do? We will store predicted ids. Then we will pass the logits to values to softmax to get the predictive value. Okay. So here I will write predicted ids equals torch dot arg max. Okay. Then logit dimension equals to minus one.


09:01:57
Okay. Then I will pass the you know prediction uh to the tokenizer decode to the transcription. Okay. So here I will write transcriptions equals to tokenizer tokenizer dot decode predicted ids zero running fine. So now let's see our output transcriptions. So now you can see hello and welcome. This is an AI voice message. So now let's play it again. Hello and welcome. This is an AI voice message. Amazing, right? So this is how you can use hugging face for this piece to text domation. It's a very small code, line


09:02:58
of code as you can see. Okay. So now let's move forward and do the sentiment analysis. Okay. Or text generation. So let me open new drive. So yeah. So now I will write here. I will write sentiment slash text [Music] generation hugging face. Okay. So first of all let's import the files. So first I'll write import warnings then warnings dot filter warnings okay I will write here ignore then we'll import numpy you already know what is numpy and import panda so these are some basic basic uh Python library and I hope


09:04:11
everyone knows and import mattplot li for the plots mattplot lib dot piplot as plt then I will import cbond for the graphs the statics graph okay SNS then SNS dot set. So now we'll import skarn model train split. So skarn is nothing scikitlearn is a probably the most useful library for machine learning python. So, so this uh library contains a lot of efficient tools for machine learning and you know statical modeling including classification you can do regression you can do clustering you can do from this


09:05:04
escal okay from skarn dot model [Music] selection Import import train test split from SQL dot matrix import F1 underscore comma confusion matrix comma ro a core. So again we'll import dims from transformers import pipeline. Everyone know what is this? Then import torch. Okay. Now let's run it. You have too many active session. Terminate existing to continue. If you're interesting using Okay, wait. Yeah. Fine. Now, which one is open, bro? Terminate this. Terminate this. Terminate this.


09:06:45
No. Okay. Fine. So now let's do sentiment analysis. So uh we will explore sentiment analysis using a pre-trained transform model. Okay. The hugging face library provides you know convenient pipeline as you already know function that allows us to easily perform sentiment on the text. So now let's first import the necessary dependencies and create a sentiments pipeline using this line. So I will write here classifier equals to pipeline sentiment analysis then type classifier. Okay. So it will download the


09:07:44
pre-trained sentiment analysis pipeline. Pipeline is not defined. How? Okay. Okay. Error is there. I hope it will run now. Yeah. So now we can uh pass a single sentence or a list of sentence to the classifier and now and get the predictive sentiment labels and associate confidence score. Fine. So now just for the testing classifier. So, let's write this is a great movie. Okay, now let me run it. So now you can see here label positive. So label positive typically refers to the outcome or a class of interest that the


09:08:46
model is designed to predict. So here we are just checking the sentiment analysis model. Okay. So this is why I wrote this. Okay. So let's check one more. This was a great course. Then I did not understood any of it. Now let's check this. Yeah, perfect. And the score you can see the accuracy score 99%. 99% which is almost close to 100%. And which is amazing. So now you have access to a GPU. So you can also utilize this for the faster processing by specifying the device using the parameter. Okay. So


09:09:55
now what I will do first I will okay wait. Yeah fine. So now I will import data set a line tweets equals to pd dot read pd means pandas library here we are using okay now first import I will import here itself so I have twitter dot tweets dot csv Okay. Tweets dot CSV. Let it upload. Yeah. Done. So now let me airline tweets do. Y dot head means you it will show me the top five rows. Okay. 0 1 2 3 4 5. Okay. You can see twine sentiment neutral positive neutral negative. Then this is this. Okay. So


09:11:15
now let's do something. Okay. So what I will do df equal df means data frame a [Music] line 28 then I will write a line sentiment we have a line sentiment this column okay text I need these two columns again df dot head five. So yeah, air a cime neutral and this is what this text is all about. Okay, because these two are the main things text and the sentiment. So now let's make plot count plot then I will write df comma x = to a line sentiment then pallet equals to Then here I will write plt dotx


09:12:42
label a line sentiment. Okay. Then plt dot label count plt do.ow show. So it will draw one graph. Okay. Okay. Spelling is wrong. Yeah. So it will show neutral positive sentiment and the negative. So as you can see the negative sentences or the sentiments are the more. Okay. So now so we have now three classes which do not match the two class available in the hugging face pipeline. So therefore we will filter out all the rows which have been labeled as neutral. Okay. So, DF equals to DF and again


09:13:49
DF airline sentiment. Fine was not equals to neutral. Okay. Then DF target equals to DF airline sentiment dot uh map then I will write here positive positive 1 and the negative 0 fine negative will be zero. Okay. Then print number of rows, df dot shape. Okay. So now you can see number of rows are 11,541. Okay. So now I will write predictions five it will okay predictions is not defined. Okay. So now what I will do? So here I will write text= to TF text text and to list dot to list then here I will write


09:15:48
predictions equals to classifier text. Okay. So here write probabilities equals to predictions then score. If predictions label dot starts with starts with P means positive. Okay. else 1 minus prediction score prediction score okay then I will write for prediction and prediction why it is not running this is taking time too much time I don't know why So as you can see finally we have the output predictions values. So this depends on you know system. So system my system took almost 17 minutes 47 seconds


09:17:13
to complete. Okay. So now let's run it. Okay. Predictions is now defined. Now predictctions. Yeah. So now I will write here predictions equals to np dot array. np means numpy numpy dot array then I will write one if prediction label is positive dot starts with p. Okay. else zero for prediction and prediction values. Yep. Okay. Fine. So now let's check the accuracy of our model. Branch accuracy. Okay. Then I will do round up the values. So here I will write np dot mean then df then the target


09:18:46
right. Yeah. Then I will write equals equals to predictions. Then I will write into 100 comma two. Then I will write percentage. Okay. Fine. Looks good. Yes. So as you can see our accuracy is 88.99%. Which is you know very good again. So now let's do some confusion matrix confusion matrix then TF target predictions common normalize equals to true. Okay. Then I will plot confusion matrix. Okay. DF then target, predictions, comma, normalize equals to true. Okay. Okay. My bad. My bad. My bad. Let


09:20:30
confusion [Music] matrix comma labels. Okay. So here uh I will you know plot a confusion matrix using cbond. Okay. So here I will use args which is confusion matrix np do. array. So which is you know labels list. So I will write plt dot figure then figure size equals to 8 comma 6 that's sn dot set font scale equals to 1.4. Okay. Then let's create the heat map diffusion matrix comma and not will be true. Okay. Then I will write fmp equals to g then confusion map equals to [Music] blues. Then X


09:21:58
labels equals to labels. Then Y labels equals to labels. Okay. Yes. So now I will write plt dot title will be confusion matrix then plt do.x X label will be predicted values then plt dot y label will be actual values then plt dot show okay why the chart is not coming okay so I have to write plot confusion matrix then write cm then negative comma positive. Okay. Now the chart will come. Okay. Yeah. So now you can see here this is you know actual and this is a confusion matrix and the negative and the positive


09:23:43
ratio is there. Okay. So now let's print the let's check the ROC score. So I will write here print ro score then here I will write ro A score then TF target props. Okay. So 94. Okay. So first let me tell you what is this ROC AC score. So this is the area under the ROC curve. So what it does it sum up how well a model can produce relative scores to discriminate between positive and the negative instance across all the classification threshold. So with this ROC score of 94 0.94 which is 94% we can conclude that


09:24:50
the that a pre-trained sentiment analysis model has achieved the high level of accuracy and effectiveness. So in predicting the sentiment labels so this indicates that the model is capable of accurately classifying text into positive or the negative sentiment categories. Okay. So now we'll do text generation. Okay. Okay. So text generation involves generating creative and coherent text based thing. So I will write here text. Okay. So text generation involves generating creative and coherent text


09:25:26
based on a given prompt or starting point. So what we will do first? We will import the necessary dependencies and load the data data set of the poem. Okay. So here I'm write poems. Let me write poems equals to PD dot read dot CSV. Don't worry, I will give you these files. Okay, description box below I will add. So here I will write robot frost with dot CSC. Fine. And then I will write poems dot head five or head just have to write head module pandas has okay not do sun csv yeah okay it will show the top five


09:26:28
stopping by woods on a snowy evening fire and ice the aim was song collection content, year of publish. Okay. So now what we will do? We will write content equals to poems content dot drop na to to list. Okay. So to generate text we extract individual lines from the poems and use the pipeline text generation function to create a text generation pipeline. Fine. So here I will write lines equals to then I will write for poem and content. Okay. For line in poem dot split into the next line. Okay. Then lines dot


09:27:52
append. Then I will add line dot write strip to the right. It will add. Okay. Fine. So now lines equals to line for line in lines. If length of [Music] line zero then show me the lines five. Okay. So now you have seen here whose words these are I think I know. Then the next line his house is in the village though. Then the next line he will not sing. Okay. Like this. So now let's uh you know import that pipeline text generation module. Okay genu [Music] pipeline. So these are the some pre-trained model you already


09:29:07
know generation. Okay. So now I will write here lines. Let's run it. Yeah. Done. So in line zero you have we have whose words these are. I think I know. Okay. So now we can now generate text by providing a prompt and specifying the parameters such as max length and num return sequence. Okay. Why? Because we have imported this text generation module. Okay. For example, see [Music] gen. Okay, sorry lines zero dot max length max length equals to 20. So now it will generate the this to the maximum


09:30:09
20. Okay. Till 20 word maximum length. Okay. Check. Okay. Uh okay. Ch expression cannot contain perhaps double equals to where? Okay. Wait. Now it will run. So see the line was this much only. Whose words these are? I think I know. Whose words these are? I think I know but here the generated text I wish to go to church because I feel like okay this is how you can do you know text generation now let's check for the more gen lines one okay then max length equals to 30 comma [Music] num return sequence


09:31:12
says sequences equals to 2. Okay. So our first line was this. His house is in the village though. Okay. 0 1 2 3 4 5. Okay. So here you can see see his house in the village though. However you might say that the place was the same with the place this this this. So these are the generated text. Okay. And this is another you know return sequence second. So there are two one and the second. Okay. So now let me you know import text app. Okay. Then creating function wrap x. I don't need this. Then


09:32:14
return text wrap dot fill x comma replace white space wide space equals to false false comm sentence endings equals to true. Okay. So now out equals to generated lines zero to maximum length equals to 30. Then print zap out to zero. [Music] Then generated text. Okay. So now we are setting uh the pad tokens to eos tokens. Okay. So whose whose these are I think I know. And this is the maximum 30 till 30. Okay. So now I will write here okay preview equals to okay. So now what you can do you can you know generate a prompt to


09:33:57
generate text on a specific topic like this prompt equals to transformers have a wide variety of applications application in NLB P okay so this is my prompt okay this I'm not importing from the data set okay so I will write out equals to gen prompt comma max length equals to 100 print wrap out. Okay, here I will write anyways here I will write generated text prompt. Okay, so it is running. Let's wait. Yeah. So, okay. Somewhere is there. Okay. So, here the issue is with 100 words I go with. We'll go till


09:35:54
50. Yeah. So let me do it again. 100. Yeah. So now you can see we have we can generate text using by giving a prompt. So we have covered you know three topics in this. First thing is piece to text using hugging face. Second thing is sentiment analysis using hugging face. And the third is text generation using hugging face. Okay. And in this text generation we have we did with two methods. One with data set and second thing with the you know by giving prompt like in chat GPT you can say. Open AI is one of the main leaders in


09:36:37
the field of generative AI with its chat GPT being one of the most popular and widely used examples. Chat GPT is powered by OpenAI's GPT family of large language models LLM. In August and September 2024, there were rumors about a new model from OpenAI, code name Strawberry. At first, it was unclear if it was the next version of GPT4 or something different. On September 12, OpenAI officially introduced the O1 model. Hi, I am M. In this video, we will discuss about OpenAI model 01 and its types. After this, we will perform


09:37:10
some basic prompts using OpenAI preview and OpenAI mini. and at the end we will see comparison between the open AI1 models and GPT4. So without any further ado let's get started. What is open AI1? The open family is a group of LLMs that have been improved to handle more complex reasoning. These models are designed to offer a different experience from GPT40 focusing on thinking through problems more thoroughly before responding. Unlike older models is built to solve challenging problems that


09:37:40
require multiple steps and deep reasoning. OpenAI O1 models also use a technique called chain of thought prompting which allows the model to think through problem step by step. OpenAI O1 consists of two models preview and O1 mini. The O1 preview model is meant for more complex task while the O1 mini is a smaller more affordable version. So what can OpenAI do? Open a1 can handle many tasks just like other GP models from open AI such as answering questions, summarizing content and creating new material. However, O1 is


09:38:14
especially good at more complex task including the first one is enhancing. The O1 models are designed for advanced problem solving particularly in subjects like science, technology, engineering and math. The second one is brainstorming and ideation with its improved reasoning. O1 is great at coming up with creative ideas and solution in various field. The number third is scientific research. O1 is perfect for task like anoting cell sequencing data or solving complex math needed in areas like quantum optics. The


09:38:44
number fourth is coding. The oven models can write and fix code performing well on coding tests like human event and code forces and helping developers build multi-step workflows. The fifth one mathematics. 01 is much better at math than previous model scoring 83% in the international mathematics olympia test compared to GPT4's 13%. It also did well in other math competition like a IME making it useful for generating complex formulas for physics and the last one is self fact checking can check the


09:39:16
accuracy of its own responses helping to improve the reliability of its answer. You can use open models in several ways. Chat GBD Plus and team users have access to O1 preview and O1 mini models and can manually choose them in the model picker. Although free users don't have access to the O1 models yet, OpenAI planning to offer O1 mini to them in the future. Developers can also use these models open API and they are available on third party platform like Microsoft is your AI studio and GitHub models. So


09:39:48
yes guys, I have opened this uh Chadgity 40 model here and Chad GPD1 preview as you can see. So I have this plus model. Okay, the paid version of Chad GPD. So I can access this 01 preview and 01 mini model. Okay, we will go with 01 preview model and we will put same prompts in both the model of the chat 40 and the 01 preview and see what are the differences are coming. Okay. So we will do some math questions and we will do some coding. We will do some advanced reasoning and quantum physics as well.


09:40:30
Okay. So let's start with so I have some prompt already written with me. So first one is number theory. Okay. So what I will do I will copy it from here and paste it in this and both. Okay. So let me run in for and preview. So here you can see it's thinking. Okay. So this is what I was saying chain of thoughts. Okay. So these are the chain of thoughts. First is breaking down the primes. This is and then is identifying the GCD. And now see the difference between the output. See output is 561 is not a prime


09:41:18
number and the GCD greatest common DC or 48 and 180 is 12. Okay. Here see 01 preview is giving the output in step by step. Firstly determine if 561 is a prime number or not. The number 56 is not a prime number. Is composite number because it has this this this. Okay. Then second step then the greatest common divisor. Then they found 12. And answer is no. 561 is not composite number because of this. And the greatest common divisor of 48 and 18 is 12. See just see the difference between the two models.


09:41:59
This is why chipity 01 models are crazy for math coding and advanced reasoning quantum physics for these things. Okay. So let's go with our second step. So here if you will see you can see the attach file option in charge 40. Okay. You can come upload from your computer. But here you we will see in 01 there is no attach file option. This is one drawback. Okay. So here upload from computer. So this is one small. Okay. And let me open this and this is the question I have. Okay. Yeah. So I will copy this. I will run this and


09:42:51
this. Okay, see it's start giving the answer and O1 is still thinking solving the equation then solving analyzing the relationship. Okay, so charge 01 will take time but it will give you more accurate more step by step as you want. Okay. So here you can see solve for X's question. This is this and here the steps you can see. Okay. This is more structured way you can see in a good structure way. Okay. Chity 01 preview give you in good structure way as 01 mini as well. Okay. So yeah. So here


09:43:34
they wrote just one and two this this and here if you'll see question one solve for x in this and step one is this step two is this and step three is this then the answer of x equals to three but here simply the wrote we know this is this this and x= to three for the second question see expanding the left hand side this this is but here step one square both sides of the given equation Start by squaring both side. Okay. It's written but not in good way. Okay. So this is why is better for math. Okay. So


09:44:12
now let's check it for the coding part. Okay. So I have one question. Okay. Let me see what output it will give to first I will write I need. Okay. Leave it. I will copy it and I will copy it as well here. Run it and run it. See it start giving answer. Okay. And still this will adjust the parameters ensuring the code generation because JGPT1 will think first then it will analyze then after that it will give you answers. Okay. Here the code is done. See here the code is done and it's still


09:45:01
thinking. Step one and first here you can't see anything. See step setup development environment pip install numpy m plot li then this then this and here nothing and but I will ask it okay give me code in one tab okay here also give me code in in single step. Okay. So I can just copy and paste. So what I will do I will open one online compiler and I will directly copy it and paste. Okay. So let's finish this. I hope it will work. So let me open W3 schools compiler. Okay. Yeah. Same I will open


09:46:15
for this W3 school. Okay. So let me copy the code and my bad. and paste it here. Same for works for this. Okay. Okay. I will copy the code and I will paste it here. Okay. I hope okay it gives something. Yeah. Cool. So yes, now you can see the difference between the output. So this is the output of 40 and this is the output of one preview. See 01 preview output is this and this is the output out output of four. So this is the difference. This is why 01 takes time but it will give you more accurate


09:47:17
result in a good way. Okay. So now let's check something else. So moving on, let's see some advanced reasoning question. Okay. So this is the logical puzzle one, the first one. Okay. So I will copy it and I will paste it here. Okay. This is for O1. This is for preview because why I'm not comparing 01 with mini because there both are same but slightly difference is there. Okay. So here we can see more difference between for old model versus new model you can say. Okay. So now see the answer


09:48:03
is end in this much only but it will explain you in a better way. See thoughts for 7 seconds explanation that case one then case two. Okay, with conclusion in both scenarios summary and this here this one small explanation and that's it right. So they created one preview for more you know it will describe you more in a better way right now let's see some scientific reasoning as well okay so let me copy it here this is still thinking but it start giving answer says thought for 16 seconds. So again I will say that you


09:49:05
know chity 01 is much better than chutity photo. Chatgity photo is great for you know content writing and all but chity 01 preview and mini are very good for reasoning math coding or quantum physics these type of things. Okay, advanced reasoning. Okay, charge 40 is great for you know generative text. Okay, like for marketing, writing copies, emails and all of those. So now let's see some comparison between open models and GPT4 model. When new models are released, their capabilities are revealed through benchmark data in the


09:49:41
technical reports. The new open AI model excel in complex reasoning task. It surpasses human PhD level accuracy in physics, chemistry, biology on the GPQA benchmark. Coding becomes easier with O1 as it ranks in the 89th percentile of the competitive programming questions code force. The model is also outstanding in math on a qualifying exam for international mathematics IMO. GPT4 solved only 13% of problems while O1 achieved 83%. This is truly next level. on the standard ML benchmarks. It has huge improvements across the board. MMLU


09:50:17
means multitask accuracy and GPQA is reasoning capabilities. Human evolution. OpenAI asked people to compare O1 mini Winnie with GPT4 on difficult open-ended task across different topics using the same method as the preview versus GPT4 comparison like preview. Mini was preferred over GPT4 for task that requires strong reasoning skills but GPT4 was still favored for language based task. Model speed as a concrete example we compared responses from GPT4 O1 mini and preview on the word jing question while GPT4 did not answer


09:50:53
correctly. Both oven mini and oven preview did and oven mini reach the answer around 3 to 5x faster limitation and what's next due to its specialization on stem science technology engineering and math reasoning capabilities open mini is factual knowledge on non- stem topics such as dates biographics and trivia is comparable to small LMS such as GP4 mini open AI will improve these limitation in future version as well as experiment the extending the model to other modalities and facilities outside of the STEM. So


09:51:27
let's talk about the very first top AI tool which is Kora.ai. This AI tool aims to provide the best AIdriven mock interview experience. You can take mock interviews for any job roles and subject receiving instant constructive feedback. Skilora's AI interviewer listens to your responses and ask dynamic flow of questions to test your depth of knowledge. With the essential plan, you gain access to six AI mock interviews per month. You'll receive instant feedback, ideal answers, and performance


09:51:58
reports, helping you upgrade your skills for just $8 per month. The premium plan allows you to enjoy 15 AI mock interviews per month. You'll get the same instant feedback, ideal answers, and performance reports along with priority support, maximizing your learning for only $15 per month. Next up, we have Visos. Visco's advanced AI coach is designed to elevate your job interview skills, harnessing insights from over 10,000 companies. It offers tailored sessions that simulate real interviews, providing personalized


09:52:29
feedback and performance analytics. It transcribes your performance and delivers a conversational experience akin to a actual interview. It also offers customized feedback and performance analytics helping you refine both your answers and communication skill. During practice session, you can craft questions specific to your desired role and industry, ensuring relevance and depth in your preparation, supporting various interview styles. The price for EVA services are available for free. Next up, we have


09:53:01
interviewing.io. Interviewing.io is an AI powered platform that provides candidates with simulated technical interviews. The platform uses machine learning algorithms to access your coding skills, problem solving abilities, and communication style. You can get four simulated technical interviews for $225. Next up, we have Mockmate. Mockmate is an AI powered tool that allows you to practice answering common interview questions. This tool provides feedback on your answers and gives tips on how to improve your performance. The


09:53:32
service is available for $29 per month. Moving on to Chad GPT. Chad GPT is an AI chatboard that can be used to practice answering interview questions in a conversational setting. The chatbot can be customized to simulate the style of an actual interview and it provides feedback on your answers. The free plan allows you to get started with writing assistant, problem solving and many more. You will have limited access to chat GPT 40 mini including data analysis, file uploads, vision, web browsing and custom GPTs. All of this is


09:54:04
available at no cost. Then we have the plus plan which is for $20 per month. The plus plan amplifies your productivity by offering early access to new features. Access to chat GPT4 for GPT4s mini up to 5x more messages for chart GPT 40. You can access to data analysis, upload files, vision web browsing and deli image generation. You can also create and use custom GPTs. Then we have my interview practice. My interview practice is an AI tool that allows you to practice interviews in a simulated environment. The platform uses


09:54:40
AI to generate interview question based on your chosen field and provides instant feedback on your responses. This tool is particularly useful for those who want to practice their interview skills in low pressure environment. The plus plan is for $49 per month. You get one month of access. And the premium plan is for $57 per month and you will get access to additional features and many more benefits. Next up, we have Udei. Udi offers real-time interview coaching using AI. It analyzes your speech and body language, providing


09:55:13
feedback on how to improve your performance. UDLI services are available for free. Now, you might be wondering how AI tools can help you ace your job interviews. AI tools can assist in several ways. At first we have research companies. AI tool provides information about company's website, news articles and social media helping you tailor your answers to the interview's question. Practice questions. AI tool help you practice answering common interview questions, boosting your confidence and


09:55:43
preparation. Feedback. AI tool offers feedback on your answers, highlighting areas for improvement and refining your responses. Personalized advice. AI tools provide tips and advice on improving your interview skills, giving you an edge to your next job interview. Choose the right AI tool and select an AI tool that fits your needs and preferences. Use the tool consistently starting a few weeks before your interview to maximize its benefits. After practicing with the AI tool, get feedback from others to


09:56:14
identify areas for further improvement. Dress professionally and be punctual for your interview to make a positive first impression. Show enthusiasm and confidence during the interview to leave a lasting impression on the interviewer. AI tools can significantly aid in interviewing preparation. By using them effectively, you can enhance your chances of success in your next job interview. Imagine you are managing a global supply chain company and where you have to handle orders, shipments and demand forecasting but unexpected issues


09:56:45
arises where certain shortages like transport delays and the changes in demand. So instead of relying on manual adjustments, what if an AI agent could handle everything automatically? This AI wouldn't just suggest actions, it would decide, execute and continuously improve its strategies. That's the power of agentic AI. With that said guys, I welcome you all on our today's tutorial on what is agentic AI. Now let us start with understanding first the first wave of artificial intelligence which was


09:57:15
predictive analytics or we could say data analytics and forecasting. what exactly happened? Uh like predictive AI focused more on analyzing the historical data, identifying the patterns and making forecast about the future events and these model do not generate any new content but instead it was predicting outcomes based on the statistical models and machine learning. Now technically how used to work. So basically what we had like we used to take uh suppose this is the ML model. Okay. So this is taking


09:57:46
a structured data which could be like suppose any past user activity or it could be a transaction record or any sensor reading for example you can consider say Netflix users watch history okay it could be any movie genre watch time and the user rating so now after this what we were basically doing is we were doing the feature engineering or pre-processing okay now in the feature uh engineering ing process. We were extracting key features like user watch time trends, preferred genres rates and watch


09:58:21
frequency and we could also apply scaling normalization and encoding techniques to basically make data more usable for the ML model. Then we were using the ML models. Suppose it could be a time series forecasting models like ARMA, LSTM and all those given algorithms which was basically predicting the future of movie preferences based on the historical data and in the output guys Netflix AI recommends new shows or movie based on the similar user patterns. So this is how exactly the Netflix model was


09:58:56
working incorporating the machine learning model. So this was exactly the first wave of AI. Now let us discuss about the second wave of AI. Now if I discuss about the second wave which was basically content creation and use of conversational AI. So you know LA models like chat GPD became very much popular during the second wave of artificial intelligence. So what exactly was happening like generative AI was taking input data and it was producing new content such as text, images, videos or even code and


09:59:31
these models learn from patterns in large data sets and it was generating humanlike outputs. Now let us bit understand how exactly this technology was working. So basically first there was a data input. Okay. So basically any prompt from the user. So suppose in the GPT okay so I'll just open GPT all over here and say we are uh suppose we are giving any new prompt say such as write a article on AI. Okay. So this was our given prompt and after this what exactly was happening was tokenization and


10:00:16
pre-processing. So the input text suppose which I have written all over here write a article on AI. So this text was basically split into smaller parts. For example like uh you could consider certain thing like this. So here you have write as one uh you know and as next and similarly you could carry on you know for the other words. Then what exactly used to happen that these words were you know uh converted into word embeddings means the numerical vectors representing words like in a higher dimensional space and then we used to


10:00:55
perform neural network processing. So here the LLM processes input such as attention mechanisms okay or you know using uh these models like GPT4 bird and llama and with the help of self attention layers they were understanding the context and they were predicting the next word. Okay. Now as a result you were getting output certain thing like this. So which was basically a generative AI phase. So this was guys our second evolution of AI. Now if I talk about our third wave, so it is basically agentic AI or


10:01:32
autonomous AI agent. Now what is this guys? So the agentic AI actually goes beyond text generation. So it integrates decision making, action execution and autonomous learning. These AI systems don't just respond to prompts but they also independently plan, execute and optimize the processes. So you could understand something like this. So so here the first uh step was the user input or receiving any goal. So user provides any highle instruction. For example, it could be like say optimize warehouse shipments for maximum


10:02:12
efficiency. It could something be like that. And unlike generative AI which would generate text, agentic AI executes real world actions. After this would suppose the prompt that we have given like optimize warehouse shipments for maximum efficiency. Then the next step would have been quering the databases. The AI would pull the real-time data from multiple sources. So it could be traditional database like SQL or NoSQL where we are fetching inventory levels or shipment history. Then it could be a


10:02:42
vector uh database from where it is receiving some unstructured data like past customer complaints and all those thing. Then with the help of external APIs it is connecting to like uh forecasting services or fuel price APIs or supplier ERP systems and these things are like present with this uh respect. Then uh the third step was the LLM uh decision making. Now after requiring the database the AI agent processes data through the LLM based reasoning engine. Example like decision rules applied like


10:03:14
suppose if inventory is low then it could automate supplier restocking orders like if shipment cost is increasing then it is rerouting shipments through cheaper vendors and suppose also if weather condition impacts the route then it is adjusting the delivery schedules. Now you could understand how agentic AI is behaving all over here in the decision-m process. Now next step would be action execution via APIs. So AI is executing task without human intervention. It is triggering an API call to reorder a


10:03:46
stock from a supplier or update the warehouse robot workflows to prioritize fastmoving products or even send emails and notifications to logistic partners and about the changes what is going to be happen and after this finally it is continuously learning which is a data flywheel all over here. Okay. The AI is monitoring the effectiveness of its action like uh it was restocking efficient or did routing shipments you know uh reduce the cost and all. So it is moni monitoring the effectiveness of the action it has taken and the data


10:04:18
flywheel is continuously improving the future decisions. So basically it is using reinforcement learning and fine-tuning to optimize its logic. Okay. Now let's have a just quick recap about the comparison of the all these three waves of AI. So basically predative AI's main focus was on forecasting the trends. Okay. While generative AIS was creating the content and agentic AI on the other hand which is at the uh final step right now is making decision and taking action. So you could see how the


10:04:50
evolution happened of AI in all these stages. And if you uh understand about the learning approach then predictive AI was basically analyzing the historical data while generative AI was learning from the patterns like using text image generation okay and but agentic AI is basically using the reinforcement learning or the self-arning to improve its learning approach now if we just look at the user involvement in predictive AI. So human is asking for the forecast and all here human is giving the prompts but in the agentic AI


10:05:22
the prompts or the intervention of human input has become very much minimal. If you could understand the technology like basically predictive AI was using machine learning time series analytics. So these kind of you know uh algorithms they were using generative AI was using transformers like GPT lama BERT and all those things. Now, agentic AI is doing what guys it is using LLM plus APIs plus autonomous execution. So, we have discussed how this workflow is you know in a short way how it is working and uh


10:05:54
moving ahead we are also going to discuss uh through an example how exactly all these steps like agent AI is working. So based on the example you could understand like uh predative AI you know Netflix recommendation model which they have on their system and uh similarly if you talk about generative AI then you could understand about chat GPT you know writing articles and all those things and agentic AI we could imagine like how AI if incorporated in supply chains how you know things are working out. So guys I hope so you would


10:06:26
have got a brief idea regarding the three waves of AI. Now let us move ahead and bit understand about what is the exact difference between generative AI and agentic AI. Now guys let us understand the difference between generative AI and agentic AI. So let us first you know deep dive into what exactly is a generative AI. Okay. So as you can see all over here that generative AI models generally are taking input query. Okay. And they are processing it using LLM or large language model. and basically returning


10:07:00
a static response without taking any further action. So in this case for example a chatbot like uh uh chat GPT you know it is taking the input from the user. So as I've shown you earlier that uh say suppose I've given an input like write a blog post on AI in healthcare. So when I have written this uh given uh you know user input or given the query. So when it goes to the large language model these model is actually you know tokenizing all these input query and it is retrieving the relevant knowledge


10:07:31
from its training data and it generate text based on the patterns. Now we give the prompt then LLM processes it okay and then we are getting the given output. So now this is basically how you know generative AI is working. So you could see all over here we have GPT model, we have Deli, we have codeex. So these are some of the you know amazing you know generative AI uh models. Okay. Now let us discuss a bit about Deli which is actually a you know realistic image generation you know gen AI. So uh


10:08:06
like deli is described as you know the realistic image generation model vab open AI and this actually is a part of you know generative AI category alongside with GPT which is basically for human like language creation purposes. This model was created and you could have also codeex for like uh it could be used for advanced code generation purposes. Now let us discuss bit about Dali. So, Dali is like a deep learning model basically which is designed to generate realistic images from the text prompt and it can create


10:08:37
highly detailed and creative visuals based on descriptions provided by the users. So, uh some of the aspects of Deli like you could have all over here like text to image generation where users can input text prompts and Deli can generate unique images based on those description. The images generated by uh Deli are highly realistic and creative. Okay. And it can generate photorealistic images, artistic illustration and even surreal or imaginative visuals. We will also have customization and variability where it


10:09:09
is allowing variation of an image edits based on text instruction and multiple style. So this is also part of a generative AI model and uh it is this tool is actually playing a very amazing role. So I will show you one example like how generative AI is actually working in image generation purposes. So guys as you can see all over here I have opened this generative AI tool called Deli. Let us give a prompt to Deli and let us see how the image is generated. So let's say we want a futuristic city add


10:09:46
sunset filled with neon skyscrapper. Say have flying cars and holographic billboards. The streets are bustling with humanoid robots and we can have people wearing uh let's just say hi-tech you know let's include some technology okay now let us see how uh Deli is trying to create a image so this is how actually generative AI is working. So let it wait for a few seconds as the output comes up. Now you can see all over here that uh this image which is generated basically this is generated by AI and you could see based


10:10:45
on our prompt it has given like the kind of you know uh the input we gave and we got the output based on this. Now so this is one of the amazing uh genai tool we could explore this guys. Okay. Now guys, let us discuss about agentic AI or autonomous decision making and action execution. So you could see this diagram all over here. So agentic AI like unlike the generative AI, it is not generating responses but it is also executing a task autonomously based on the given query. For example like if you take uh


10:11:20
AI in managing a warehouse inventory. Okay. Suppose we want to optimize the warehouse shipment for the next quarter. So here what is going to happen? So first the agent is going to receive its goal all over here. Okay. And um this AI agent uh you know is going to query the external data sources. So it could uh you know for example it could be your uh you know inventory databases or logistics API and then it retrieves realtime inventory levels and it demands the given forecast. Okay. Now at here it


10:11:53
is going to make the autonomous discussions and uh the kind of output we are going to get will be kept in observation by this agent. Okay. So basically it is going to analyze the current warehouse stock predict demand for the next quarter check the supplers's availability and automate the restocking if inventory is below the given threshold. So u for example you could uh imagine uh you know suppose based on the you know output what we are going to get all over here. So based on this output we could uh get certain


10:12:27
thing like this like uh say current inventory level like say 75% capacity. Okay. Then uh it could have also other thing like uh say demand forecast say 30% increase in expected in uh quarter 2 and also it is going to go say like say reordering initiated. So this is the output what we are going to get based on the supply chain management you know example what we are trying to get. So as we have seen in generative AI user is giving the input okay prompt then it is using LLM model to generate the given output but


10:13:31
agentic AI is doing what guys it is going it is going to take a action you know beyond just generating a text. So in this scenario it is squaring the inventory databases. It is automating the purchase order. It is going to select the optimal shipping providers which could be you know you know suitable for the given company. It is going to continuously refine the strategies based on the realtime feedback. So guys let's recap once more. So if you talk about the function base then ji is more concerned with producing


10:14:00
a written content or a visual content. Okay. And even it can code from the pre-existing input. But if you talk about agentic AI guys uh it is actually you know it's all about decision making taking actions towards a specific goal and it is focused on achieving the objectives by interacting with the environment and making the autonomous decision. Genai is exactly relying on the existing data to predict and generate content based on say patterns it has learned during its training phase but it does not adapt or evolve from its


10:14:30
experiences. Whereas if I talk about agentic AI it is adaptive. So it is learning from its actions and experiences. It is improving over time by analyzing the feedback adjusting its behavior to meet objectives more effectively. With the help of genai, human input is essential to the prompt. So that you know basically with the help of that it could go into the LM model and it could generate the given uh you know output based on your prompt. Once uh you set up the agentic AI it requires like minimum human involvement. It


10:15:03
operates autonomously making decisions and adapting to changes like without continuous human guidance and it can even learn in real time. So that's what the beauty of agentic AI is. So we have given one example of genai like basically giving prompt to the chat GPT or Delhi. Okay. And agentic AI one example could be your supply chain management system. Now let us bit deep dive into understanding the technical aspects of how agentic AI is exactly working. Now guys let us try to understand how agentic AI is exactly


10:15:40
working. So there is actually a four-step process of you know how agentic AI exactly works. So the first step is you know perceiving where basically what we are doing is we are gathering and processing information from databases, sensors and digital environments and also the next step is reasoning. So with the help of large language model as a decision-m engine it is generating the solutions. If we talk about the third step which is acting. So it is integrating with external tools and softwares to autonomously execute


10:16:14
the given task. And finally it is learning continuously to improve through the feedback loop which is also known as the data fly. Okay. Now let us explore each of the step one by one and let us try to understand. So if you talk about perceiving. Okay. So this is actually the first step where agentic AI is actually stepping up. So it is doing the perception where what exactly is happening guys that AI is collecting data from multiple sources. So this data could be from database okay like your traditional and vector databases okay so


10:16:49
it could be graphql like uh vector database means the same and uh if you talk about other from the data it could be from APIs like it is fetching realtime information from external systems it is uh basically taking data from the IoT sensors like for real world applications like robotics and logistics and also it could take you know data from the user inputs also like it could be text command, voice commands or a chatbot interaction. Now how it is exactly working guys? So basically let us recolct everything technically and


10:17:23
let us see how this is happening. So the first step which is going in perceiving is the data extraction where exactly the AI agent queries the structured uh databases like SQL or NoSQL for relevant records. uh it is also using vector databases to retrieve any semantic data for context aware responses like uh it could be you know any complaints certain uh you know it is trying to find out okay so next after it has got the data extraction it goes for feature extraction and pre-processing where AI


10:17:56
is filtering the relevant features from the raw data for example like a fraud detection AI is scanning the transaction log for anomalies the third thing it is entity recognition and object detection. So AI uses basically computer vision to detect objects and images and uh then it applying the named entity recognition. This is a technique okay uh to extract the critical terms from the given text also. So we have three uh step-by-step process which is happening in uh perceiving. The first one is data


10:18:28
extraction. Second one is feature extraction and pre-processing. The third one is like entity recognition and object detection. So uh let us take a very simple example like AI based customer support system. So if you consider an agentic AI assistance like for a customer service. So say a customer is asking where is my order? So the AI queries multiple databases all over here. Suppose it is going to query the e-commerce order database to retrieve the order status or it could go to the logistics API to track the


10:19:00
real-time shipment location. Also it could go for customer interaction history to provide personalized response. The result what we get all over here is that the AI is fetching the tracking details identify any delays if it is happening and suggesting the best course of action. Now uh the next step is reasoning. Okay. Now AI's understanding and decision making and problem solving is making agentic AI very way very uh greater. So here what is exactly happening like once the uh AI has perceived the data now it should


10:19:33
start reasoning it. Okay. So the LM model acts as a reasoning engine you know orchestrating AI processes and integrating with specialized models for various function. So if you talk about the key components uh like here used in the reasoning it could be LLM based decision making. So AI agents could use models like LLMs like GBT4 cloud lama to interpret a user intent and generate a response. It is basically coordinating with smaller AI models for domain specific task like it could be like financial prediction or medical


10:20:06
diagnostics. So these could be uh you know the given example then it is using retrieval augmented generation or RA model. Okay. So with the help of which AI is enhancing the accuracy you know by retrieving any proprietary data from the company's databases for example like uh instead of relying on GBT4's knowledge the AI can fetch company specific policies to generate the accurate answers. So this could be the one and uh in in the reasoning the final step is AI workflow and planning. So it is a


10:20:38
multi-step reasoning where AI is breaking down complex task into logical step. For example, like if asks to automate a financial report, AI is retrieving the transaction data, analyzing the trend and it is formatting the results. So for example, you could use this in uh supply chain management. Suppose consider there is a logistics company which is using the agentic AI to optimize what could be the you know uh shipping routes you know. So a supply chain manager requesting the AI agent to find the best shipping route to reduce


10:21:10
the delivery cost. So the AI processes realtime fuel prices, traffic conditions and weather report. So using LLM plus data retrieval, it finds out the optimized routes and selects the cheapest carrier. Result you get is that AI chooses the best delivery option. So here the cost is reduced and improving efficiency. So this is one of the uh use cases guys. So after pursuing you get his uh reasoning. Okay. Now let us move ahead and discuss about the third step which is act. So in this step basically


10:21:42
what is happening like AI is taking autonomous actions. So unlike generative AI which stops at generating content. So agentic AI takes uh the real world action. Okay. How AI is executing task autonomously guys. So basically first step is like here the integration with APIs and software could be happen where AI can send automated API calls to the business systems. for example like uh reordering the stock from the supplers's API. So suppose any inventory level is going down. So it could you know reorder


10:22:14
that particular stock from the supplers's API. So it is interacting with the given API. Now it could also automate the workflows like AI executes multi-step workflows without human supervision. So here like AI can handle like insurance claims by verifying the documents, checking policies and approving the payouts. And finally AI could operate within predefined business rules. Okay. To prevent any unauthorized actions also. So ethical AI is basically being worked in this direction. For example, like AI can automatically


10:22:48
process claims up to say uh $10,000 US, you know, but it is requiring the human approval for the higher amounts. So based on you know uh insurance and policy making stuff. So agentic AI could be you know really helpful in this scenario. Uh one example like uh let's consider so let's say we have this agentic managing an IT support system. So suppose a user says my email server is down. So the AI can diagnose the issue restart the server and confirms the given resolution. Now if it is unresolved then AI escalates to a human


10:23:25
technician. Then uh it results into you know AI is fixing the issues autonomously reducing the downtime. Okay. So this is where your action or act is coming up into the picture. Now if you go on to the next and the final step which is learning. So uh learning basically with the help of data fly wheel it is continuously learning. Okay. So this is the feedback loop all over here which is the data fly wheel. So how AI learns over the time. If we ask this question, so what is exactly happening that it is interacting with the data


10:23:59
collection? Suppose AI logs uh successful and failed actions. For example, like if users correct AI generated responses, the AI is learning from those corrections. Second thing what you could do is you could model uh you could fine-tune the model and do reinforcement learning. So AI adjusts its decision-m models, you know, basically to improve future accuracy. It uses reinforcement learning basically to optimize workflows based on past performance. Okay. Now, uh third step could be automated data labeling and


10:24:31
selfcorrection. So here what is happening that AI is labeling and categorizing past interactions to refine its knowledge base. Example like AI autonomously is updating frequently asked answers based on the recurring user queries. So in this way AI is learning over the time. Uh example one you could consider. So say we have this uh AI is optimizing any financial fraud detection. So say this is uh consider that this is a bank which is AI powered which has this AI powered fraud detection system. So AI is analyzing


10:25:04
these financial transaction and it is detecting any suspicious activity and if flagged the transactions are false and AI is learning to reduce this false alerts. So over the time AI is improving the fraud detection accuracy like minimizing disruptions for the customer. So in this way AI is getting smarter over the time like reducing the false alerts and also the financial fraud. So let's have a just quick recap of what uh we studied right now. So agentic AI works in four steps. The first step is


10:25:36
perceiving where AI is gathering data from databases, sensors and APIs. The next step is reasoning. So it is using uh LLM to interpret task, applies logic and generating the solution. The third step is acting. So here AI is integrating with external systems and automating the task. And finally it is learning. So AI is improving over the time you know via feedback loop or which is basically called as data wheel. So guys uh now let us see this diagram and try to understand what this diagram is trying to say. So the first thing you


10:26:09
could see an AI agent all over here. So this is an AI agent which is basically an autonomous system. So which has a capability of perceiving its environment making decision and executing actions without any human intervention. Now AI agent is acting as the central intelligence okay in this given diagram and it interacts with the user. Okay. Uh and various other data sources. It processes input, queries databases, makes decision using a large language model. And it is executing action and it is learning from the given feedback. Now


10:26:43
the next step you could see the LLM model. So if you talk about LLMs, these are the large language model which is kind of an advanced AI model trained on massive amount of text data to understand, generate and reason over natural language. Now if I talk about this LLM so this is actually acting as the reasoning engine all over here and it is interpreting the user inputs and making informed decision. It is also retrieving relevant data from the databases generating uh responses. It can also coordinate with multiple AI


10:27:15
models for different task like it could be content generation okay predictions or decision making. Now when the user is asking a chatbot like for example let's say what is my account balance so the LLM processes the query retrieves the relevant data and responds the given bank balance accordingly. Now if you look at the kind of database the LLM is interacting. So we have the traditional database and the vector database. So uh here if I say the database like AI agent basically is quering the structured


10:27:47
database. So suppose structured database like it could be a customer records or inventory data or it could be any transactional log also. So traditional databases basically store well definfined you know structured information. Okay. So for example like when a bank EI assistant is processing a query like show my last five transaction. So it is basically fetching the information from a traditional SQLbased database. Next we have this vector database also guys. So vector database is a specialized uh kind of a


10:28:17
database for storing unstructured data which could be like text embeddings, images or audio representations. So guys like unlike traditional databases that store exact values, vector databases store in a highdimensional mathematical space. It allows AI models to search semantically uh similar data instead of like exact matches. Now AI is retrieving the contextual information from the vector databases which is ex uh actually enhancing the decision making. It is improving the AI memory by allowing the


10:28:48
system also to search for you know conceptually similar past interaction. Let us take an example to understand this. For example we have discussed about a customer support jackbot. So suppose if it queries a vector database to find out similar past tickets like when responding to a customer query. So a recommendation engine could use a vector database to find out similar products on a user's past preferences. So this could be done in that scenario. Also some of the like popular vector databases could be like Facebook's AI


10:29:21
similarity search pine cone on VV8. These are the certain uh amazing vector databases. Then you could see the next step is you know after it has worked on these given data it is performing the action. So the action component is referring where AI's agent has this ability to now execute task autonomously after the reasoning is done. So AI is integrating with external tools, APIs or automation software to complete the given task. It does not provide only information but it is actually uh say you know performing the given action. So


10:29:56
for example like in a customer support the AI can automatically reset a user's password after verifying the identity. If we talk about in finance then AI can approve a loan also like based on the predefined eligibility criteria. Now finally we have the data flywheel. So data flywheel is a continuous feedback loop where AI is learning from the past interactions refining its models and it is always improving over the time. Now every time like the AI is interacting with the data or taking an action or


10:30:26
receiving a feedback that information is fed into this model. So this is creating a self improving AI system that is becoming smarter over the time. So the data flywheel is allowing AI to learn from every interaction and uh AI is becoming more efficient by continuously optimizing responses and refining strategies. Best thing in could be used in a fraud detection. So in this the AI is going to learn from the past fraud cases and it is going to detect new fraudulent patterns and more effectively. Chatbots also can learn


10:31:00
from user feedback and improve the responses. And finally, you have the model customization which is basically you are trying to fine-tune the AI models on specific business need or any industry requirement. So AI models are not static like they can be adapted and optimized for a specific task. So custom fine-tuning is actually improving the accuracy and domain specific application like it could be finance, healthcare or cyber security. So a financial institution say fine-tuning an LLM to generate a investment advice okay on a


10:31:33
historical market trends that could be one use case or in healthcare if we discuss like uh the healthcare provider is fine-tuning then AI model to interpret the medical reports and recommend the treatments. So guys based on the given diagram you would have got a brief idea like how uh you know agentic AI is working. Now if we discuss about the future of agentic AI then guys I would say it looks very much promising because it is keep improving itself and it is finding new ways to be useful like with better machine learning algorithms


10:32:07
and smarter decision making these AI system will be more uh independent handling complex task on their own and believe me in industries like healthcare finance customer service they have already started to see how AI agents can make more impact act and it could be more efficient from personalization perspective you know managing resources and many more other things. So as the system continue to learn and adapt I think so they will be opening up even more possibilities helping businesses grow improving how we live and work. Now


10:32:41
I would say that uh in conclusion that agentic AI is actually paving the way for new opportunities like unlike the older versions of AI which was assisting with generating content or predicting the data you know or responding to any queries but agentic AI can perform techniques independently with minimal human effort and agentic AI has become self-reliant in decision-m way and it is making a very big differences in industry like healthcare logistics customer services which is enabling companies to be more efficient. As a


10:33:14
result, it is providing better services to their clients. Today we are diving into the fascinating world of Google Quantum AI. We'll break it down step by step. What Google Quantum AI is, how is different from classical computers and why it's a gamecher and the real problem it's solving. We'll also explore the latest developments, their innovative hardware, the challenges they face, and why despite the hurdles, it's still an incredibly exciting field with a bright future. Stick with me because by the


10:33:44
end, you'll be amazed at how this technology is shaping tomorrow. So, let's get started. The universe operates on quantum mechanics, constantly adapting and evolving to overcome the hurdles it encounters. Quantum computing mirrors the dynamic nature. It doesn't just work within its environment. it responds to it. This unique trait opens the door to groundbreaking solutions for tomorrow's toughest challenges. The question arises, what is Google Quantum AI? Quantum AI is Google's leap into the


10:34:13
future of computing. It's a cuttingedge project where they are building powerful quantum computers and exploring how these machines can solve problems that traditional computers struggle with or can't solve at all. If not aware, classical computers use bits like zero or one and solve tasks step by step. Great for everyday use. Now, quantum computers use cubits, which can be zero, one, or both simultaneously, allowing them to solve complex problems much faster. So, think of Google quantum AI


10:34:44
like you're trying to design a new medicine to fight a disease. A regular computer would analyze molecules step by step, which could take years. But Google quantum AI on the other hand can simulate how molecules interact at the quantum level almost instantly. This speeds up drug discovery potentially saving millions of lives by finding treatments faster. Now you must be wondering why is it so necessary. Google quantum AI is necessary because some problems are just too big and complex for regular computers to solve


10:35:13
efficiently. These are challenges like developing life-saving medicines, creating unbreakable cyber security, optimizing traffic systems, or even understanding how the universe works. Regular computers can take years or even centuries to crack these problems while quantum computers could solve them in minutes or hours. So the question is actually what problems they're solving. It is basically solving so many problems. I will list some of them. Number one, drug discovery. Simulating molecules to find new treatments faster.


10:35:44
Then comes cyber security developing ultra seccure encryption systems to keep your data safe. AI advancements training AI models much quicker and with more accuracy. Climate modeling understanding climate changes to create better solutions for global warming. So in simple terms, Google quantum AI is here to tackle the impossible problems and bring futuristic solutions to today's challenges. It's like upgrading the world's brain to think smarter and faster. So Google quantum AI has been at


10:36:13
the forefront of quantum computing advancements pushing boundaries from the groundbreaking psycho process to the latest innovation willow. In 2019 Google introduced psycho a 53 cubit processor that achieves something called quantum supremacy. So cubits or quantum bits are the core of quantum computers. Unlike regular bits which are either zero or one, cubits can be zero, one or both at once. This called superposition, allowing quantum computers to process vast data simultaneously. They are powerful but fragile, needing precise


10:36:47
control and hold the key to solving complex problems. Psychos solved a problem in just 200 seconds that would take the world's fastest supercomput over 10,000 years. This was a big moment. It showed quantum computers could do things that classical computers couldn't. After psycho, scientists realized a key issue. Quantum computers are very sensitive to errors. Even small disturbances can mess up calculations. To fix this, Google started working on error correction, making their systems more accurate and reliable for real


10:37:19
world use. In 2024, Google launched Willow, a One Note 5 cable processor. This chip is smarter and more powerful, and it can correct errors as they happen. So, Willow shows how much closer we are to building quantum computers that can solve practical problems. Google's logical chibits have reached a huge breakthrough. They now operate below the critical quantum error correction threshold. Sounds exciting, right? But what does this mean? Let's break it down. So, quantum computers use cubits which are very powerful but also


10:37:49
very fragile. They can easily be disrupted by noise or interference causing errors. So, to make quantum computers practical, they need to correct these errors while running complex calculations. This is where logical cubits comes in. They group multiple physical cubits to create a more stable and reliable unit for computing. The error correction threshold is like a magic line. If errors can be corrected faster than they appear, the system becomes scalable and much more reliable by getting their


10:38:18
logical cubits to operate below this threshold. Google has shown that their quantum computers can handle eders effectively, paving the way for larger and more powerful quantum systems. So let's discuss what is a great hardware approach in Google quantum AI that made it possible. Google quantum AI's hardware approach focuses on making quantum computers stable and reliable for practical use. They group cubits which are the building blocks of quantum computers to work together allowing the


10:38:46
system to fix errors as they happen. So by keeping the chips at extremely cold temperatures they reduce interference which keeps the calculations accurate. This setup helps the system handle bigger and more complex tasks like simulating molecules for drug discovery, improving AI models, and creating stronger encryption for data security. It's a big step in making quantum computing a tool for solving real world problems. So, while Google quantum AI has achieved incredible milestone, it still faces some key limitations which


10:39:15
are fragile cubits. Cubits are extremely sensitive to noise and interference which can cause errors. Keeping them stable requires ultra cold temperatures and precise control. Error correction challenges. Although Google has made progress in fixing errors, quantum error correction still isn't perfect and needs more work before quantum computers can scale to solve real world problems reliably. Limited applications. Right now, quantum computers are great for specialized problems like optimization


10:39:43
and simulation. For everyday computing tasks, classical computers are still better. Hardware complexity. Building and maintaining quantum computers incredibly expensive and complicated. The advanced cooling systems and infrastructure make it hard to expand these systems widely still in early stages. Quantum computers including Google's are still in the experimental phase. They're not yet ready for large scale practical use in industries. But despite its challenges, Google quantum AI is paving the way for a future where


10:40:12
quantum computing tackles problems that regular computers can't handle. Like finding new medicines, predicting climate changes, and building smarter AI. It's an exciting start to a whole new era of technology full of possibilities we are just beginning to explore. The future of Google quantum AI is incredibly exciting with a potential to solve real world problems that traditional computers can't handle. It's set to revolutionalize industries like healthcare by speeding up drug discovery, finance through advanced


10:40:40
optimization and energy with better material modeling. So, quantum AI could also lead to breakthroughs in AI by training smarter models faster and commuting unbreakable encryption for stronger data security. As Google improves its hardware and error correction, its quantum systems will become more powerful and reliable, paving the way for large scale practical applications. The possibilities are endless and Google quantum AI is the forefront of shaping a transformative future. So, thank you for joining our


10:41:06
Gen AI full course by simply learn. We hope this training has provided you with valuable insights. Don't forget to subscribe to our channel for more expert courses and tutorials. See you in the next video. Hi there. If you like this video, subscribe to the SimplyLearn YouTube channel and click here to watch similar videos. To nerd up and get certified, click here.

